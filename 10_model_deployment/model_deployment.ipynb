{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment with PyTorch\n",
    "\n",
    "This notebook demonstrates various methods for deploying PyTorch models for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.quantization\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"deployment_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training a Simple Model\n",
    "\n",
    "First, let's train a simple CNN on MNIST that we'll use for deployment examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    \"\"\"A simple CNN for MNIST classification.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = SimpleConvNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training model...\")\n",
    "model.train()\n",
    "for epoch in range(2):  # Quick training for demo\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if batch_idx > 100:  # Limit for demo\n",
    "            break\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/100:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"simple_model.pth\"))\n",
    "print(\"Model trained and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TorchScript - Tracing and Scripting\n",
    "\n",
    "TorchScript provides two ways to convert PyTorch models for deployment: tracing and scripting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracing: Records operations as they are executed\n",
    "model.eval()\n",
    "example_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "\n",
    "# Trace the model\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "\n",
    "# Save traced model\n",
    "traced_path = os.path.join(output_dir, \"traced_model.pt\")\n",
    "traced_model.save(traced_path)\n",
    "print(f\"Traced model saved to {traced_path}\")\n",
    "\n",
    "# Test traced model\n",
    "output_original = model(example_input)\n",
    "output_traced = traced_model(example_input)\n",
    "print(f\"Output difference: {torch.max(torch.abs(output_original - output_traced)).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripting: Analyzes Python code directly\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "# Save scripted model\n",
    "scripted_path = os.path.join(output_dir, \"scripted_model.pt\")\n",
    "scripted_model.save(scripted_path)\n",
    "print(f\"Scripted model saved to {scripted_path}\")\n",
    "\n",
    "# Print the TorchScript code\n",
    "print(\"\\nTorchScript code:\")\n",
    "print(scripted_model.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare inference speeds\n",
    "test_input = torch.randn(100, 1, 28, 28).to(device)\n",
    "\n",
    "# Warm up\n",
    "for _ in range(10):\n",
    "    _ = model(test_input)\n",
    "    _ = traced_model(test_input)\n",
    "\n",
    "# Time original model\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):\n",
    "        _ = model(test_input)\n",
    "original_time = time.time() - start\n",
    "\n",
    "# Time traced model\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):\n",
    "        _ = traced_model(test_input)\n",
    "traced_time = time.time() - start\n",
    "\n",
    "print(f\"Original model: {original_time:.4f}s\")\n",
    "print(f\"Traced model: {traced_time:.4f}s\")\n",
    "print(f\"Speedup: {original_time/traced_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ONNX Export\n",
    "\n",
    "ONNX (Open Neural Network Exchange) allows models to be deployed across different frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "onnx_path = os.path.join(output_dir, \"model.onnx\")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,                       # model\n",
    "    dummy_input,                 # model input\n",
    "    onnx_path,                   # output path\n",
    "    export_params=True,          # store trained params\n",
    "    opset_version=11,            # ONNX version\n",
    "    do_constant_folding=True,    # optimize\n",
    "    input_names=['input'],       # input names\n",
    "    output_names=['output'],     # output names\n",
    "    dynamic_axes={               # variable batch size\n",
    "        'input': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Model exported to ONNX: {onnx_path}\")\n",
    "print(f\"ONNX file size: {os.path.getsize(onnx_path) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify ONNX model (if onnx is installed)\n",
    "try:\n",
    "    import onnx\n",
    "    import onnxruntime as ort\n",
    "    \n",
    "    # Check model\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"ONNX model is valid!\")\n",
    "    \n",
    "    # Run inference with ONNX Runtime\n",
    "    ort_session = ort.InferenceSession(onnx_path)\n",
    "    \n",
    "    # Prepare input\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    test_data = np.random.randn(1, 1, 28, 28).astype(np.float32)\n",
    "    \n",
    "    # Run inference\n",
    "    ort_outputs = ort_session.run(None, {input_name: test_data})\n",
    "    print(f\"ONNX output shape: {ort_outputs[0].shape}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"ONNX/ONNXRuntime not installed. Install with:\")\n",
    "    print(\"pip install onnx onnxruntime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Quantization\n",
    "\n",
    "Quantization reduces model size and improves inference speed by using lower precision representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a quantization-friendly model\n",
    "class QuantizableConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantizableConvNet, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic quantization (weights only)\n",
    "model_fp32 = QuantizableConvNet()\n",
    "model_fp32.load_state_dict(torch.load(os.path.join(output_dir, \"simple_model.pth\"), \n",
    "                                      map_location=device))\n",
    "model_fp32.eval()\n",
    "\n",
    "# Apply dynamic quantization\n",
    "model_int8_dynamic = torch.quantization.quantize_dynamic(\n",
    "    model_fp32,\n",
    "    {nn.Linear},  # Quantize Linear layers\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Compare model sizes\n",
    "def print_model_size(model, name):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size_mb = os.path.getsize(\"temp.p\") / 1e6\n",
    "    os.remove(\"temp.p\")\n",
    "    print(f\"{name} size: {size_mb:.2f} MB\")\n",
    "    return size_mb\n",
    "\n",
    "size_fp32 = print_model_size(model_fp32, \"FP32 model\")\n",
    "size_int8 = print_model_size(model_int8_dynamic, \"INT8 model\")\n",
    "print(f\"Size reduction: {(1 - size_int8/size_fp32)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static quantization requires calibration\n",
    "model_fp32_static = QuantizableConvNet()\n",
    "model_fp32_static.load_state_dict(torch.load(os.path.join(output_dir, \"simple_model.pth\"), \n",
    "                                             map_location='cpu'))  # Must be on CPU\n",
    "model_fp32_static.eval()\n",
    "\n",
    "# Set quantization config\n",
    "model_fp32_static.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Prepare model for quantization\n",
    "model_fp32_prepared = torch.quantization.prepare(model_fp32_static)\n",
    "\n",
    "# Calibrate with representative data\n",
    "print(\"Calibrating model...\")\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, _) in enumerate(test_loader):\n",
    "        if batch_idx > 10:  # Use a few batches for calibration\n",
    "            break\n",
    "        model_fp32_prepared(data)\n",
    "\n",
    "# Convert to quantized model\n",
    "model_int8_static = torch.quantization.convert(model_fp32_prepared)\n",
    "print(\"Static quantization complete!\")\n",
    "\n",
    "# Compare sizes\n",
    "size_static = print_model_size(model_int8_static, \"Static INT8 model\")\n",
    "print(f\"Size reduction vs FP32: {(1 - size_static/size_fp32)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Quantization Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare inference speeds\n",
    "test_input_cpu = torch.randn(100, 1, 28, 28)  # On CPU for quantized models\n",
    "\n",
    "# Time FP32 model\n",
    "model_fp32.eval()\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(50):\n",
    "        _ = model_fp32(test_input_cpu)\n",
    "fp32_time = time.time() - start\n",
    "\n",
    "# Time static quantized model\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(50):\n",
    "        _ = model_int8_static(test_input_cpu)\n",
    "int8_time = time.time() - start\n",
    "\n",
    "print(f\"FP32 model: {fp32_time:.4f}s\")\n",
    "print(f\"INT8 model: {int8_time:.4f}s\")\n",
    "print(f\"Speedup: {fp32_time/int8_time:.2f}x\")\n",
    "\n",
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Model sizes\n",
    "models = ['FP32', 'Dynamic INT8', 'Static INT8']\n",
    "sizes = [size_fp32, size_int8, size_static]\n",
    "ax1.bar(models, sizes)\n",
    "ax1.set_ylabel('Size (MB)')\n",
    "ax1.set_title('Model Size Comparison')\n",
    "\n",
    "# Inference times\n",
    "models_time = ['FP32', 'Static INT8']\n",
    "times = [fp32_time, int8_time]\n",
    "ax2.bar(models_time, times)\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.set_title('Inference Time Comparison (50 iterations)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Serving Example\n",
    "\n",
    "Here's an example of how to create a simple REST API for model serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple inference function\n",
    "def create_inference_function(model_path):\n",
    "    \"\"\"\n",
    "    Create an inference function for the model.\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = torch.jit.load(model_path)\n",
    "    model.eval()\n",
    "    \n",
    "    # Define preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((28, 28)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    def predict(image):\n",
    "        \"\"\"\n",
    "        Make prediction on a PIL image.\n",
    "        \"\"\"\n",
    "        # Preprocess\n",
    "        tensor = transform(image).unsqueeze(0)\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor)\n",
    "            probabilities = F.softmax(output, dim=1)\n",
    "            prediction = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0][prediction].item()\n",
    "        \n",
    "        return {\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence,\n",
    "            'probabilities': probabilities[0].tolist()\n",
    "        }\n",
    "    \n",
    "    return predict\n",
    "\n",
    "# Test the inference function\n",
    "predict_fn = create_inference_function(traced_path)\n",
    "\n",
    "# Create a test image\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Create a simple digit image\n",
    "img = Image.new('L', (28, 28), 0)\n",
    "draw = ImageDraw.Draw(img)\n",
    "draw.text((10, 5), '7', fill=255)\n",
    "\n",
    "# Make prediction\n",
    "result = predict_fn(img)\n",
    "print(f\"Prediction: {result['prediction']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask API example code\n",
    "flask_api_code = '''\n",
    "# save as app.py\n",
    "from flask import Flask, request, jsonify\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load model at startup\n",
    "model = torch.jit.load('traced_model.pt')\n",
    "model.eval()\n",
    "\n",
    "# Define preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Get image from request\n",
    "        data = request.get_json()\n",
    "        image_base64 = data['image']\n",
    "        \n",
    "        # Decode and preprocess\n",
    "        image_bytes = base64.b64decode(image_base64)\n",
    "        image = Image.open(io.BytesIO(image_bytes)).convert('L')\n",
    "        tensor = transform(image).unsqueeze(0)\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor)\n",
    "            probabilities = F.softmax(output, dim=1)\n",
    "            prediction = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0][prediction].item()\n",
    "        \n",
    "        return jsonify({\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence,\n",
    "            'probabilities': probabilities[0].tolist()\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 400\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({'status': 'healthy'})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "'''\n",
    "\n",
    "# Save the Flask app code\n",
    "with open(os.path.join(output_dir, 'flask_app.py'), 'w') as f:\n",
    "    f.write(flask_api_code)\n",
    "\n",
    "print(\"Flask API code saved to flask_app.py\")\n",
    "print(\"\\nTo run the server:\")\n",
    "print(\"1. pip install flask pillow\")\n",
    "print(\"2. python flask_app.py\")\n",
    "print(\"3. Send POST requests to http://localhost:5000/predict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mobile Deployment\n",
    "\n",
    "PyTorch Mobile allows models to run on iOS and Android devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize model for mobile\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "# Load the traced model\n",
    "mobile_model = torch.jit.load(traced_path)\n",
    "mobile_model.eval()\n",
    "\n",
    "# Optimize for mobile\n",
    "optimized_model = optimize_for_mobile(mobile_model)\n",
    "\n",
    "# Save for mobile\n",
    "mobile_path = os.path.join(output_dir, \"model_mobile.ptl\")\n",
    "optimized_model._save_for_lite_interpreter(mobile_path)\n",
    "print(f\"Mobile model saved to {mobile_path}\")\n",
    "print(f\"Mobile model size: {os.path.getsize(mobile_path) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Android code\n",
    "android_code = '''\n",
    "// Android (Java) example\n",
    "import org.pytorch.IValue;\n",
    "import org.pytorch.Module;\n",
    "import org.pytorch.Tensor;\n",
    "import org.pytorch.torchvision.TensorImageUtils;\n",
    "\n",
    "// Load model\n",
    "Module module = Module.load(assetFilePath(this, \"model_mobile.ptl\"));\n",
    "\n",
    "// Prepare input\n",
    "Bitmap bitmap = ... // Your input image\n",
    "Tensor inputTensor = TensorImageUtils.bitmapToFloat32Tensor(\n",
    "    bitmap,\n",
    "    new float[]{0.1307f}, // mean\n",
    "    new float[]{0.3081f}  // std\n",
    ");\n",
    "\n",
    "// Run inference\n",
    "Tensor outputTensor = module.forward(IValue.from(inputTensor)).toTensor();\n",
    "float[] scores = outputTensor.getDataAsFloatArray();\n",
    "\n",
    "// Find prediction\n",
    "int maxIdx = 0;\n",
    "float maxScore = scores[0];\n",
    "for (int i = 1; i < scores.length; i++) {\n",
    "    if (scores[i] > maxScore) {\n",
    "        maxScore = scores[i];\n",
    "        maxIdx = i;\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "print(\"Android integration example:\")\n",
    "print(android_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deployment Best Practices\n",
    "\n",
    "Summary of key considerations for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deployment checklist\n",
    "deployment_checklist = \"\"\"\n",
    "# PyTorch Model Deployment Checklist\n",
    "\n",
    "## 1. Model Optimization\n",
    "- [ ] Convert to TorchScript (tracing or scripting)\n",
    "- [ ] Apply quantization if needed (dynamic or static)\n",
    "- [ ] Optimize for target hardware (mobile, edge, server)\n",
    "- [ ] Profile and benchmark performance\n",
    "\n",
    "## 2. Input/Output Handling\n",
    "- [ ] Document input format and preprocessing steps\n",
    "- [ ] Implement input validation\n",
    "- [ ] Define output format and post-processing\n",
    "- [ ] Handle edge cases and errors gracefully\n",
    "\n",
    "## 3. Serving Infrastructure\n",
    "- [ ] Choose deployment method (REST API, gRPC, batch)\n",
    "- [ ] Implement health checks and monitoring\n",
    "- [ ] Set up logging and metrics collection\n",
    "- [ ] Configure auto-scaling if needed\n",
    "\n",
    "## 4. Testing\n",
    "- [ ] Test on representative data\n",
    "- [ ] Verify numerical accuracy after optimization\n",
    "- [ ] Load test for expected traffic\n",
    "- [ ] Test on target deployment hardware\n",
    "\n",
    "## 5. Security\n",
    "- [ ] Validate and sanitize all inputs\n",
    "- [ ] Use HTTPS for API endpoints\n",
    "- [ ] Implement authentication if needed\n",
    "- [ ] Set up rate limiting\n",
    "\n",
    "## 6. Maintenance\n",
    "- [ ] Version your models\n",
    "- [ ] Plan for model updates\n",
    "- [ ] Monitor model performance over time\n",
    "- [ ] Set up A/B testing framework\n",
    "\"\"\"\n",
    "\n",
    "# Save checklist\n",
    "with open(os.path.join(output_dir, 'deployment_checklist.md'), 'w') as f:\n",
    "    f.write(deployment_checklist)\n",
    "\n",
    "print(\"Deployment checklist saved!\")\n",
    "print(\"\\nKey deployment formats created:\")\n",
    "for file in os.listdir(output_dir):\n",
    "    size = os.path.getsize(os.path.join(output_dir, file)) / 1024\n",
    "    print(f\"- {file}: {size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. **TorchScript**: Converting models using tracing and scripting for production deployment\n",
    "2. **ONNX Export**: Exporting models for cross-framework compatibility\n",
    "3. **Quantization**: Reducing model size and improving speed with INT8 quantization\n",
    "4. **Model Serving**: Creating REST APIs for model inference\n",
    "5. **Mobile Deployment**: Optimizing models for iOS and Android\n",
    "6. **Best Practices**: Key considerations for production deployment\n",
    "\n",
    "Each method has its trade-offs:\n",
    "- **TorchScript**: Best for PyTorch-native deployment with good performance\n",
    "- **ONNX**: Best for cross-framework compatibility\n",
    "- **Quantization**: Best for edge devices and mobile deployment\n",
    "- **Mobile**: Purpose-built for iOS/Android with smallest size\n",
    "\n",
    "Choose the right approach based on your deployment target and requirements!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}