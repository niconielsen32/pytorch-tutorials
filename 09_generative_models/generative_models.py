#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nGenerative Models in PyTorch\n\nThis script demonstrates implementations of various generative models using PyTorch,\nincluding Autoencoders (AEs), Variational Autoencoders (VAEs), and Generative\nAdversarial Networks (GANs). It also provides a conceptual overview of Diffusion Models.\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image, make_grid\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Device configuration\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\nprint(f\"Using device: {device}\")\n\n# Create output directory\output_dir = \"09_generative_models_outputs\"\nos.makedirs(output_dir, exist_ok=True)\nos.makedirs(os.path.join(output_dir, \"generated_images\"), exist_ok=True)\nos.makedirs(os.path.join(output_dir, \"plots\"), exist_ok=True)\n\n# MNIST Dataset (common for generative model demos)\nimg_transform = transforms.Compose([\n    transforms.ToTensor(),\n    # transforms.Normalize((0.5,), (0.5,)) # Normalize to [-1, 1] if using Tanh in generator output\n    # For Sigmoid output in generator, [0,1] from ToTensor() is fine.\n])\n\nmnist_dataset = torchvision.datasets.MNIST(root=\'./data\', train=True, download=True, transform=img_transform)\nmnist_loader = DataLoader(mnist_dataset, batch_size=128, shuffle=True, num_workers=2 if os.name==\'posix\' else 0)\n\n# Image dimensions for MNIST\nIMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS = 28, 28, 1\nIMG_SHAPE = (IMG_CHANNELS, IMG_WIDTH, IMG_HEIGHT)\nFLATTENED_IMG_SIZE = IMG_WIDTH * IMG_HEIGHT * IMG_CHANNELS\n\n# -----------------------------------------------------------------------------\n# Section 1: Introduction to Generative Models (Conceptual - in README)\n# -----------------------------------------------------------------------------\ndef intro_generative_models():\n    print(\"\\nSection 1: Introduction to Generative Models\")\n    print(\"-\" * 70)\n    print(\"This section is conceptual and detailed in the README.md.\")\n    print(\"Covers: What are Generative Models, Importance, Taxonomy.\")\n\n# -----------------------------------------------------------------------------\n# Section 2: Autoencoders (AEs)\n# -----------------------------------------------------------------------------\nclass Autoencoder(nn.Module):\n    def __init__(self, input_dim=FLATTENED_IMG_SIZE, latent_dim=32):\n        super(Autoencoder, self).__init__()\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(True),\n            nn.Linear(256, 128),\n            nn.ReLU(True),\n            nn.Linear(128, latent_dim), # Latent space representation\n            nn.ReLU(True) # Or Tanh, depends on desired latent space properties\n        )\n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, 128),\n            nn.ReLU(True),\n            nn.Linear(128, 256),\n            nn.ReLU(True),\n            nn.Linear(256, input_dim),\n            nn.Sigmoid() # To output values between 0 and 1 for image pixels\n        )\n\n    def forward(self, x):\n        x_flat = x.view(x.size(0), -1) # Flatten input image\n        encoded = self.encoder(x_flat)\n        decoded_flat = self.decoder(encoded)\n        decoded = decoded_flat.view(x.size(0), *IMG_SHAPE) # Reshape to original image shape\n        return decoded, encoded # Return reconstructed image and latent code\n\ndef train_autoencoder(model, dataloader, num_epochs=5, lr=1e-3):\n    print(\"Training Autoencoder...\")\n    criterion = nn.MSELoss() # Mean Squared Error for reconstruction\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    model.train()\n    losses = []\n    start_time = time.time()\n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        for batch_idx, (imgs, _) in enumerate(dataloader): # Labels are not used for AE\n            imgs = imgs.to(device)\n            optimizer.zero_grad()\n            reconstructed_imgs, _ = model(imgs)\n            loss = criterion(reconstructed_imgs, imgs) # Compare reconstruction with original\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item() * imgs.size(0)\n            if batch_idx % 100 == 0:\n                print(f\"  Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(dataloader)} | Loss: {loss.item():.4f}\")\n        avg_epoch_loss = epoch_loss / len(dataloader.dataset)\n        losses.append(avg_epoch_loss)\n        print(f\"=> AE Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_epoch_loss:.4f}\")\n    print(f\"Autoencoder training finished in {time.time()-start_time:.2f}s\")\n    return losses\n\ndef demonstrate_autoencoder():\n    print(\"\\nSection 2: Autoencoders (AEs)\")\n    print(\"-\" * 70)\n    ae_model = Autoencoder(latent_dim=64).to(device)\n    print(\"Autoencoder Architecture:\")\n    print(ae_model)\n    
    # Train the Autoencoder (short training for demo)\n    ae_losses = train_autoencoder(ae_model, mnist_loader, num_epochs=3) # Increase epochs for better results\n    if ae_losses:\n        plt.figure(figsize=(8,4))\n        plt.plot(ae_losses, label=\"AE Training Loss (MSE)\")\n        plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Autoencoder Training Loss\")\n        plt.legend(); plt.grid(True)\n        plt.savefig(os.path.join(output_dir, \"plots\", \"autoencoder_loss.png\")); plt.close()\n        print(\"AE training loss plot saved.\")\n\n    # Visualize some reconstructions\n    ae_model.eval()\n    with torch.no_grad():\n        test_imgs, _ = next(iter(mnist_loader))\n        test_imgs = test_imgs.to(device)[:16] # Take 16 images for visualization\n        reconstructed_test_imgs, _ = ae_model(test_imgs)\n        comparison = torch.cat([test_imgs.view(-1, 1, IMG_WIDTH, IMG_HEIGHT), \n                                reconstructed_test_imgs.view(-1, 1, IMG_WIDTH, IMG_HEIGHT)])\n        save_image(comparison.cpu(), os.path.join(output_dir, \"generated_images\", \"ae_reconstruction.png\"), nrow=16)\n        print(\"AE original vs reconstructed images saved.\")\n\n# -----------------------------------------------------------------------------\n# Section 3: Variational Autoencoders (VAEs)\n# -----------------------------------------------------------------------------
class VAE(nn.Module):\n    def __init__(self, input_dim=FLATTENED_IMG_SIZE, h_dim=400, z_dim=20): # z_dim is latent space dimension\n        super(VAE, self).__init__()\n        self.fc_enc1 = nn.Linear(input_dim, h_dim)\n        self.fc_enc_mean = nn.Linear(h_dim, z_dim)  # To output mu (mean)\n        self.fc_enc_logvar = nn.Linear(h_dim, z_dim) # To output log_var (log variance)\n        \n        self.fc_dec1 = nn.Linear(z_dim, h_dim)\n        self.fc_dec2 = nn.Linear(h_dim, input_dim) # To reconstruct image\n\n    def encode(self, x):\n        h = F.relu(self.fc_enc1(x))\n        return self.fc_enc_mean(h), self.fc_enc_logvar(h)\n\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var) # std = sqrt(variance) = exp(0.5 * log_var)\n        eps = torch.randn_like(std)    # Sample epsilon from N(0, I)\n        return mu + eps * std            # Sample z from N(mu, var)\n\n    def decode(self, z):\n        h = F.relu(self.fc_dec1(z))\n        return torch.sigmoid(self.fc_dec2(h)) # Use sigmoid for pixel values [0,1]\n\n    def forward(self, x):\n        x_flat = x.view(x.size(0), -1)\n        mu, log_var = self.encode(x_flat)\n        z = self.reparameterize(mu, log_var)\n        x_reconstructed_flat = self.decode(z)\n        x_reconstructed = x_reconstructed_flat.view(x.size(0), *IMG_SHAPE)\n        return x_reconstructed, mu, log_var\n\n# VAE Loss = Reconstruction Loss + KL Divergence\ndef vae_loss_function(recon_x, x, mu, log_var):\n    # Reconstruction loss (e.g., Binary Cross Entropy for sigmoid output)\n    BCE = F.binary_cross_entropy(recon_x.view(-1, FLATTENED_IMG_SIZE), \n                                 x.view(-1, FLATTENED_IMG_SIZE), reduction=\'sum\')\n    # KL divergence: 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n    return BCE + KLD\n\ndef train_vae(model, dataloader, num_epochs=5, lr=1e-3):\n    print(\"Training VAE...\")\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    model.train()\n    losses = []\n    start_time = time.time()\n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        for batch_idx, (imgs, _) in enumerate(dataloader):\n            imgs = imgs.to(device)\n            optimizer.zero_grad()\n            recon_imgs, mu, log_var = model(imgs)\n            loss = vae_loss_function(recon_imgs, imgs, mu, log_var)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            if batch_idx % 100 == 0:\n                print(f\"  Epoch {epoch+1}/{num_epochs} | Batch {batch_idx}/{len(dataloader)} | Loss: {loss.item()/imgs.size(0):.4f}\")\n        avg_epoch_loss = epoch_loss / len(dataloader.dataset)\n        losses.append(avg_epoch_loss)\n        print(f\"=> VAE Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_epoch_loss:.4f}\")\n    print(f\"VAE training finished in {time.time()-start_time:.2f}s\")\n    return losses\n\ndef demonstrate_vae():\n    print(\"\\nSection 3: Variational Autoencoders (VAEs)\")\n    print(\"-\" * 70)\n    vae_model = VAE(z_dim=20).to(device) # Latent dimension z_dim=20\n    print(\"VAE Architecture:\")\n    print(vae_model)\n\n    vae_losses = train_vae(vae_model, mnist_loader, num_epochs=3) # Increase epochs for better results\n    if vae_losses:\n        plt.figure(figsize=(8,4))\n        plt.plot(vae_losses, label=\"VAE Training Loss (Reconstruction + KLD)\")\n        plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"VAE Training Loss\")\n        plt.legend(); plt.grid(True)\n        plt.savefig(os.path.join(output_dir, \"plots\", \"vae_loss.png\")); plt.close()\n        print(\"VAE training loss plot saved.\")\n\n    # Visualize reconstructions and generated samples\n    vae_model.eval()\n    with torch.no_grad():\n        # Reconstructions\n        test_imgs_vae, _ = next(iter(mnist_loader))\n        test_imgs_vae = test_imgs_vae.to(device)[:8] # 8 images for reconstruction viz\n        reconstructed_vae, _, _ = vae_model(test_imgs_vae)\n        comparison_vae = torch.cat([test_imgs_vae.view(-1, 1, IMG_WIDTH, IMG_HEIGHT), \n                                    reconstructed_vae.view(-1, 1, IMG_WIDTH, IMG_HEIGHT)])\n        save_image(comparison_vae.cpu(), os.path.join(output_dir, \"generated_images\", \"vae_reconstruction.png\"), nrow=8)\n        print(\"VAE original vs reconstructed images saved.\")\n\n        # Generate new samples from latent space\n        num_generated_samples = 16\n        random_latent_z = torch.randn(num_generated_samples, 20).to(device) # Sample z from N(0,I)\n        generated_imgs_flat = vae_model.decode(random_latent_z)\n        generated_imgs = generated_imgs_flat.view(num_generated_samples, *IMG_SHAPE)\n        save_image(generated_imgs.cpu(), os.path.join(output_dir, \"generated_images\", \"vae_generated_samples.png\"), nrow=4)\n        print(\"VAE generated samples from random latent vectors saved.\")\n\n# -----------------------------------------------------------------------------\n# Section 4: Generative Adversarial Networks (GANs)\n# -----------------------------------------------------------------------------\n\n# --- Generator for GAN ---\nclass GANGenerator(nn.Module):\n    def __init__(self, z_dim=100, img_dim=FLATTENED_IMG_SIZE): # z_dim is latent noise vector size\n        super(GANGenerator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(z_dim, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(1024, img_dim),\n            nn.Tanh() # Output pixel values in [-1, 1] - requires input data normalization to [-1,1]\n            # If using Sigmoid, output is [0,1] and input data should be [0,1]\n        )\n\n    def forward(self, z): # z is noise vector\n        img_flat = self.model(z)\n        img = img_flat.view(img_flat.size(0), *IMG_SHAPE) # Reshape to image\n        return img\n\n# --- Discriminator for GAN ---\nclass GANDiscriminator(nn.Module):\n    def __init__(self, img_dim=FLATTENED_IMG_SIZE):\n        super(GANDiscriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(img_dim, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid() # Output probability: Real (1) or Fake (0)\n        )\n\n    def forward(self, img):\n        img_flat = img.view(img.size(0), -1) # Flatten image\n        validity = self.model(img_flat)\n        return validity\n\ndef train_gan(generator, discriminator, dataloader, num_epochs=10, lr=0.0002, z_dim=100):\n    print(\"Training GAN...\")\n    # Loss function: Binary Cross Entropy\n    adversarial_loss = nn.BCELoss()\n    # Optimizers\n    optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n\n    g_losses, d_losses = [], []\n    fixed_noise = torch.randn(64, z_dim).to(device) # For consistent visualization of G\'s progress\n    start_time = time.time()\n\n    for epoch in range(num_epochs):\n        epoch_g_loss, epoch_d_loss = 0.0, 0.0\n        for i, (real_imgs, _) in enumerate(dataloader):\n            real_imgs = real_imgs.to(device)\n            # For Tanh output, normalize real_imgs to [-1, 1]\n            # real_imgs = (real_imgs - 0.5) * 2.0 
            # If Generator uses Sigmoid, real_imgs are fine as [0,1]\n
            batch_size = real_imgs.size(0)\n            # Adversarial ground truths\n            real_label = torch.full((batch_size, 1), 1.0, device=device, dtype=torch.float)\n            fake_label = torch.full((batch_size, 1), 0.0, device=device, dtype=torch.float)\n
            # --- Train Discriminator ---
            optimizer_D.zero_grad()\n            # Loss for real images\n            real_pred = discriminator(real_imgs)\n            d_real_loss = adversarial_loss(real_pred, real_label)\n            # Loss for fake images\n            z_noise = torch.randn(batch_size, z_dim).to(device)\n            fake_imgs = generator(z_noise)\n            fake_pred = discriminator(fake_imgs.detach()) # detach to avoid training G on D\'s loss\n            d_fake_loss = adversarial_loss(fake_pred, fake_label)\n            # Total discriminator loss\n            d_loss = (d_real_loss + d_fake_loss) / 2\n            d_loss.backward()\n            optimizer_D.step()\n
            # --- Train Generator ---
            optimizer_G.zero_grad()\n            # Generate fake images again (new batch, or use the one above if not detached)\n            # z_noise = torch.randn(batch_size, z_dim).to(device) # Can reuse or make new
            # fake_imgs = generator(z_noise) # No need to generate again if we used .detach() correctly\n            fake_pred_for_g = discriminator(fake_imgs) # Pass fake_imgs (not detached) through D\n            g_loss = adversarial_loss(fake_pred_for_g, real_label) # Generator wants D to think fake is real\n            g_loss.backward()\n            optimizer_G.step()\n            \n            epoch_d_loss += d_loss.item()\n            epoch_g_loss += g_loss.item()\n
            if (i + 1) % 100 == 0:\n                print(f\"  Epoch {epoch+1}/{num_epochs} | Batch {i+1}/{len(dataloader)} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")
        
        avg_epoch_d_loss = epoch_d_loss / len(dataloader)\n        avg_epoch_g_loss = epoch_g_loss / len(dataloader)\n        d_losses.append(avg_epoch_d_loss)\n        g_losses.append(avg_epoch_g_loss)\n        print(f\"=> GAN Epoch {epoch+1}/{num_epochs}, Avg D Loss: {avg_epoch_d_loss:.4f}, Avg G Loss: {avg_epoch_g_loss:.4f}\")\n        
        # Save generated image samples from fixed noise\n        if (epoch + 1) % 2 == 0 or epoch == num_epochs -1 : # Save every 2 epochs and last epoch
            with torch.no_grad():\n                generator.eval()\n                generated_fixed_noise = generator(fixed_noise).detach().cpu()\n                # If generator used Tanh, unnormalize from [-1,1] to [0,1] for saving\n                # generated_fixed_noise = generated_fixed_noise * 0.5 + 0.5 
                save_image(generated_fixed_noise, \n                           os.path.join(output_dir, \"generated_images\", f\"gan_epoch_{epoch+1}.png\"), \n                           nrow=8, normalize=True if generator.model[-1].__class__.__name__ == \"Tanh\" else False)
                generator.train()\n    print(f\"GAN training finished in {time.time()-start_time:.2f}s\")\n    return g_losses, d_losses\n\ndef demonstrate_gan():\n    print(\"\\nSection 4: Generative Adversarial Networks (GANs)\")\n    print(\"-\" * 70)\n    Z_DIMENSION = 100 # Latent dimension for noise vector\n\n    gan_generator = GANGenerator(z_dim=Z_DIMENSION).to(device)\n    gan_discriminator = GANDiscriminator().to(device)\n    print(\"GAN Generator Architecture:\")\n    print(gan_generator)\n    print(\"GAN Discriminator Architecture:\")\n    print(gan_discriminator)\n\n    # Train GAN (short training for demo)\n    # Note: GANs often require many epochs (50-200+) and careful hyperparameter tuning.\n    # Using normalized MNIST data if Tanh is in generator output
    # For this demo, using Sigmoid in generator and ToTensor() is fine for MNIST [0,1]
    # If Tanh, then need: transforms.Normalize((0.5,), (0.5,))
    # And unnormalize when saving: img = img * 0.5 + 0.5
    g_losses, d_losses = train_gan(gan_generator, gan_discriminator, mnist_loader, num_epochs=5, z_dim=Z_DIMENSION)
\n    if g_losses and d_losses:\n        plt.figure(figsize=(10,5))\n        plt.plot(g_losses, label=\"Generator Loss\")\n        plt.plot(d_losses, label=\"Discriminator Loss\")\n        plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss (BCE)\"); plt.title(\"GAN Training Losses\")\n        plt.legend(); plt.grid(True)\n        plt.savefig(os.path.join(output_dir, \"plots\", \"gan_training_losses.png\")); plt.close()\n        print(\"GAN training losses plot saved.\")\n    print(\"Generated image samples from GAN training are saved in the outputs directory.\")\n\n# -----------------------------------------------------------------------------\n# Section 5: Introduction to Diffusion Models (Conceptual - in README)\n# -----------------------------------------------------------------------------
\ndef intro_diffusion_models():\n    print(\"\\nSection 5: Introduction to Diffusion Models\")\n    print(\"-\" * 70)\n    print(\"This section is conceptual and detailed in the README.md.\")\n    print(\"Covers: Forward (Noising) Process, Reverse (Denoising) Process, U-Net architecture.\")\n    print(\"Diffusion models are known for high-quality sample generation but can be complex to implement fully.\")\n\n# -----------------------------------------------------------------------------\n# Section 6: Applications of Generative Models (Conceptual - in README)\n# -----------------------------------------------------------------------------
\ndef applications_generative_models():\n    print(\"\\nSection 6: Applications of Generative Models\")\n    print(\"-\" * 70)\n    print(\"This section is conceptual and detailed in the README.md.\")\n    print(\"Covers: Image Synthesis/Editing, Text Generation, Drug Discovery, Anomaly Detection, Data Augmentation.\")\n\n# -----------------------------------------------------------------------------\n# Main function to run selected demonstrations\n# -----------------------------------------------------------------------------
\ndef main():\n    \"\"\"Main function to run Generative Models tutorial sections.\"\"\"\n    print(\"=\" * 80)\n    print(\"PyTorch Generative Models Tutorial\")\n    print(\"=\" * 80)\n\n    intro_generative_models() # Section 1\n    demonstrate_autoencoder()   # Section 2\n    demonstrate_vae()           # Section 3\n    demonstrate_gan()           # Section 4\n    intro_diffusion_models()    # Section 5 (Conceptual)\n    applications_generative_models() # Section 6 (Conceptual)\n\n    print(\"\\nTutorial complete! Outputs (generated images, plots) are in \'09_generative_models_outputs\' directory.\")\n\nif __name__ == \'__main__\':\n    main() 