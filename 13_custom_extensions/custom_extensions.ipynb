{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 13: Custom Extensions (C++ and CUDA)\n",
    "\n",
    "This tutorial demonstrates how to create custom C++ and CUDA extensions for PyTorch to achieve better performance for specialized operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Custom Extensions?\n",
    "\n",
    "Custom extensions are useful when:\n",
    "1. **Performance**: C++/CUDA can be much faster than Python\n",
    "2. **Memory efficiency**: Better control over memory allocation\n",
    "3. **Novel operations**: Implement operations not available in PyTorch\n",
    "4. **Hardware optimization**: Leverage specific hardware features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple C++ Extension\n",
    "\n",
    "Let's create a custom ReLU implementation in C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C++ source code for a custom ReLU implementation\n",
    "cpp_source = '''\n",
    "#include <torch/extension.h>\n",
    "#include <vector>\n",
    "\n",
    "// Forward pass\n",
    "torch::Tensor custom_relu_forward(torch::Tensor input) {\n",
    "    auto output = torch::zeros_like(input);\n",
    "    output = torch::where(input > 0, input, output);\n",
    "    return output;\n",
    "}\n",
    "\n",
    "// Backward pass\n",
    "torch::Tensor custom_relu_backward(torch::Tensor grad_output, torch::Tensor input) {\n",
    "    auto grad_input = torch::zeros_like(grad_output);\n",
    "    grad_input = torch::where(input > 0, grad_output, grad_input);\n",
    "    return grad_input;\n",
    "}\n",
    "\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "    m.def(\"forward\", &custom_relu_forward, \"Custom ReLU forward\");\n",
    "    m.def(\"backward\", &custom_relu_backward, \"Custom ReLU backward\");\n",
    "}\n",
    "'''\n",
    "\n",
    "# Load the extension\n",
    "custom_relu_cpp = load_inline(\n",
    "    name='custom_relu_cpp',\n",
    "    cpp_sources=[cpp_source],\n",
    "    functions=['forward', 'backward'],\n",
    "    verbose=True,\n",
    "    build_directory='./cpp_build'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom autograd Function\n",
    "class CustomReLUFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return custom_relu_cpp.forward(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        return custom_relu_cpp.backward(grad_output, input)\n",
    "\n",
    "# Wrap it in a module\n",
    "class CustomReLU(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return CustomReLUFunction.apply(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the custom ReLU\n",
    "x = torch.randn(10, 10, requires_grad=True)\n",
    "custom_relu = CustomReLU()\n",
    "y = custom_relu(x)\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Gradient computed: {x.grad is not None}\")\n",
    "\n",
    "# Visualize the ReLU function\n",
    "x_test = torch.linspace(-2, 2, 100)\n",
    "y_test = custom_relu(x_test)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_test.detach().numpy(), y_test.detach().numpy(), label='Custom ReLU')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('Custom ReLU Function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Fused Linear Layer\n",
    "\n",
    "Let's create a fused linear layer that combines linear transformation, bias addition, and ReLU activation in a single operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C++ code for fused linear layer (bias + activation)\n",
    "fused_cpp_source = '''\n",
    "#include <torch/extension.h>\n",
    "#include <vector>\n",
    "\n",
    "torch::Tensor fused_linear_relu_forward(\n",
    "    torch::Tensor input,\n",
    "    torch::Tensor weight,\n",
    "    torch::Tensor bias) {\n",
    "    \n",
    "    // Perform linear transformation\n",
    "    auto output = torch::matmul(input, weight.t());\n",
    "    \n",
    "    // Add bias and apply ReLU in one pass\n",
    "    output = torch::clamp_min(output + bias, 0);\n",
    "    \n",
    "    return output;\n",
    "}\n",
    "\n",
    "std::vector<torch::Tensor> fused_linear_relu_backward(\n",
    "    torch::Tensor grad_output,\n",
    "    torch::Tensor input,\n",
    "    torch::Tensor weight,\n",
    "    torch::Tensor output) {\n",
    "    \n",
    "    // ReLU backward\n",
    "    auto relu_grad = torch::where(output > 0, grad_output, torch::zeros_like(grad_output));\n",
    "    \n",
    "    // Linear backward\n",
    "    auto grad_input = torch::matmul(relu_grad, weight);\n",
    "    auto grad_weight = torch::matmul(relu_grad.t(), input);\n",
    "    auto grad_bias = relu_grad.sum(0);\n",
    "    \n",
    "    return {grad_input, grad_weight, grad_bias};\n",
    "}\n",
    "\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "    m.def(\"forward\", &fused_linear_relu_forward, \"Fused Linear-ReLU forward\");\n",
    "    m.def(\"backward\", &fused_linear_relu_backward, \"Fused Linear-ReLU backward\");\n",
    "}\n",
    "'''\n",
    "\n",
    "# Load the fused operation\n",
    "fused_linear_relu = load_inline(\n",
    "    name='fused_linear_relu',\n",
    "    cpp_sources=[fused_cpp_source],\n",
    "    functions=['forward', 'backward'],\n",
    "    verbose=True,\n",
    "    build_directory='./cpp_build'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedLinearReLUFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias):\n",
    "        output = fused_linear_relu.forward(input, weight, bias)\n",
    "        ctx.save_for_backward(input, weight, output)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, output = ctx.saved_tensors\n",
    "        grad_input, grad_weight, grad_bias = fused_linear_relu.backward(\n",
    "            grad_output, input, weight, output\n",
    "        )\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "\n",
    "class FusedLinearReLU(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return FusedLinearReLUFunction.apply(input, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fused layer\n",
    "fused_layer = FusedLinearReLU(100, 50)\n",
    "x = torch.randn(32, 100)\n",
    "y = fused_layer(x)\n",
    "print(f\"Fused layer output shape: {y.shape}\")\n",
    "\n",
    "# Verify gradients work correctly\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "print(f\"Weight gradient shape: {fused_layer.weight.grad.shape}\")\n",
    "print(f\"Bias gradient shape: {fused_layer.bias.grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Custom Optimizer in C++\n",
    "\n",
    "Let's implement a custom SGD optimizer in C++ for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_optimizer_source = '''\n",
    "#include <torch/extension.h>\n",
    "#include <vector>\n",
    "\n",
    "void custom_sgd_step(\n",
    "    torch::Tensor param,\n",
    "    torch::Tensor grad,\n",
    "    torch::Tensor momentum_buffer,\n",
    "    float lr,\n",
    "    float momentum,\n",
    "    float weight_decay) {\n",
    "    \n",
    "    if (weight_decay != 0) {\n",
    "        grad = grad + weight_decay * param;\n",
    "    }\n",
    "    \n",
    "    if (momentum != 0) {\n",
    "        momentum_buffer.mul_(momentum).add_(grad);\n",
    "        param.add_(momentum_buffer, -lr);\n",
    "    } else {\n",
    "        param.add_(grad, -lr);\n",
    "    }\n",
    "}\n",
    "\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "    m.def(\"step\", &custom_sgd_step, \"Custom SGD step\");\n",
    "}\n",
    "'''\n",
    "\n",
    "custom_sgd = load_inline(\n",
    "    name='custom_sgd',\n",
    "    cpp_sources=[custom_optimizer_source],\n",
    "    functions=['step'],\n",
    "    verbose=True,\n",
    "    build_directory='./cpp_build'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSGD:\n",
    "    def __init__(self, params, lr=0.01, momentum=0.9, weight_decay=0):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum_buffers = {}\n",
    "        \n",
    "        for p in self.params:\n",
    "            self.momentum_buffers[p] = torch.zeros_like(p)\n",
    "    \n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                custom_sgd.step(\n",
    "                    p.data,\n",
    "                    p.grad.data,\n",
    "                    self.momentum_buffers[p],\n",
    "                    self.lr,\n",
    "                    self.momentum,\n",
    "                    self.weight_decay\n",
    "                )\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test custom optimizer\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = CustomSGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Simple training loop\n",
    "for i in range(10):\n",
    "    x = torch.randn(32, 10)\n",
    "    target = torch.randn(32, 1)\n",
    "    \n",
    "    output = model(x)\n",
    "    loss = F.mse_loss(output, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        print(f\"Iteration {i}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare the performance of our custom operations with PyTorch's built-in operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_operation(name, func, *args, num_runs=1000):\n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        func(*args)\n",
    "    \n",
    "    # Benchmark\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        result = func(*args)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    avg_time = (end_time - start_time) / num_runs * 1000  # Convert to ms\n",
    "    \n",
    "    return avg_time, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare custom ReLU with PyTorch ReLU\n",
    "sizes = [100, 500, 1000, 2000]\n",
    "pytorch_times = []\n",
    "custom_times = []\n",
    "\n",
    "for size in sizes:\n",
    "    x = torch.randn(size, size)\n",
    "    pytorch_relu = nn.ReLU()\n",
    "    custom_relu = CustomReLU()\n",
    "    \n",
    "    pytorch_time, _ = benchmark_operation(\"PyTorch ReLU\", pytorch_relu, x, num_runs=100)\n",
    "    custom_time, _ = benchmark_operation(\"Custom ReLU\", custom_relu, x, num_runs=100)\n",
    "    \n",
    "    pytorch_times.append(pytorch_time)\n",
    "    custom_times.append(custom_time)\n",
    "    \n",
    "    print(f\"Size {size}x{size}: PyTorch {pytorch_time:.4f}ms, Custom {custom_time:.4f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sizes, pytorch_times, 'o-', label='PyTorch ReLU', linewidth=2)\n",
    "plt.plot(sizes, custom_times, 's-', label='Custom ReLU', linewidth=2)\n",
    "plt.xlabel('Tensor Size (NxN)')\n",
    "plt.ylabel('Time (ms)')\n",
    "plt.title('Performance Comparison: PyTorch vs Custom ReLU')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Extension Example (Pseudo-code)\n",
    "\n",
    "Here's an example of how you would write a CUDA extension. Note that this requires CUDA to be available and properly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA kernel example (this is just for demonstration)\n",
    "cuda_kernel_example = '''\n",
    "// custom_matmul_kernel.cu\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "template <typename scalar_t>\n",
    "__global__ void custom_matmul_kernel(\n",
    "    const scalar_t* __restrict__ a,\n",
    "    const scalar_t* __restrict__ b,\n",
    "    scalar_t* __restrict__ c,\n",
    "    int m, int n, int k) {\n",
    "    \n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (row < m && col < n) {\n",
    "        scalar_t sum = 0;\n",
    "        for (int i = 0; i < k; i++) {\n",
    "            sum += a[row * k + i] * b[i * n + col];\n",
    "        }\n",
    "        c[row * n + col] = sum;\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "print(\"CUDA Kernel Example:\")\n",
    "print(cuda_kernel_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Extensions with setuptools\n",
    "\n",
    "For production use, you'll want to build extensions using setuptools instead of JIT compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_py_example = '''\n",
    "# setup.py\n",
    "from setuptools import setup, Extension\n",
    "from torch.utils import cpp_extension\n",
    "\n",
    "setup(\n",
    "    name='custom_ops',\n",
    "    ext_modules=[\n",
    "        cpp_extension.CppExtension(\n",
    "            'custom_ops',\n",
    "            ['custom_ops.cpp'],\n",
    "            extra_compile_args=['-O3']\n",
    "        ),\n",
    "        cpp_extension.CUDAExtension(\n",
    "            'custom_cuda_ops',\n",
    "            ['custom_cuda_ops.cpp', 'custom_cuda_ops_kernel.cu'],\n",
    "            extra_compile_args={'cxx': ['-O3'],\n",
    "                              'nvcc': ['-O3', '--use_fast_math']}\n",
    "        ) if torch.cuda.is_available() else None\n",
    "    ],\n",
    "    cmdclass={\n",
    "        'build_ext': cpp_extension.BuildExtension\n",
    "    }\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"Example setup.py:\")\n",
    "print(setup_py_example)\n",
    "print(\"\\nTo build: python setup.py install\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Debugging Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Gradient checking for custom operations\n",
    "def test_custom_operation():\n",
    "    # Test forward pass\n",
    "    x = torch.randn(10, 10, requires_grad=True, dtype=torch.double)\n",
    "    \n",
    "    # Use gradcheck to verify gradients\n",
    "    from torch.autograd import gradcheck\n",
    "    \n",
    "    # Test custom ReLU\n",
    "    input = (x,)\n",
    "    test = gradcheck(CustomReLUFunction.apply, input, eps=1e-6, atol=1e-4)\n",
    "    print(f\"Custom ReLU gradient check: {'PASSED' if test else 'FAILED'}\")\n",
    "    \n",
    "    # Test fused linear layer\n",
    "    weight = torch.randn(5, 10, requires_grad=True, dtype=torch.double)\n",
    "    bias = torch.randn(5, requires_grad=True, dtype=torch.double)\n",
    "    input = (x, weight, bias)\n",
    "    test = gradcheck(FusedLinearReLUFunction.apply, input, eps=1e-6, atol=1e-4)\n",
    "    print(f\"Fused Linear ReLU gradient check: {'PASSED' if test else 'FAILED'}\")\n",
    "\n",
    "test_custom_operation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we've covered:\n",
    "\n",
    "1. **Writing C++ Extensions**: How to create custom operations in C++\n",
    "2. **Autograd Integration**: Making custom ops work with PyTorch's autograd\n",
    "3. **Performance Optimization**: Fusing operations for better performance\n",
    "4. **Custom Optimizers**: Implementing optimizers in C++\n",
    "5. **CUDA Extensions**: Basics of GPU-accelerated custom operations\n",
    "6. **Building and Packaging**: How to properly build and distribute extensions\n",
    "\n",
    "### Key Takeaways:\n",
    "- Only use custom extensions when necessary\n",
    "- Always verify gradients with gradcheck\n",
    "- Profile before and after to ensure performance gains\n",
    "- Consider memory layout and tensor continuity\n",
    "- Test on different platforms and configurations\n",
    "\n",
    "Custom extensions are powerful but should be used judiciously. PyTorch's built-in operations are highly optimized and sufficient for most use cases!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}