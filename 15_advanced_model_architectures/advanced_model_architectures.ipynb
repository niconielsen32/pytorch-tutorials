{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 15: Advanced Model Architectures\n",
    "\n",
    "This tutorial explores cutting-edge neural network architectures including Graph Neural Networks, Vision Transformers, and other state-of-the-art models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Optional, Tuple, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Graph Neural Networks (GNNs)\n",
    "\n",
    "Graph Neural Networks are designed to work with graph-structured data, where relationships between entities are as important as the entities themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionLayer(nn.Module):\n",
    "    \"\"\"Simple Graph Convolution Layer\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        # x: [num_nodes, in_features]\n",
    "        # adj: [num_nodes, num_nodes]\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.sparse.mm(adj, support)\n",
    "        return output + self.bias\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"Graph Convolutional Network\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(GraphConvolutionLayer(input_dim, hidden_dim))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GraphConvolutionLayer(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(GraphConvolutionLayer(hidden_dim, output_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = layer(x, adj)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.layers[-1](x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and visualize a simple graph\n",
    "num_nodes = 20\n",
    "num_features = 16\n",
    "num_classes = 4\n",
    "\n",
    "# Create a random graph\n",
    "G = nx.erdos_renyi_graph(num_nodes, 0.3)\n",
    "adj_matrix = nx.adjacency_matrix(G).todense()\n",
    "adj_tensor = torch.FloatTensor(adj_matrix).to_sparse()\n",
    "\n",
    "# Visualize the graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', \n",
    "        node_size=500, font_size=10, edge_color='gray')\n",
    "plt.title(\"Example Graph Structure\")\n",
    "plt.show()\n",
    "\n",
    "# Random node features\n",
    "x = torch.randn(num_nodes, num_features)\n",
    "\n",
    "# Create and test GCN\n",
    "gcn = GCN(num_features, 32, num_classes)\n",
    "output = gcn(x, adj_tensor)\n",
    "print(f\"GCN output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Graph Attention Networks (GAT)\n",
    "\n",
    "GATs use attention mechanisms to weigh the importance of neighboring nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"Graph Attention Layer\"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.W = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.a = nn.Parameter(torch.randn(2 * out_features, 1))\n",
    "        \n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.xavier_uniform_(self.a)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        # x: [N, in_features]\n",
    "        h = torch.mm(x, self.W)  # [N, out_features]\n",
    "        N = h.size(0)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), \n",
    "                           h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "        \n",
    "        # Mask attention scores\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj.to_dense() > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        \n",
    "        h_prime = torch.matmul(attention, h)\n",
    "        return F.elu(h_prime), attention\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    \"\"\"Graph Attention Network\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            GraphAttentionLayer(input_dim, hidden_dim) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.out_att = GraphAttentionLayer(hidden_dim * num_heads, output_dim)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        # Multi-head attention\n",
    "        att_outputs = []\n",
    "        attentions = []\n",
    "        for att in self.attention_heads:\n",
    "            out, attn = att(x, adj)\n",
    "            att_outputs.append(out)\n",
    "            attentions.append(attn)\n",
    "            \n",
    "        x = torch.cat(att_outputs, dim=1)\n",
    "        x = F.dropout(x, 0.6, training=self.training)\n",
    "        x, final_attn = self.out_att(x, adj)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1), attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GAT and visualize attention\n",
    "gat = GAT(num_features, 8, num_classes, num_heads=4)\n",
    "output, attentions = gat(x, adj_tensor)\n",
    "print(f\"GAT output shape: {output.shape}\")\n",
    "\n",
    "# Visualize attention weights for one head\n",
    "if len(attentions) > 0:\n",
    "    attn = attentions[0].detach().numpy()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(attn, cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.title('Graph Attention Weights (Head 1)')\n",
    "    plt.xlabel('Node')\n",
    "    plt.ylabel('Node')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vision Transformer (ViT)\n",
    "\n",
    "Vision Transformers apply the transformer architecture directly to sequences of image patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Convert image into patches and embed them\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', \n",
    "                     p1=patch_size, p2=patch_size),\n",
    "            nn.Linear(patch_size * patch_size * in_channels, embed_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Self Attention\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.attention_dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attention_dropout(attn)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_dropout(x)\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer Block\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out, attn_weights = self.attn(self.norm1(x))\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x, attn_weights\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, \n",
    "                 num_classes=1000, embed_dim=768, depth=12, num_heads=12):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(0.1)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads) for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        attention_maps = []\n",
    "        for block in self.blocks:\n",
    "            x, attn = block(x)\n",
    "            attention_maps.append(attn)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = x[:, 0]  # Use CLS token\n",
    "        x = self.head(x)\n",
    "        return x, attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small ViT and visualize patches\n",
    "vit = VisionTransformer(\n",
    "    img_size=32, \n",
    "    patch_size=4, \n",
    "    in_channels=3, \n",
    "    num_classes=10, \n",
    "    embed_dim=192, \n",
    "    depth=6, \n",
    "    num_heads=6\n",
    ").to(device)\n",
    "\n",
    "# Test with random image\n",
    "img = torch.randn(1, 3, 32, 32).to(device)\n",
    "output, attention_maps = vit(img)\n",
    "print(f\"ViT output shape: {output.shape}\")\n",
    "print(f\"ViT parameters: {sum(p.numel() for p in vit.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "# Visualize how image is divided into patches\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Original image (using random data for visualization)\n",
    "img_vis = img[0].cpu().permute(1, 2, 0).numpy()\n",
    "img_vis = (img_vis - img_vis.min()) / (img_vis.max() - img_vis.min())\n",
    "axes[0].imshow(img_vis)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Show patch grid\n",
    "axes[1].imshow(img_vis)\n",
    "patch_size = 4\n",
    "for i in range(0, 32, patch_size):\n",
    "    axes[1].axhline(i, color='red', linewidth=0.5)\n",
    "    axes[1].axvline(i, color='red', linewidth=0.5)\n",
    "axes[1].set_title('Image Patches (4x4)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EfficientNet Architecture\n",
    "\n",
    "EfficientNet uses compound scaling to efficiently scale network width, depth, and resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcite(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation block\"\"\"\n",
    "    def __init__(self, in_channels, squeeze_ratio=0.25):\n",
    "        super().__init__()\n",
    "        squeeze_channels = max(1, int(in_channels * squeeze_ratio))\n",
    "        self.fc1 = nn.Conv2d(in_channels, squeeze_channels, 1)\n",
    "        self.fc2 = nn.Conv2d(squeeze_channels, in_channels, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        scale = x.mean((2, 3), keepdim=True)\n",
    "        scale = F.relu(self.fc1(scale))\n",
    "        scale = torch.sigmoid(self.fc2(scale))\n",
    "        return x * scale\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"Mobile Inverted Bottleneck Convolution Block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, expand_ratio, stride, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.use_residual = stride == 1 and in_channels == out_channels\n",
    "        \n",
    "        hidden_channels = in_channels * expand_ratio\n",
    "        \n",
    "        layers = []\n",
    "        # Expansion\n",
    "        if expand_ratio != 1:\n",
    "            layers.extend([\n",
    "                nn.Conv2d(in_channels, hidden_channels, 1, bias=False),\n",
    "                nn.BatchNorm2d(hidden_channels),\n",
    "                nn.SiLU()\n",
    "            ])\n",
    "        \n",
    "        # Depthwise conv\n",
    "        layers.extend([\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size, \n",
    "                     stride=stride, padding=kernel_size//2, groups=hidden_channels, bias=False),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.SiLU(),\n",
    "            SqueezeExcite(hidden_channels)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        layers.extend([\n",
    "            nn.Conv2d(hidden_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        ])\n",
    "        \n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.use_residual:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate compound scaling\n",
    "def calculate_scaled_params(width_mult, depth_mult, resolution_mult):\n",
    "    \"\"\"Calculate parameters after compound scaling\"\"\"\n",
    "    base_width = 1.0\n",
    "    base_depth = 1.0\n",
    "    base_resolution = 224\n",
    "    \n",
    "    scaled_width = base_width * width_mult\n",
    "    scaled_depth = base_depth * depth_mult\n",
    "    scaled_resolution = int(base_resolution * resolution_mult)\n",
    "    \n",
    "    # Approximate parameter count (simplified)\n",
    "    params = scaled_width * scaled_depth * (scaled_resolution / base_resolution) ** 2\n",
    "    \n",
    "    return {\n",
    "        'width': scaled_width,\n",
    "        'depth': scaled_depth,\n",
    "        'resolution': scaled_resolution,\n",
    "        'relative_params': params\n",
    "    }\n",
    "\n",
    "# EfficientNet scaling coefficients\n",
    "efficientnet_configs = {\n",
    "    'B0': (1.0, 1.0, 1.0),\n",
    "    'B1': (1.0, 1.1, 1.15),\n",
    "    'B2': (1.1, 1.2, 1.3),\n",
    "    'B3': (1.2, 1.4, 1.5),\n",
    "    'B4': (1.4, 1.8, 1.8),\n",
    "    'B5': (1.6, 2.2, 2.1),\n",
    "    'B6': (1.8, 2.6, 2.4),\n",
    "    'B7': (2.0, 3.1, 2.7)\n",
    "}\n",
    "\n",
    "print(\"EfficientNet Compound Scaling:\")\n",
    "print(\"-\" * 60)\n",
    "for name, (w, d, r) in efficientnet_configs.items():\n",
    "    config = calculate_scaled_params(w, d, r)\n",
    "    print(f\"{name}: Width={config['width']:.1f}x, Depth={config['depth']:.1f}x, \"\n",
    "          f\"Resolution={config['resolution']}, RelativeParams={config['relative_params']:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Ordinary Differential Equations (Neural ODEs)\n",
    "\n",
    "Neural ODEs parameterize the continuous dynamics of hidden states using neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(nn.Module):\n",
    "    \"\"\"ODE function for Neural ODE\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, t, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class NeuralODE(nn.Module):\n",
    "    \"\"\"Neural ODE Block with different solvers\"\"\"\n",
    "    def __init__(self, func, solver='euler', step_size=0.1):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        self.solver = solver\n",
    "        self.step_size = step_size\n",
    "        \n",
    "    def euler_solve(self, x, t_span):\n",
    "        \"\"\"Euler method solver\"\"\"\n",
    "        t0, t1 = t_span\n",
    "        num_steps = int((t1 - t0) / self.step_size)\n",
    "        \n",
    "        h = self.step_size\n",
    "        for _ in range(num_steps):\n",
    "            x = x + h * self.func(t0, x)\n",
    "            t0 += h\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def rk4_solve(self, x, t_span):\n",
    "        \"\"\"4th order Runge-Kutta solver\"\"\"\n",
    "        t0, t1 = t_span\n",
    "        num_steps = int((t1 - t0) / self.step_size)\n",
    "        \n",
    "        h = self.step_size\n",
    "        for _ in range(num_steps):\n",
    "            k1 = self.func(t0, x)\n",
    "            k2 = self.func(t0 + h/2, x + h*k1/2)\n",
    "            k3 = self.func(t0 + h/2, x + h*k2/2)\n",
    "            k4 = self.func(t0 + h, x + h*k3)\n",
    "            \n",
    "            x = x + h * (k1 + 2*k2 + 2*k3 + k4) / 6\n",
    "            t0 += h\n",
    "            \n",
    "        return x\n",
    "        \n",
    "    def forward(self, x, t_span):\n",
    "        if self.solver == 'euler':\n",
    "            return self.euler_solve(x, t_span)\n",
    "        elif self.solver == 'rk4':\n",
    "            return self.rk4_solve(x, t_span)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown solver: {self.solver}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Neural ODE dynamics\n",
    "dim = 2\n",
    "ode_func = ODEFunc(dim)\n",
    "neural_ode = NeuralODE(ode_func, solver='rk4', step_size=0.01)\n",
    "\n",
    "# Create a grid of initial points\n",
    "n_points = 20\n",
    "x_range = torch.linspace(-2, 2, n_points)\n",
    "y_range = torch.linspace(-2, 2, n_points)\n",
    "grid_x, grid_y = torch.meshgrid(x_range, y_range)\n",
    "initial_points = torch.stack([grid_x.flatten(), grid_y.flatten()], dim=1)\n",
    "\n",
    "# Solve ODE for each point\n",
    "t_span = (0.0, 1.0)\n",
    "trajectories = []\n",
    "\n",
    "for i in range(initial_points.shape[0]):\n",
    "    point = initial_points[i:i+1]\n",
    "    trajectory = [point.clone()]\n",
    "    \n",
    "    # Solve with smaller time steps for visualization\n",
    "    num_vis_steps = 10\n",
    "    for j in range(num_vis_steps):\n",
    "        t0 = j / num_vis_steps\n",
    "        t1 = (j + 1) / num_vis_steps\n",
    "        point = neural_ode(point, (t0, t1))\n",
    "        trajectory.append(point.clone())\n",
    "    \n",
    "    trajectories.append(torch.cat(trajectory, dim=0))\n",
    "\n",
    "# Visualize vector field\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot trajectories\n",
    "for traj in trajectories[::20]:  # Plot every 20th trajectory for clarity\n",
    "    traj_np = traj.detach().numpy()\n",
    "    plt.plot(traj_np[:, 0], traj_np[:, 1], 'b-', alpha=0.5, linewidth=0.5)\n",
    "    plt.plot(traj_np[0, 0], traj_np[0, 1], 'go', markersize=3)  # Start\n",
    "    plt.plot(traj_np[-1, 0], traj_np[-1, 1], 'ro', markersize=3)  # End\n",
    "\n",
    "plt.title('Neural ODE Dynamics')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multimodal Architecture\n",
    "\n",
    "Modern AI systems often need to process multiple modalities (vision, language, audio) together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalTransformer(nn.Module):\n",
    "    \"\"\"Simple multimodal transformer for vision and language\"\"\"\n",
    "    def __init__(self, vocab_size, max_seq_len, img_size=224, patch_size=16,\n",
    "                 embed_dim=512, depth=6, num_heads=8, num_classes=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Vision encoder\n",
    "        self.vision_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)\n",
    "        num_patches = self.vision_embed.num_patches\n",
    "        \n",
    "        # Language encoder\n",
    "        self.text_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.text_pos_embed = nn.Parameter(torch.zeros(1, max_seq_len, embed_dim))\n",
    "        \n",
    "        # Modality embeddings\n",
    "        self.vision_type_embed = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.text_type_embed = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Shared transformer\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads) for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, images=None, text_ids=None, return_embeddings=False):\n",
    "        embeddings = []\n",
    "        \n",
    "        # Process vision input\n",
    "        if images is not None:\n",
    "            vision_embeds = self.vision_embed(images)\n",
    "            vision_embeds = vision_embeds + self.vision_type_embed\n",
    "            embeddings.append(vision_embeds)\n",
    "        \n",
    "        # Process text input\n",
    "        if text_ids is not None:\n",
    "            text_embeds = self.text_embed(text_ids)\n",
    "            seq_len = text_embeds.size(1)\n",
    "            text_embeds = text_embeds + self.text_pos_embed[:, :seq_len]\n",
    "            text_embeds = text_embeds + self.text_type_embed\n",
    "            embeddings.append(text_embeds)\n",
    "        \n",
    "        # Concatenate modalities\n",
    "        x = torch.cat(embeddings, dim=1)\n",
    "        \n",
    "        # Apply transformer\n",
    "        attention_maps = []\n",
    "        for block in self.blocks:\n",
    "            x, attn = block(x)\n",
    "            attention_maps.append(attn)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            return x, attention_maps\n",
    "        \n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multimodal model and visualize cross-modal attention\n",
    "multimodal = MultimodalTransformer(\n",
    "    vocab_size=10000, \n",
    "    max_seq_len=128, \n",
    "    img_size=32, \n",
    "    patch_size=4,\n",
    "    embed_dim=256, \n",
    "    depth=4, \n",
    "    num_classes=100\n",
    ")\n",
    "\n",
    "# Test with both modalities\n",
    "images = torch.randn(2, 3, 32, 32)\n",
    "text_ids = torch.randint(0, 10000, (2, 20))\n",
    "embeddings, attention_maps = multimodal(images=images, text_ids=text_ids, return_embeddings=True)\n",
    "\n",
    "print(f\"Multimodal embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Number of patches: {(32//4)**2}\")\n",
    "print(f\"Number of text tokens: 20\")\n",
    "print(f\"Total sequence length: {embeddings.shape[1]}\")\n",
    "\n",
    "# Visualize cross-modal attention\n",
    "if len(attention_maps) > 0:\n",
    "    # Get attention from last layer, first head\n",
    "    attn = attention_maps[-1][0, 0].detach().cpu().numpy()\n",
    "    \n",
    "    # Split attention into vision-vision, vision-text, text-vision, text-text\n",
    "    n_patches = 64  # 8x8 patches\n",
    "    n_text = 20\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Vision to Vision\n",
    "    axes[0, 0].imshow(attn[:n_patches, :n_patches], cmap='hot')\n",
    "    axes[0, 0].set_title('Vision → Vision')\n",
    "    axes[0, 0].set_xlabel('Vision Patches')\n",
    "    axes[0, 0].set_ylabel('Vision Patches')\n",
    "    \n",
    "    # Vision to Text\n",
    "    axes[0, 1].imshow(attn[:n_patches, n_patches:], cmap='hot')\n",
    "    axes[0, 1].set_title('Vision → Text')\n",
    "    axes[0, 1].set_xlabel('Text Tokens')\n",
    "    axes[0, 1].set_ylabel('Vision Patches')\n",
    "    \n",
    "    # Text to Vision\n",
    "    axes[1, 0].imshow(attn[n_patches:, :n_patches], cmap='hot')\n",
    "    axes[1, 0].set_title('Text → Vision')\n",
    "    axes[1, 0].set_xlabel('Vision Patches')\n",
    "    axes[1, 0].set_ylabel('Text Tokens')\n",
    "    \n",
    "    # Text to Text\n",
    "    axes[1, 1].imshow(attn[n_patches:, n_patches:], cmap='hot')\n",
    "    axes[1, 1].set_title('Text → Text')\n",
    "    axes[1, 1].set_xlabel('Text Tokens')\n",
    "    axes[1, 1].set_ylabel('Text Tokens')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Comparison\n",
    "\n",
    "Let's compare the different architectures we've implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model sizes and theoretical properties\n",
    "architectures = {\n",
    "    \"GCN\": (gcn, \"Graph-structured data\"),\n",
    "    \"GAT\": (gat, \"Graph data with attention\"),\n",
    "    \"Vision Transformer\": (vit, \"Image classification\"),\n",
    "    \"Neural ODE\": (neural_ode, \"Continuous dynamics\"),\n",
    "    \"Multimodal\": (multimodal, \"Multi-input processing\")\n",
    "}\n",
    "\n",
    "print(\"Model Architecture Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Architecture':<20} {'Parameters':<15} {'Best Use Case':<35}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, (model, use_case) in architectures.items():\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name:<20} {params/1e6:>8.2f}M      {use_case:<35}\")\n",
    "\n",
    "# Plot parameter counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "names = list(architectures.keys())\n",
    "params = [sum(p.numel() for p in architectures[name][0].parameters())/1e6 for name in names]\n",
    "\n",
    "bars = plt.bar(names, params, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])\n",
    "plt.ylabel('Parameters (Millions)')\n",
    "plt.title('Model Size Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, param in zip(bars, params):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{param:.2f}M', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we explored several advanced model architectures:\n",
    "\n",
    "1. **Graph Neural Networks (GCN, GAT)**: Process graph-structured data by aggregating information from neighboring nodes\n",
    "2. **Vision Transformers (ViT)**: Apply self-attention to image patches for powerful visual understanding\n",
    "3. **EfficientNet**: Use compound scaling to balance model size and accuracy\n",
    "4. **Neural ODEs**: Model continuous dynamics with neural networks\n",
    "5. **Multimodal Architectures**: Process multiple input modalities simultaneously\n",
    "\n",
    "### Key Takeaways:\n",
    "- Different architectures excel at different tasks\n",
    "- Attention mechanisms are powerful across domains\n",
    "- Efficient scaling is crucial for practical deployment\n",
    "- Multimodal learning is increasingly important\n",
    "- Choose architecture based on your data structure and task requirements\n",
    "\n",
    "These architectures represent the cutting edge of deep learning and continue to push the boundaries of what's possible with neural networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}