{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 19: Neural Architecture Search\n",
    "\n",
    "This tutorial explores Neural Architecture Search (NAS) techniques for automatically designing optimal neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import networkx as nx\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Search Space Design\n",
    "\n",
    "The first step in NAS is defining the search space - the set of possible architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic operations for search space\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Basic convolutional block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    \"\"\"Depthwise separable convolution\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, \n",
    "                                  stride, padding, groups=in_channels)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.depthwise(x)))\n",
    "        x = self.bn2(self.pointwise(x))\n",
    "        return self.relu(x)\n",
    "\n",
    "class IdentityBlock(nn.Module):\n",
    "    \"\"\"Identity/skip connection\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "# Define search space\n",
    "PRIMITIVES = {\n",
    "    'conv3x3': lambda c: ConvBlock(c, c, 3),\n",
    "    'conv5x5': lambda c: ConvBlock(c, c, 5),\n",
    "    'dw_conv3x3': lambda c: DepthwiseSeparableConv(c, c, 3),\n",
    "    'dw_conv5x5': lambda c: DepthwiseSeparableConv(c, c, 5),\n",
    "    'max_pool3x3': lambda c: nn.MaxPool2d(3, stride=1, padding=1),\n",
    "    'avg_pool3x3': lambda c: nn.AvgPool2d(3, stride=1, padding=1),\n",
    "    'identity': lambda c: IdentityBlock(c)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize search space\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Operation properties\n",
    "op_properties = {\n",
    "    'conv3x3': {'params': 9, 'flops': 9, 'type': 'conv'},\n",
    "    'conv5x5': {'params': 25, 'flops': 25, 'type': 'conv'},\n",
    "    'dw_conv3x3': {'params': 3, 'flops': 3, 'type': 'conv'},\n",
    "    'dw_conv5x5': {'params': 5, 'flops': 5, 'type': 'conv'},\n",
    "    'max_pool3x3': {'params': 0, 'flops': 1, 'type': 'pool'},\n",
    "    'avg_pool3x3': {'params': 0, 'flops': 1, 'type': 'pool'},\n",
    "    'identity': {'params': 0, 'flops': 0, 'type': 'skip'}\n",
    "}\n",
    "\n",
    "ops = list(PRIMITIVES.keys())\n",
    "params = [op_properties[op]['params'] for op in ops]\n",
    "flops = [op_properties[op]['flops'] for op in ops]\n",
    "colors = ['#3498db' if op_properties[op]['type'] == 'conv' else \n",
    "          '#2ecc71' if op_properties[op]['type'] == 'pool' else \n",
    "          '#e74c3c' for op in ops]\n",
    "\n",
    "scatter = ax.scatter(params, flops, s=200, c=colors, alpha=0.7, edgecolors='black', linewidth=2)\n",
    "\n",
    "for i, op in enumerate(ops):\n",
    "    ax.annotate(op, (params[i], flops[i]), xytext=(5, 5), \n",
    "                textcoords='offset points', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Relative Parameters', fontsize=12)\n",
    "ax.set_ylabel('Relative FLOPs', fontsize=12)\n",
    "ax.set_title('Search Space Operations', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='#3498db', label='Convolution'),\n",
    "                   Patch(facecolor='#2ecc71', label='Pooling'),\n",
    "                   Patch(facecolor='#e74c3c', label='Skip')]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture Representation\n",
    "\n",
    "We need a way to represent and manipulate architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Architecture:\n",
    "    \"\"\"Represents a neural architecture as a list of operations\"\"\"\n",
    "    def __init__(self, layers: List[str]):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Architecture({' -> '.join(self.layers)})\"\n",
    "    \n",
    "    def mutate(self, mutation_prob=0.1):\n",
    "        \"\"\"Random mutation for evolutionary search\"\"\"\n",
    "        new_layers = self.layers.copy()\n",
    "        for i in range(len(new_layers)):\n",
    "            if random.random() < mutation_prob:\n",
    "                new_layers[i] = random.choice(list(PRIMITIVES.keys()))\n",
    "        return Architecture(new_layers)\n",
    "    \n",
    "    def crossover(self, other: 'Architecture'):\n",
    "        \"\"\"Crossover for evolutionary search\"\"\"\n",
    "        crossover_point = random.randint(1, len(self.layers) - 1)\n",
    "        new_layers = self.layers[:crossover_point] + other.layers[crossover_point:]\n",
    "        return Architecture(new_layers)\n",
    "    \n",
    "    def visualize(self):\n",
    "        \"\"\"Visualize architecture as a graph\"\"\"\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes\n",
    "        G.add_node('input', label='Input')\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            G.add_node(f'layer_{i}', label=layer)\n",
    "        G.add_node('output', label='Output')\n",
    "        \n",
    "        # Add edges\n",
    "        G.add_edge('input', 'layer_0')\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            G.add_edge(f'layer_{i}', f'layer_{i+1}')\n",
    "        G.add_edge(f'layer_{len(self.layers)-1}', 'output')\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "        # Draw nodes with different colors\n",
    "        node_colors = ['lightblue' if 'layer' in node else 'lightgreen' for node in G.nodes()]\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=1500)\n",
    "        nx.draw_networkx_edges(G, pos, edge_color='gray', arrows=True, arrowsize=20)\n",
    "        \n",
    "        # Draw labels\n",
    "        labels = {node: data['label'] for node, data in G.nodes(data=True)}\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=10)\n",
    "        \n",
    "        plt.title('Architecture Visualization')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example architectures\n",
    "arch1 = Architecture(['conv3x3', 'max_pool3x3', 'conv5x5', 'identity'])\n",
    "arch2 = Architecture(['dw_conv3x3', 'avg_pool3x3', 'dw_conv5x5', 'conv3x3'])\n",
    "\n",
    "print(\"Architecture 1:\")\n",
    "print(arch1)\n",
    "arch1.visualize()\n",
    "\n",
    "print(\"\\nMutation and Crossover Examples:\")\n",
    "mutated = arch1.mutate(0.5)\n",
    "print(f\"Mutated: {mutated}\")\n",
    "\n",
    "crossed = arch1.crossover(arch2)\n",
    "print(f\"Crossover: {crossed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Search\n",
    "\n",
    "The simplest search strategy is random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NASModel(nn.Module):\n",
    "    \"\"\"Model that can be built from architecture description\"\"\"\n",
    "    def __init__(self, architecture: Architecture, input_channels=3, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.architecture = architecture\n",
    "        \n",
    "        # Build layers from architecture\n",
    "        layers = []\n",
    "        channels = 32  # Starting channels\n",
    "        \n",
    "        # Initial conv\n",
    "        layers.append(ConvBlock(input_channels, channels, 3))\n",
    "        \n",
    "        # Architecture-defined layers\n",
    "        for op_name in architecture.layers:\n",
    "            layers.append(PRIMITIVES[op_name](channels))\n",
    "        \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Linear(channels, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def evaluate_architecture(architecture: Architecture, num_epochs=5):\n",
    "    \"\"\"Quick evaluation of an architecture\"\"\"\n",
    "    model = NASModel(architecture).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Simulate training (in practice, you'd train on real data)\n",
    "    # Here we use a simple heuristic based on architecture properties\n",
    "    base_accuracy = 0.7\n",
    "    \n",
    "    # Bonus for good operations\n",
    "    for op in architecture.layers:\n",
    "        if 'conv' in op:\n",
    "            base_accuracy += 0.02\n",
    "        elif op == 'identity':\n",
    "            base_accuracy += 0.01\n",
    "    \n",
    "    # Penalty for too many parameters\n",
    "    if num_params > 1e6:\n",
    "        base_accuracy -= 0.1\n",
    "    \n",
    "    # Add some randomness\n",
    "    accuracy = np.clip(base_accuracy + np.random.normal(0, 0.05), 0, 1)\n",
    "    latency = num_params / 1e6 * np.random.uniform(0.8, 1.2)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'params': num_params,\n",
    "        'latency': latency,\n",
    "        'architecture': architecture\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search\n",
    "def random_search(num_architectures=50, layers_per_arch=4):\n",
    "    \"\"\"Random architecture search\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(\"Running Random Search...\")\n",
    "    for i in range(num_architectures):\n",
    "        # Generate random architecture\n",
    "        layers = [random.choice(list(PRIMITIVES.keys())) for _ in range(layers_per_arch)]\n",
    "        arch = Architecture(layers)\n",
    "        \n",
    "        # Evaluate\n",
    "        result = evaluate_architecture(arch)\n",
    "        results.append(result)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Evaluated {i+1}/{num_architectures} architectures\")\n",
    "    \n",
    "    # Sort by accuracy\n",
    "    results.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run random search\n",
    "random_results = random_search(num_architectures=50)\n",
    "\n",
    "print(\"\\nTop 5 Architectures (Random Search):\")\n",
    "for i, result in enumerate(random_results[:5]):\n",
    "    print(f\"{i+1}. Accuracy: {result['accuracy']:.3f}, \"\n",
    "          f\"Params: {result['params']/1e6:.2f}M, \"\n",
    "          f\"Arch: {result['architecture'].layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evolutionary Search\n",
    "\n",
    "Evolutionary algorithms can be more efficient than random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolutionarySearch:\n",
    "    \"\"\"Evolutionary algorithm for NAS\"\"\"\n",
    "    def __init__(self, population_size=20, mutation_prob=0.1, layers_per_arch=4):\n",
    "        self.population_size = population_size\n",
    "        self.mutation_prob = mutation_prob\n",
    "        self.layers_per_arch = layers_per_arch\n",
    "        \n",
    "    def initialize_population(self):\n",
    "        \"\"\"Create initial random population\"\"\"\n",
    "        population = []\n",
    "        for _ in range(self.population_size):\n",
    "            layers = [random.choice(list(PRIMITIVES.keys())) \n",
    "                     for _ in range(self.layers_per_arch)]\n",
    "            population.append(Architecture(layers))\n",
    "        return population\n",
    "    \n",
    "    def evaluate_population(self, population):\n",
    "        \"\"\"Evaluate all architectures in population\"\"\"\n",
    "        results = []\n",
    "        for arch in population:\n",
    "            result = evaluate_architecture(arch)\n",
    "            results.append(result)\n",
    "        return results\n",
    "    \n",
    "    def select_parents(self, population_results, num_parents):\n",
    "        \"\"\"Tournament selection\"\"\"\n",
    "        parents = []\n",
    "        for _ in range(num_parents):\n",
    "            # Tournament of size 3\n",
    "            tournament = random.sample(population_results, 3)\n",
    "            winner = max(tournament, key=lambda x: x['accuracy'])\n",
    "            parents.append(winner['architecture'])\n",
    "        return parents\n",
    "    \n",
    "    def evolve(self, num_generations=30):\n",
    "        \"\"\"Run evolutionary search\"\"\"\n",
    "        # Initialize\n",
    "        population = self.initialize_population()\n",
    "        history = {'best_accuracy': [], 'avg_accuracy': [], 'diversity': []}\n",
    "        all_results = []\n",
    "        \n",
    "        print(\"Running Evolutionary Search...\")\n",
    "        for generation in range(num_generations):\n",
    "            # Evaluate current population\n",
    "            results = self.evaluate_population(population)\n",
    "            all_results.extend(results)\n",
    "            \n",
    "            # Track statistics\n",
    "            accuracies = [r['accuracy'] for r in results]\n",
    "            history['best_accuracy'].append(max(accuracies))\n",
    "            history['avg_accuracy'].append(np.mean(accuracies))\n",
    "            \n",
    "            # Calculate diversity (unique architectures)\n",
    "            unique_archs = len(set(str(r['architecture'].layers) for r in results))\n",
    "            history['diversity'].append(unique_archs / len(results))\n",
    "            \n",
    "            # Select parents\n",
    "            num_parents = self.population_size // 2\n",
    "            parents = self.select_parents(results, num_parents)\n",
    "            \n",
    "            # Generate offspring\n",
    "            offspring = []\n",
    "            while len(offspring) < self.population_size:\n",
    "                if random.random() < 0.5 and len(parents) >= 2:\n",
    "                    # Crossover\n",
    "                    p1, p2 = random.sample(parents, 2)\n",
    "                    child = p1.crossover(p2)\n",
    "                else:\n",
    "                    # Mutation\n",
    "                    parent = random.choice(parents)\n",
    "                    child = parent.mutate(self.mutation_prob)\n",
    "                offspring.append(child)\n",
    "            \n",
    "            # Replace population\n",
    "            population = offspring\n",
    "            \n",
    "            if (generation + 1) % 5 == 0:\n",
    "                print(f\"Generation {generation + 1}: \"\n",
    "                      f\"Best Acc = {history['best_accuracy'][-1]:.3f}, \"\n",
    "                      f\"Avg Acc = {history['avg_accuracy'][-1]:.3f}, \"\n",
    "                      f\"Diversity = {history['diversity'][-1]:.2f}\")\n",
    "        \n",
    "        return all_results, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evolutionary search\n",
    "evo_search = EvolutionarySearch(population_size=30, mutation_prob=0.2)\n",
    "evo_results, evo_history = evo_search.evolve(num_generations=25)\n",
    "\n",
    "# Sort results\n",
    "evo_results.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "\n",
    "print(\"\\nTop 5 Architectures (Evolutionary Search):\")\n",
    "for i, result in enumerate(evo_results[:5]):\n",
    "    print(f\"{i+1}. Accuracy: {result['accuracy']:.3f}, \"\n",
    "          f\"Params: {result['params']/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evolutionary search progress\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Accuracy over generations\n",
    "axes[0].plot(evo_history['best_accuracy'], label='Best', linewidth=2)\n",
    "axes[0].plot(evo_history['avg_accuracy'], label='Average', linewidth=2)\n",
    "axes[0].fill_between(range(len(evo_history['best_accuracy'])),\n",
    "                     evo_history['avg_accuracy'], \n",
    "                     evo_history['best_accuracy'],\n",
    "                     alpha=0.3)\n",
    "axes[0].set_xlabel('Generation')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Evolutionary Search Progress')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Diversity\n",
    "axes[1].plot(evo_history['diversity'], linewidth=2, color='green')\n",
    "axes[1].set_xlabel('Generation')\n",
    "axes[1].set_ylabel('Population Diversity')\n",
    "axes[1].set_title('Genetic Diversity Over Time')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Compare random vs evolutionary\n",
    "random_accs = [r['accuracy'] for r in random_results]\n",
    "evo_accs = [r['accuracy'] for r in evo_results[:len(random_results)]]\n",
    "\n",
    "axes[2].hist([random_accs, evo_accs], bins=15, label=['Random', 'Evolutionary'], alpha=0.7)\n",
    "axes[2].set_xlabel('Accuracy')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_title('Search Strategy Comparison')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Differentiable Architecture Search (DARTS)\n",
    "\n",
    "DARTS makes the architecture search differentiable by using continuous relaxation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedOperation(nn.Module):\n",
    "    \"\"\"Mixed operation for DARTS\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.ops = nn.ModuleList([\n",
    "            op(channels) for op in PRIMITIVES.values()\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, weights):\n",
    "        \"\"\"weights: softmax over operations\"\"\"\n",
    "        return sum(w * op(x) for w, op in zip(weights, self.ops))\n",
    "\n",
    "class DARTSCell(nn.Module):\n",
    "    \"\"\"DARTS cell with learnable architecture\"\"\"\n",
    "    def __init__(self, channels, num_nodes=4):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        \n",
    "        # Mixed operations for each edge\n",
    "        self.ops = nn.ModuleList()\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i):\n",
    "                self.ops.append(MixedOperation(channels))\n",
    "        \n",
    "        # Architecture parameters (to be learned)\n",
    "        num_edges = len(self.ops)\n",
    "        self.arch_params = nn.Parameter(\n",
    "            torch.randn(num_edges, len(PRIMITIVES)) / 10\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        states = [x]\n",
    "        offset = 0\n",
    "        \n",
    "        # Compute intermediate nodes\n",
    "        for i in range(1, self.num_nodes):\n",
    "            s = []\n",
    "            for j in range(i):\n",
    "                weights = F.softmax(self.arch_params[offset], dim=0)\n",
    "                s.append(self.ops[offset](states[j], weights))\n",
    "                offset += 1\n",
    "            states.append(sum(s))\n",
    "        \n",
    "        # Output is concatenation of intermediate nodes\n",
    "        return torch.cat(states[1:], dim=1)\n",
    "    \n",
    "    def get_genotype(self):\n",
    "        \"\"\"Extract discrete architecture\"\"\"\n",
    "        gene = []\n",
    "        offset = 0\n",
    "        \n",
    "        for i in range(1, self.num_nodes):\n",
    "            edges = []\n",
    "            for j in range(i):\n",
    "                weights = F.softmax(self.arch_params[offset], dim=0)\n",
    "                op_idx = weights.argmax().item()\n",
    "                op_name = list(PRIMITIVES.keys())[op_idx]\n",
    "                edges.append((op_name, j, weights[op_idx].item()))\n",
    "                offset += 1\n",
    "            \n",
    "            # Select top 2 edges\n",
    "            edges.sort(key=lambda x: x[2], reverse=True)\n",
    "            gene.extend([(e[0], e[1]) for e in edges[:2]])\n",
    "        \n",
    "        return gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DARTS architecture weights\n",
    "darts_cell = DARTSCell(32, num_nodes=4)\n",
    "\n",
    "# Get architecture weights\n",
    "arch_weights = F.softmax(darts_cell.arch_params, dim=1).detach().numpy()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(arch_weights.T, \n",
    "            xticklabels=[f'Edge {i}' for i in range(arch_weights.shape[0])],\n",
    "            yticklabels=list(PRIMITIVES.keys()),\n",
    "            cmap='YlOrRd',\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            cbar_kws={'label': 'Operation Weight'})\n",
    "\n",
    "plt.xlabel('Cell Edges')\n",
    "plt.ylabel('Operations')\n",
    "plt.title('DARTS Architecture Weights (Before Training)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Extract genotype\n",
    "genotype = darts_cell.get_genotype()\n",
    "print(\"\\nExtracted Genotype:\")\n",
    "for i, (op, node) in enumerate(genotype):\n",
    "    print(f\"Edge {i}: {op} from node {node}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Objective NAS\n",
    "\n",
    "Often we need to optimize for multiple objectives like accuracy, latency, and model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_frontier(results, objectives=['accuracy', 'latency']):\n",
    "    \"\"\"Find Pareto frontier for multi-objective optimization\"\"\"\n",
    "    pareto_front = []\n",
    "    \n",
    "    for candidate in results:\n",
    "        dominated = False\n",
    "        \n",
    "        for other in results:\n",
    "            if other == candidate:\n",
    "                continue\n",
    "                \n",
    "            # Check if candidate is dominated\n",
    "            better_in_all = all(\n",
    "                other[obj] >= candidate[obj] if obj == 'accuracy' \n",
    "                else other[obj] <= candidate[obj]\n",
    "                for obj in objectives\n",
    "            )\n",
    "            better_in_one = any(\n",
    "                other[obj] > candidate[obj] if obj == 'accuracy'\n",
    "                else other[obj] < candidate[obj]\n",
    "                for obj in objectives\n",
    "            )\n",
    "            \n",
    "            if better_in_all and better_in_one:\n",
    "                dominated = True\n",
    "                break\n",
    "        \n",
    "        if not dominated:\n",
    "            pareto_front.append(candidate)\n",
    "    \n",
    "    return pareto_front\n",
    "\n",
    "# Combine all results\n",
    "all_results = random_results + evo_results\n",
    "\n",
    "# Find Pareto optimal architectures\n",
    "pareto_architectures = pareto_frontier(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multi-objective optimization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Accuracy vs Latency\n",
    "all_acc = [r['accuracy'] for r in all_results]\n",
    "all_lat = [r['latency'] for r in all_results]\n",
    "pareto_acc = [r['accuracy'] for r in pareto_architectures]\n",
    "pareto_lat = [r['latency'] for r in pareto_architectures]\n",
    "\n",
    "axes[0].scatter(all_lat, all_acc, alpha=0.4, label='All architectures', s=50)\n",
    "axes[0].scatter(pareto_lat, pareto_acc, color='red', s=100, \n",
    "               label='Pareto optimal', edgecolors='black', linewidth=2)\n",
    "\n",
    "# Connect Pareto front\n",
    "pareto_sorted = sorted(zip(pareto_lat, pareto_acc))\n",
    "if pareto_sorted:\n",
    "    pareto_lat_sorted, pareto_acc_sorted = zip(*pareto_sorted)\n",
    "    axes[0].plot(pareto_lat_sorted, pareto_acc_sorted, 'r--', alpha=0.5)\n",
    "\n",
    "axes[0].set_xlabel('Latency (ms)')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy vs Latency Trade-off')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy vs Parameters\n",
    "all_params = [r['params']/1e6 for r in all_results]\n",
    "pareto_params = [r['params']/1e6 for r in pareto_architectures]\n",
    "\n",
    "axes[1].scatter(all_params, all_acc, alpha=0.4, label='All architectures', s=50)\n",
    "axes[1].scatter(pareto_params, pareto_acc, color='red', s=100, \n",
    "               label='Pareto optimal', edgecolors='black', linewidth=2)\n",
    "\n",
    "axes[1].set_xlabel('Parameters (M)')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy vs Model Size Trade-off')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Found {len(pareto_architectures)} Pareto optimal architectures out of {len(all_results)} total\")\n",
    "print(\"\\nTop Pareto Architectures:\")\n",
    "for i, arch in enumerate(pareto_architectures[:5]):\n",
    "    print(f\"{i+1}. Acc: {arch['accuracy']:.3f}, Latency: {arch['latency']:.2f}ms, \"\n",
    "          f\"Params: {arch['params']/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Architecture Analysis\n",
    "\n",
    "Let's analyze what makes good architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze operation frequency in top architectures\n",
    "top_n = 20\n",
    "top_architectures = sorted(all_results, key=lambda x: x['accuracy'], reverse=True)[:top_n]\n",
    "\n",
    "# Count operations\n",
    "op_counts = defaultdict(int)\n",
    "op_positions = defaultdict(list)\n",
    "\n",
    "for result in top_architectures:\n",
    "    for pos, op in enumerate(result['architecture'].layers):\n",
    "        op_counts[op] += 1\n",
    "        op_positions[op].append(pos)\n",
    "\n",
    "# Visualize operation analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Operation frequency\n",
    "ops = list(op_counts.keys())\n",
    "counts = list(op_counts.values())\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(ops)))\n",
    "\n",
    "bars = axes[0].bar(ops, counts, color=colors)\n",
    "axes[0].set_xlabel('Operation')\n",
    "axes[0].set_ylabel('Frequency in Top Architectures')\n",
    "axes[0].set_title(f'Operation Popularity (Top {top_n} Architectures)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, counts):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                str(count), ha='center', va='bottom')\n",
    "\n",
    "# Operation position preference\n",
    "position_data = []\n",
    "for op in ops:\n",
    "    if op_positions[op]:\n",
    "        position_data.append(op_positions[op])\n",
    "\n",
    "axes[1].boxplot(position_data, labels=ops)\n",
    "axes[1].set_xlabel('Operation')\n",
    "axes[1].set_ylabel('Position in Architecture')\n",
    "axes[1].set_title('Operation Position Preferences')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hardware-Aware NAS\n",
    "\n",
    "Real-world NAS often needs to consider specific hardware constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware performance lookup table (simulated)\n",
    "HARDWARE_LATENCY = {\n",
    "    'mobile': {\n",
    "        'conv3x3': 2.0,\n",
    "        'conv5x5': 5.0,\n",
    "        'dw_conv3x3': 1.0,\n",
    "        'dw_conv5x5': 2.0,\n",
    "        'max_pool3x3': 0.5,\n",
    "        'avg_pool3x3': 0.5,\n",
    "        'identity': 0.1\n",
    "    },\n",
    "    'gpu': {\n",
    "        'conv3x3': 0.5,\n",
    "        'conv5x5': 0.8,\n",
    "        'dw_conv3x3': 0.6,\n",
    "        'dw_conv5x5': 1.0,\n",
    "        'max_pool3x3': 0.2,\n",
    "        'avg_pool3x3': 0.2,\n",
    "        'identity': 0.05\n",
    "    }\n",
    "}\n",
    "\n",
    "def hardware_aware_evaluate(architecture, target_hardware='mobile'):\n",
    "    \"\"\"Evaluate architecture for specific hardware\"\"\"\n",
    "    result = evaluate_architecture(architecture)\n",
    "    \n",
    "    # Calculate hardware-specific latency\n",
    "    total_latency = 0\n",
    "    for op in architecture.layers:\n",
    "        total_latency += HARDWARE_LATENCY[target_hardware][op]\n",
    "    \n",
    "    result['hardware_latency'] = total_latency\n",
    "    result['hardware'] = target_hardware\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Evaluate top architectures on different hardware\n",
    "hardware_results = {'mobile': [], 'gpu': []}\n",
    "\n",
    "for result in top_architectures[:10]:\n",
    "    arch = result['architecture']\n",
    "    for hardware in ['mobile', 'gpu']:\n",
    "        hw_result = hardware_aware_evaluate(arch, hardware)\n",
    "        hardware_results[hardware].append(hw_result)\n",
    "\n",
    "# Visualize hardware comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "mobile_latencies = [r['hardware_latency'] for r in hardware_results['mobile']]\n",
    "gpu_latencies = [r['hardware_latency'] for r in hardware_results['gpu']]\n",
    "accuracies = [r['accuracy'] for r in hardware_results['mobile']]\n",
    "\n",
    "x = np.arange(len(mobile_latencies))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, mobile_latencies, width, label='Mobile', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, gpu_latencies, width, label='GPU', alpha=0.8)\n",
    "\n",
    "# Add accuracy as line\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(x, accuracies, 'ro-', linewidth=2, markersize=8, label='Accuracy')\n",
    "ax2.set_ylabel('Accuracy', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "ax.set_xlabel('Architecture Index')\n",
    "ax.set_ylabel('Latency (ms)')\n",
    "ax.set_title('Hardware-Aware Architecture Evaluation')\n",
    "ax.set_xticks(x)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Hardware-Specific Insights:\")\n",
    "print(f\"Average mobile latency: {np.mean(mobile_latencies):.2f}ms\")\n",
    "print(f\"Average GPU latency: {np.mean(gpu_latencies):.2f}ms\")\n",
    "print(f\"Mobile/GPU latency ratio: {np.mean(mobile_latencies)/np.mean(gpu_latencies):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Let's summarize the key concepts and best practices for Neural Architecture Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(0.5, 0.95, 'Neural Architecture Search Summary', \n",
    "        ha='center', fontsize=20, fontweight='bold')\n",
    "\n",
    "# Method comparison table\n",
    "methods_data = [\n",
    "    ['Method', 'Search Efficiency', 'Quality', 'Hardware Aware', 'Complexity'],\n",
    "    ['Random Search', '⭐', '⭐⭐', '✓', '⭐'],\n",
    "    ['Evolutionary', '⭐⭐', '⭐⭐⭐', '✓', '⭐⭐'],\n",
    "    ['DARTS', '⭐⭐⭐⭐', '⭐⭐⭐⭐', '✗', '⭐⭐⭐⭐'],\n",
    "    ['Predictor-based', '⭐⭐⭐⭐⭐', '⭐⭐⭐', '✓', '⭐⭐⭐']\n",
    "]\n",
    "\n",
    "# Create table\n",
    "table = ax.table(cellText=methods_data[1:], colLabels=methods_data[0],\n",
    "                cellLoc='center', loc='center',\n",
    "                bbox=[0.1, 0.5, 0.8, 0.35])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Style header\n",
    "for i in range(len(methods_data[0])):\n",
    "    table[(0, i)].set_facecolor('#3498db')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Key insights\n",
    "insights = [\n",
    "    \"• Start with random search as baseline\",\n",
    "    \"• Use evolutionary algorithms for discrete search spaces\",\n",
    "    \"• DARTS is efficient but memory-intensive\",\n",
    "    \"• Consider hardware constraints early\",\n",
    "    \"• Multi-objective optimization is often necessary\",\n",
    "    \"• Use performance predictors to reduce search cost\"\n",
    "]\n",
    "\n",
    "ax.text(0.1, 0.35, 'Key Insights:', fontsize=14, fontweight='bold')\n",
    "for i, insight in enumerate(insights):\n",
    "    ax.text(0.1, 0.30 - i*0.04, insight, fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"NAS Best Practices:\")\n",
    "print(\"1. Define clear objectives (accuracy, latency, size)\")\n",
    "print(\"2. Design appropriate search space for your problem\")\n",
    "print(\"3. Use early stopping and performance prediction\")\n",
    "print(\"4. Consider hardware constraints from the beginning\")\n",
    "print(\"5. Validate discovered architectures thoroughly\")\n",
    "print(\"\\nNAS is a powerful tool for automating deep learning design!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}