{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks in PyTorch: A Comprehensive Guide\n",
    "\n",
    "This notebook provides an in-depth guide to training neural networks effectively using PyTorch. We cover everything from the fundamental training loop to advanced techniques for optimization, regularization, and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR, ExponentialLR, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.nn.utils import clip_grad_norm_, clip_grad_value_\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directory for plots, models, and logs\n",
    "output_dir = \"04_training_neural_networks_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"runs\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"saved_models\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Neural Network Training\n",
    "\n",
    "Neural network training is the process of teaching a model to learn patterns from data through an iterative optimization process. The core components include:\n",
    "\n",
    "- **Model**: The neural network architecture (e.g., MLP, CNN) defined using `nn.Module`\n",
    "- **Data**: Input features and target labels, typically split into training, validation, and test sets\n",
    "- **Loss Function**: Measures the discrepancy between predictions and true values\n",
    "- **Optimizer**: Algorithm that adjusts model parameters to minimize the loss function\n",
    "\n",
    "The training process involves:\n",
    "- **Epochs**: Complete passes through the entire training dataset\n",
    "- **Batches**: The dataset is divided into smaller subsets for more manageable processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing Your Data with Dataset and DataLoader\n",
    "\n",
    "PyTorch provides convenient utilities for handling data efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset(Dataset):\n",
    "    \"\"\"Example of a custom Dataset.\"\"\"\n",
    "    def __init__(self, num_samples=1000, input_features=10, num_classes=2, transform=None):\n",
    "        # Generate some random data for demonstration\n",
    "        self.data = torch.randn(num_samples, input_features)\n",
    "        self.targets = torch.randint(0, num_classes, (num_samples,))\n",
    "        self.transform = transform\n",
    "        print(f\"CustomDataset: Created {num_samples} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "# Using the custom dataset\n",
    "print(\"--- Custom Dataset Example ---\")\n",
    "custom_train_dataset = MyCustomDataset(num_samples=100, input_features=5, num_classes=3)\n",
    "sample_data, sample_target = custom_train_dataset[0]\n",
    "print(f\"First sample data shape: {sample_data.shape}, target: {sample_target}\")\n",
    "\n",
    "# Using DataLoader with the custom dataset\n",
    "custom_train_loader = DataLoader(custom_train_dataset, batch_size=32, shuffle=True)\n",
    "print(f\"Number of batches in custom_train_loader: {len(custom_train_loader)}\")\n",
    "for i, (batch_data, batch_targets) in enumerate(custom_train_loader):\n",
    "    print(f\"Batch {i+1} data shape: {batch_data.shape}, targets shape: {batch_targets.shape}\")\n",
    "    if i == 0:  # Print only first batch details\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using torchvision for a standard dataset (MNIST)\n",
    "print(\"\\n--- torchvision MNIST Dataset and DataLoader Example ---\")\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "mnist_train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=mnist_transform\n",
    ")\n",
    "mnist_train_loader = DataLoader(mnist_train_dataset, batch_size=64, shuffle=True)\n",
    "print(f\"Number of samples in MNIST training set: {len(mnist_train_dataset)}\")\n",
    "print(f\"Number of batches in MNIST train_loader: {len(mnist_train_loader)}\")\n",
    "mnist_batch_data, mnist_batch_targets = next(iter(mnist_train_loader))\n",
    "print(f\"MNIST first batch data shape: {mnist_batch_data.shape}, targets shape: {mnist_batch_targets.shape}\")\n",
    "print(\"`Dataset` manages data samples, `DataLoader` provides batches for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining a Neural Network\n",
    "\n",
    "Let's define a simple neural network that we'll use throughout this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size=28*28, hidden_size=128, num_classes=10, use_dropout=False, use_bn=False):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.use_dropout = use_dropout\n",
    "        self.use_bn = use_bn\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        if self.use_bn:\n",
    "            self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        if self.use_dropout:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.fc1(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create a model instance\n",
    "model = SimpleNN().to(device)\n",
    "print(\"Neural Network Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Essential Training Loop\n",
    "\n",
    "The training loop is the core of neural network training. Here's how one iteration works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data for demonstration\n",
    "dummy_inputs = torch.randn(64, 28*28).to(device)  # Batch of 64 flattened images\n",
    "dummy_targets = torch.randint(0, 10, (64,)).to(device)  # 64 labels for 10 classes\n",
    "\n",
    "model = SimpleNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Running one iteration of the training loop...\")\n",
    "\n",
    "# 1. Set model to training mode\n",
    "model.train()\n",
    "\n",
    "# 2. Zero gradients (typically at start of batch loop)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 3. Forward pass: Getting predictions\n",
    "outputs = model(dummy_inputs)\n",
    "print(f\"  Output shape: {outputs.shape}\")\n",
    "\n",
    "# 4. Calculate the loss\n",
    "loss = criterion(outputs, dummy_targets)\n",
    "print(f\"  Calculated loss: {loss.item():.4f}\")\n",
    "\n",
    "# 5. Backward pass: Computing gradients\n",
    "loss.backward()\n",
    "print(f\"  Gradients computed (e.g., model.fc1.weight.grad is not None: {model.fc1.weight.grad is not None})\")\n",
    "\n",
    "# 6. Optimizer step: Updating weights\n",
    "optimizer.step()\n",
    "print(f\"  Optimizer step taken (weights updated).\")\n",
    "print(\"This forms one iteration. A full epoch repeats this for all batches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation: Evaluating Model Performance\n",
    "\n",
    "Validation helps monitor overfitting and assess how well the model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_loaders(batch_size=64, validation_split=0.1):\n",
    "    \"\"\"Helper function to get MNIST data loaders\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    full_train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    num_train = len(full_train_dataset)\n",
    "    val_size = int(validation_split * num_train)\n",
    "    train_size = num_train - val_size\n",
    "    \n",
    "    train_subset, val_subset = random_split(full_train_dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Get data loaders\n",
    "train_loader, val_loader, test_loader = get_mnist_loaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}, Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation loop demonstration\n",
    "model = SimpleNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Running one validation epoch...\")\n",
    "# 1. Set model to evaluation mode\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "# 2. Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        val_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted_classes = outputs.max(1)\n",
    "        total_samples += targets.size(0)\n",
    "        correct_predictions += predicted_classes.eq(targets).sum().item()\n",
    "        \n",
    "        # Print only first batch details\n",
    "        if total_samples == inputs.size(0):\n",
    "            print(f\"  Validation batch: Loss={loss.item():.4f}\")\n",
    "        \n",
    "        # Break after a few batches for demo\n",
    "        if total_samples >= 256:\n",
    "            break\n",
    "\n",
    "epoch_loss = val_loss / total_samples\n",
    "epoch_accuracy = correct_predictions / total_samples\n",
    "print(f\"Validation Results: Average Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy*100:.2f}%\")\n",
    "print(\"`model.eval()` and `torch.no_grad()` are crucial for correct validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving and Loading Models\n",
    "\n",
    "It's essential to save your trained model for later use or to resume training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a model briefly for saving\n",
    "model_to_save = SimpleNN().to(device)\n",
    "optimizer = optim.Adam(model_to_save.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Simulate some training\n",
    "dummy_input = torch.randn(10, 28*28).to(device)\n",
    "dummy_target = torch.randint(0, 10, (10,)).to(device)\n",
    "for _ in range(2):  # Few dummy steps\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(model_to_save(dummy_input), dummy_target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model_path = os.path.join(output_dir, \"saved_models\", \"simple_nn_statedict.pth\")\n",
    "checkpoint_path = os.path.join(output_dir, \"saved_models\", \"checkpoint.pth\")\n",
    "\n",
    "# --- Saving and Loading State Dictionary (Recommended) ---\n",
    "print(\"--- Saving and Loading Model State Dictionary ---\")\n",
    "torch.save(model_to_save.state_dict(), model_path)\n",
    "print(f\"Model state_dict saved to: {model_path}\")\n",
    "\n",
    "# Load the state_dict\n",
    "model_loaded_state_dict = SimpleNN().to(device)  # Create a new instance\n",
    "model_loaded_state_dict.load_state_dict(torch.load(model_path))\n",
    "model_loaded_state_dict.eval()  # Set to evaluation mode\n",
    "print(\"Model loaded from state_dict successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Saving and Loading Checkpoints (for resuming training) ---\n",
    "print(\"\\n--- Saving and Loading Checkpoints ---\")\n",
    "epoch = 5\n",
    "current_loss = 0.123\n",
    "checkpoint = {\n",
    "    'epoch': epoch + 1,  # Save next epoch to start from\n",
    "    'model_state_dict': model_to_save.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': current_loss,\n",
    "}\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"Checkpoint saved to: {checkpoint_path}\")\n",
    "\n",
    "# Load from checkpoint\n",
    "model_for_resume = SimpleNN().to(device)\n",
    "optimizer_for_resume = optim.Adam(model_for_resume.parameters(), lr=0.001)\n",
    "\n",
    "loaded_checkpoint = torch.load(checkpoint_path)\n",
    "model_for_resume.load_state_dict(loaded_checkpoint['model_state_dict'])\n",
    "optimizer_for_resume.load_state_dict(loaded_checkpoint['optimizer_state_dict'])\n",
    "start_epoch = loaded_checkpoint['epoch']\n",
    "previous_loss = loaded_checkpoint['loss']\n",
    "model_for_resume.train()  # Set to train mode to resume training\n",
    "print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}, previous loss: {previous_loss:.4f}\")\n",
    "print(\"Always save `state_dict` for portability and flexibility.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Learning Rate Scheduling\n",
    "\n",
    "Adjusting learning rate during training can improve convergence and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different learning rate schedulers\n",
    "model = SimpleNN(hidden_size=32).to(device)  # Smaller model for quick demo\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "num_epochs_lr_demo = 15\n",
    "\n",
    "schedulers_to_test = {\n",
    "    \"StepLR (step=5, gamma=0.5)\": StepLR(optimizer, step_size=5, gamma=0.5),\n",
    "    \"ExponentialLR (gamma=0.85)\": ExponentialLR(optimizer, gamma=0.85),\n",
    "    \"CosineAnnealingLR (T_max=15)\": CosineAnnealingLR(optimizer, T_max=num_epochs_lr_demo),\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, scheduler in schedulers_to_test.items():\n",
    "    # Reset optimizer for each scheduler test\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    # Re-create scheduler with new optimizer\n",
    "    if \"StepLR\" in name:\n",
    "        current_scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    elif \"ExponentialLR\" in name:\n",
    "        current_scheduler = ExponentialLR(optimizer, gamma=0.85)\n",
    "    elif \"CosineAnnealingLR\" in name:\n",
    "        current_scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs_lr_demo)\n",
    "    \n",
    "    lr_history = []\n",
    "    for epoch in range(num_epochs_lr_demo):\n",
    "        lr_history.append(optimizer.param_groups[0]['lr'])\n",
    "        current_scheduler.step()\n",
    "    \n",
    "    plt.plot(lr_history, label=name)\n",
    "\n",
    "plt.title(\"Learning Rate Schedules\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Different schedulers provide various learning rate decay patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Regularization Techniques\n",
    "\n",
    "Regularization helps prevent overfitting by constraining the model's capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- L2 Regularization (Weight Decay) ---\n",
    "print(\"--- L2 Regularization (Weight Decay) ---\")\n",
    "model_wd = SimpleNN().to(device)\n",
    "# Add weight_decay to the optimizer\n",
    "optimizer_wd = optim.Adam(model_wd.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "print(f\"Optimizer with weight_decay (L2 penalty): weight_decay={optimizer_wd.param_groups[0]['weight_decay']}\")\n",
    "\n",
    "# --- Dropout ---\n",
    "print(\"\\n--- Dropout ---\")\n",
    "model_dropout = SimpleNN(use_dropout=True).to(device)\n",
    "print(\"Model with Dropout layer:\")\n",
    "print(model_dropout)\n",
    "\n",
    "# Demonstrate dropout behavior\n",
    "model_dropout.train()\n",
    "dummy_input_reg = torch.randn(5, 28*28).to(device)\n",
    "output_train = model_dropout(dummy_input_reg)\n",
    "print(f\"Output with dropout (train mode): {output_train[0,:5]}\")\n",
    "\n",
    "model_dropout.eval()\n",
    "output_eval = model_dropout(dummy_input_reg)\n",
    "print(f\"Output with dropout (eval mode): {output_eval[0,:5]}\")\n",
    "print(\"During training, dropout randomly zeros elements. During evaluation, it's disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Gradient Clipping\n",
    "\n",
    "Prevents exploding gradients by capping their norm or value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gc = SimpleNN().to(device)\n",
    "optimizer_gc = optim.SGD(model_gc.parameters(), lr=0.01)\n",
    "criterion_gc = nn.MSELoss()\n",
    "dummy_input_gc = torch.randn(5, 28*28).to(device)\n",
    "dummy_target_gc = torch.randn(5, 10).to(device)\n",
    "\n",
    "optimizer_gc.zero_grad()\n",
    "outputs_gc = model_gc(dummy_input_gc)\n",
    "loss_gc = criterion_gc(outputs_gc, dummy_target_gc)\n",
    "loss_gc.backward()\n",
    "\n",
    "# Print norm of gradients before clipping\n",
    "original_grad_norm = model_gc.fc1.weight.grad.norm().item()\n",
    "print(f\"Original grad norm for fc1.weight: {original_grad_norm:.4f}\")\n",
    "\n",
    "# Clip gradient norm\n",
    "max_norm = 1.0\n",
    "total_norm_clipped = clip_grad_norm_(model_gc.parameters(), max_norm=max_norm)\n",
    "print(f\"Total norm of gradients after clipping by norm to {max_norm}: {total_norm_clipped:.4f}\")\n",
    "clipped_grad_norm = model_gc.fc1.weight.grad.norm().item()\n",
    "print(f\"Clipped grad norm for fc1.weight: {clipped_grad_norm:.4f}\")\n",
    "\n",
    "# Demonstrate gradient value clipping\n",
    "optimizer_gc.zero_grad()\n",
    "outputs_gc = model_gc(dummy_input_gc)\n",
    "loss_gc = criterion_gc(outputs_gc, dummy_target_gc)\n",
    "loss_gc.backward()\n",
    "clip_val = 0.1\n",
    "clip_grad_value_(model_gc.parameters(), clip_value=clip_val)\n",
    "print(f\"\\nAfter clipping values to +/- {clip_val}:\")\n",
    "print(f\"  Min grad value: {model_gc.fc1.weight.grad.min().item():.4f}\")\n",
    "print(f\"  Max grad value: {model_gc.fc1.weight.grad.max().item():.4f}\")\n",
    "print(\"Gradient clipping is applied after .backward() and before .step().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Batch Normalization\n",
    "\n",
    "Normalizes activations, stabilizes and accelerates training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_bn = SimpleNN(use_bn=True).to(device)\n",
    "print(\"Model with Batch Normalization layer:\")\n",
    "print(model_with_bn)\n",
    "\n",
    "dummy_input_bn = torch.randn(5, 28*28).to(device)\n",
    "\n",
    "# Behavior in train() mode\n",
    "model_with_bn.train()\n",
    "output_bn_train = model_with_bn(dummy_input_bn)\n",
    "print(f\"\\nOutput with BatchNorm (train mode) shape: {output_bn_train.shape}\")\n",
    "print(f\"Running mean of bn1 after one forward pass (train): {model_with_bn.bn1.running_mean[0].item():.4f}\")\n",
    "\n",
    "# Behavior in eval() mode\n",
    "model_with_bn.eval()\n",
    "output_bn_eval = model_with_bn(dummy_input_bn)\n",
    "print(f\"Output with BatchNorm (eval mode) shape: {output_bn_eval.shape}\")\n",
    "print(\"`model.train()` and `model.eval()` are crucial for BatchNorm to work correctly.\")\n",
    "print(\"In train mode, BN uses batch statistics and updates running mean/var.\")\n",
    "print(\"In eval mode, BN uses computed running mean/var and does not update them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Complete Training Pipeline Example\n",
    "\n",
    "Let's put everything together in a complete training pipeline that demonstrates the concepts we've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 2  # Low for demo, typically 10-100+\n",
    "HIDDEN_SIZE = 256\n",
    "WEIGHT_DECAY = 1e-5\n",
    "CLIP_GRAD_NORM = 1.0\n",
    "\n",
    "# Data Loading\n",
    "train_loader, val_loader, test_loader = get_mnist_loaders(batch_size=BATCH_SIZE)\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Model Definition with regularization\n",
    "model = SimpleNN(28*28, HIDDEN_SIZE, 10, use_dropout=True, use_bn=True).to(device)\n",
    "\n",
    "# Initialize weights\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None: \n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# Loss, Optimizer, Scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "print(\"Model and training setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        if CLIP_GRAD_NORM > 0:\n",
    "            clip_grad_norm_(model.parameters(), max_norm=CLIP_GRAD_NORM)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += targets.size(0)\n",
    "        train_correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            batch_acc = predicted.eq(targets).sum().item() / targets.size(0)\n",
    "            print(f'  Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f} | Acc: {batch_acc*100:.2f}%')\n",
    "    \n",
    "    avg_train_loss = train_loss / train_total\n",
    "    avg_train_acc = train_correct / train_total\n",
    "    \n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += targets.size(0)\n",
    "            val_correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / val_total\n",
    "    avg_val_acc = val_correct / val_total\n",
    "    \n",
    "    # Update scheduler\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['train_acc'].append(avg_train_acc)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['val_acc'].append(avg_val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc*100:.2f}%\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc*100:.2f}%\")\n",
    "    print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history['lr'], label='Learning Rate')\n",
    "plt.title('Learning Rate over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history visualization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook covered the essential aspects of training neural networks in PyTorch:\n",
    "\n",
    "1. **Data Preparation**: Using Dataset and DataLoader for efficient data handling\n",
    "2. **Training Loop**: The core iterative process of training\n",
    "3. **Validation**: Evaluating model performance during training\n",
    "4. **Model Persistence**: Saving and loading models and checkpoints\n",
    "5. **Learning Rate Scheduling**: Dynamically adjusting learning rates\n",
    "6. **Regularization**: Techniques to prevent overfitting (dropout, weight decay)\n",
    "7. **Gradient Clipping**: Preventing exploding gradients\n",
    "8. **Batch Normalization**: Stabilizing and accelerating training\n",
    "9. **Complete Pipeline**: Integrating all concepts in a real training scenario\n",
    "\n",
    "These techniques form the foundation for training effective neural networks. In practice, you'll combine multiple techniques based on your specific problem, dataset size, and computational resources.\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with different architectures (CNNs, RNNs, Transformers)\n",
    "- Try different optimizers (SGD, Adam, RMSprop)\n",
    "- Explore advanced regularization techniques\n",
    "- Learn about transfer learning and fine-tuning pre-trained models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
