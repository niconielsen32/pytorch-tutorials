{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 14: Performance Optimization\n",
    "\n",
    "This tutorial covers comprehensive performance optimization techniques for PyTorch models, from profiling to advanced optimization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import numpy as np\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch.cuda.amp as amp\n",
    "from torch.nn.parallel import DataParallel, DistributedDataParallel\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch Profiler\n",
    "\n",
    "The PyTorch profiler is essential for identifying performance bottlenecks in your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model for profiling\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the model\n",
    "model = SimpleModel().to(device)\n",
    "inputs = torch.randn(32, 3, 32, 32).to(device)\n",
    "\n",
    "# Use PyTorch profiler\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "             record_shapes=True,\n",
    "             profile_memory=True,\n",
    "             with_stack=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        for _ in range(10):\n",
    "            model(inputs)\n",
    "\n",
    "# Print profiler results\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory Optimization\n",
    "\n",
    "Memory optimization is crucial for training large models. Let's explore gradient checkpointing and other techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "    else:\n",
    "        return psutil.Process().memory_info().rss / 1024**2  # MB\n",
    "\n",
    "# Memory-efficient gradient checkpointing\n",
    "class CheckpointedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            ) for _ in range(10)\n",
    "        ])\n",
    "        self.final = nn.Linear(1024, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Use checkpoint to trade compute for memory\n",
    "            x = torch.utils.checkpoint.checkpoint(layer, x)\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare memory usage\n",
    "x = torch.randn(128, 1024).to(device)\n",
    "\n",
    "# Without checkpointing\n",
    "regular_model = nn.Sequential(*[\n",
    "    nn.Sequential(nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(0.1))\n",
    "    for _ in range(10)\n",
    "] + [nn.Linear(1024, 10)]).to(device)\n",
    "\n",
    "mem_before = get_memory_usage()\n",
    "y1 = regular_model(x)\n",
    "loss1 = y1.sum()\n",
    "loss1.backward()\n",
    "mem_regular = get_memory_usage() - mem_before\n",
    "print(f\"Regular model: {mem_regular:.2f} MB\")\n",
    "\n",
    "# Clear memory\n",
    "del regular_model, y1, loss1\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# With checkpointing\n",
    "checkpointed_model = CheckpointedModel().to(device)\n",
    "optimizer = torch.optim.Adam(checkpointed_model.parameters())\n",
    "optimizer.zero_grad()\n",
    "\n",
    "mem_before = get_memory_usage()\n",
    "y2 = checkpointed_model(x)\n",
    "loss2 = y2.sum()\n",
    "loss2.backward()\n",
    "mem_checkpoint = get_memory_usage() - mem_before\n",
    "print(f\"Checkpointed model: {mem_checkpoint:.2f} MB\")\n",
    "print(f\"Memory saved: {(1 - mem_checkpoint/mem_regular)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mixed Precision Training\n",
    "\n",
    "Mixed precision training can significantly speed up training while maintaining model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model for mixed precision demo\n",
    "class MixedPrecisionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with mixed precision\n",
    "def train_with_amp(model, dataloader, use_amp=True, epochs=2):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    scaler = amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    model.train()\n",
    "    total_time = 0\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, (inputs, targets) in enumerate(dataloader):\n",
    "            if i >= 10:  # Limit iterations for demo\n",
    "                break\n",
    "                \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_amp:\n",
    "                # Mixed precision forward pass\n",
    "                with amp.autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = F.cross_entropy(outputs, targets)\n",
    "                \n",
    "                # Scaled backward pass\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Regular forward/backward\n",
    "                outputs = model(inputs)\n",
    "                loss = F.cross_entropy(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        total_time += epoch_time\n",
    "        losses.append(epoch_loss / (i + 1))\n",
    "    \n",
    "    return total_time / epochs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy dataset\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, size=1000):\n",
    "        self.size = size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.randn(3, 32, 32), torch.randint(0, 10, (1,)).item()\n",
    "\n",
    "dataset = DummyDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=64, num_workers=0)\n",
    "\n",
    "# Compare training times\n",
    "model_fp32 = MixedPrecisionModel()\n",
    "model_amp = MixedPrecisionModel()\n",
    "\n",
    "print(\"Training with FP32...\")\n",
    "time_fp32, losses_fp32 = train_with_amp(model_fp32, dataloader, use_amp=False)\n",
    "print(f\"Average epoch time: {time_fp32:.3f}s\")\n",
    "\n",
    "print(\"\\nTraining with AMP...\")\n",
    "time_amp, losses_amp = train_with_amp(model_amp, dataloader, use_amp=True)\n",
    "print(f\"Average epoch time: {time_amp:.3f}s\")\n",
    "print(f\"Speedup: {time_fp32/time_amp:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_fp32, 'o-', label='FP32', linewidth=2)\n",
    "plt.plot(losses_amp, 's-', label='Mixed Precision', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading Optimization\n",
    "\n",
    "Efficient data loading is crucial for GPU utilization. Let's explore various optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized dataset with caching and prefetching\n",
    "class OptimizedDataset(Dataset):\n",
    "    def __init__(self, size=1000, cache_size=100):\n",
    "        self.size = size\n",
    "        self.cache_size = cache_size\n",
    "        self.cache = {}\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Simple caching mechanism\n",
    "        if idx in self.cache:\n",
    "            return self.cache[idx]\n",
    "        \n",
    "        # Simulate data loading\n",
    "        image = torch.randn(3, 32, 32)\n",
    "        label = torch.randint(0, 10, (1,)).item()\n",
    "        \n",
    "        # Cache recent items\n",
    "        if len(self.cache) < self.cache_size:\n",
    "            self.cache[idx] = (image, label)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark data loading performance\n",
    "def benchmark_dataloader(dataset, num_workers, pin_memory=False, prefetch_factor=2):\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=(num_workers > 0),\n",
    "        prefetch_factor=prefetch_factor if num_workers > 0 else 2\n",
    "    )\n",
    "    \n",
    "    # Warmup\n",
    "    for i, _ in enumerate(dataloader):\n",
    "        if i >= 5:\n",
    "            break\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i, (data, target) in enumerate(dataloader):\n",
    "        if i >= 50:  # Limit iterations\n",
    "            break\n",
    "        # Simulate processing\n",
    "        data = data.to(device, non_blocking=True)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    return total_time\n",
    "\n",
    "dataset = OptimizedDataset(5000)\n",
    "\n",
    "# Test different configurations\n",
    "results = []\n",
    "configs = [\n",
    "    (0, False),\n",
    "    (2, False),\n",
    "    (2, True),\n",
    "    (4, False),\n",
    "    (4, True),\n",
    "]\n",
    "\n",
    "print(\"Data loading benchmark:\")\n",
    "for num_workers, pin_memory in configs:\n",
    "    time_taken = benchmark_dataloader(dataset, num_workers, pin_memory)\n",
    "    results.append((num_workers, pin_memory, time_taken))\n",
    "    print(f\"Workers: {num_workers}, Pin memory: {pin_memory} - Time: {time_taken:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data loading performance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "workers = [r[0] for r in results]\n",
    "times = [r[2] for r in results]\n",
    "pin_memory = [r[1] for r in results]\n",
    "\n",
    "colors = ['blue' if not pm else 'red' for pm in pin_memory]\n",
    "labels = [f\"Workers: {w}\\nPin: {pm}\" for w, pm in zip(workers, pin_memory)]\n",
    "\n",
    "bars = ax.bar(range(len(results)), times, color=colors)\n",
    "ax.set_xlabel('Configuration')\n",
    "ax.set_ylabel('Time (seconds)')\n",
    "ax.set_title('Data Loading Performance')\n",
    "ax.set_xticks(range(len(results)))\n",
    "ax.set_xticklabels(labels, rotation=45)\n",
    "\n",
    "# Add legend\n",
    "blue_patch = plt.Rectangle((0, 0), 1, 1, fc=\"blue\")\n",
    "red_patch = plt.Rectangle((0, 0), 1, 1, fc=\"red\")\n",
    "ax.legend([blue_patch, red_patch], ['No Pin Memory', 'Pin Memory'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TorchScript Optimization\n",
    "\n",
    "TorchScript can significantly improve inference performance by optimizing the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model for scripting\n",
    "class ScriptableModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc = nn.Linear(64 * 6 * 6, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare scripted vs regular model\n",
    "model = ScriptableModel().to(device)\n",
    "model.eval()\n",
    "\n",
    "# Script the model\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "# Also try tracing\n",
    "example_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "\n",
    "# Benchmark\n",
    "x = torch.randn(100, 3, 32, 32).to(device)\n",
    "num_runs = 100\n",
    "\n",
    "# Regular model\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_runs):\n",
    "        _ = model(x)\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "regular_time = time.time() - start\n",
    "\n",
    "# Scripted model\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_runs):\n",
    "        _ = scripted_model(x)\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "scripted_time = time.time() - start\n",
    "\n",
    "# Traced model\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_runs):\n",
    "        _ = traced_model(x)\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "traced_time = time.time() - start\n",
    "\n",
    "print(f\"Regular model: {regular_time:.3f}s\")\n",
    "print(f\"Scripted model: {scripted_time:.3f}s (Speedup: {regular_time/scripted_time:.2f}x)\")\n",
    "print(f\"Traced model: {traced_time:.3f}s (Speedup: {regular_time/traced_time:.2f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tensor Operations Optimization\n",
    "\n",
    "Efficient tensor operations are crucial for performance. Let's compare different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different implementations of the same operation\n",
    "def batch_norm_manual(x, gamma, beta, eps=1e-5):\n",
    "    \"\"\"Manual batch normalization (inefficient)\"\"\"\n",
    "    mean = x.mean(dim=(0, 2, 3), keepdim=True)\n",
    "    var = ((x - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "    x_norm = (x - mean) / torch.sqrt(var + eps)\n",
    "    return gamma.view(1, -1, 1, 1) * x_norm + beta.view(1, -1, 1, 1)\n",
    "\n",
    "def batch_norm_optimized(x, gamma, beta, eps=1e-5):\n",
    "    \"\"\"Optimized batch normalization\"\"\"\n",
    "    # Use running statistics and fused operations\n",
    "    return F.batch_norm(x, None, None, gamma, beta, True, 0.1, eps)\n",
    "\n",
    "# Test\n",
    "batch_size, channels, height, width = 32, 64, 32, 32\n",
    "x = torch.randn(batch_size, channels, height, width).to(device)\n",
    "gamma = torch.ones(channels).to(device)\n",
    "beta = torch.zeros(channels).to(device)\n",
    "\n",
    "# Benchmark\n",
    "num_runs = 100\n",
    "\n",
    "# Manual implementation\n",
    "start = time.time()\n",
    "for _ in range(num_runs):\n",
    "    _ = batch_norm_manual(x, gamma, beta)\n",
    "manual_time = time.time() - start\n",
    "\n",
    "# Optimized implementation\n",
    "start = time.time()\n",
    "for _ in range(num_runs):\n",
    "    _ = batch_norm_optimized(x, gamma, beta)\n",
    "optimized_time = time.time() - start\n",
    "\n",
    "print(f\"Manual batch norm: {manual_time:.3f}s\")\n",
    "print(f\"Optimized batch norm: {optimized_time:.3f}s\")\n",
    "print(f\"Speedup: {manual_time/optimized_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory-Efficient Attention\n",
    "\n",
    "For transformer models, attention can be a memory bottleneck. Let's implement memory-efficient attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, chunk_size=256):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.chunk_size = chunk_size\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Chunked attention computation\n",
    "        attn_chunks = []\n",
    "        for i in range(0, N, self.chunk_size):\n",
    "            end_idx = min(i + self.chunk_size, N)\n",
    "            q_chunk = q[:, :, i:end_idx]\n",
    "            \n",
    "            # Compute attention for this chunk\n",
    "            attn = (q_chunk @ k.transpose(-2, -1)) * self.scale\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn_chunk = attn @ v\n",
    "            attn_chunks.append(attn_chunk)\n",
    "        \n",
    "        # Concatenate chunks\n",
    "        x = torch.cat(attn_chunks, dim=2)\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "# Compare with standard attention\n",
    "class StandardAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory usage\n",
    "seq_len = 2048\n",
    "dim = 512\n",
    "batch_size = 4\n",
    "\n",
    "# Standard attention\n",
    "standard_attn = StandardAttention(dim).to(device)\n",
    "efficient_attn = EfficientAttention(dim, chunk_size=256).to(device)\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, dim).to(device)\n",
    "\n",
    "# Measure memory for standard attention\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "mem_before = get_memory_usage()\n",
    "output1 = standard_attn(x)\n",
    "mem_standard = get_memory_usage() - mem_before\n",
    "\n",
    "# Clear memory\n",
    "del output1\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Measure memory for efficient attention\n",
    "mem_before = get_memory_usage()\n",
    "output2 = efficient_attn(x)\n",
    "mem_efficient = get_memory_usage() - mem_before\n",
    "\n",
    "print(f\"Standard attention memory: {mem_standard:.2f} MB\")\n",
    "print(f\"Efficient attention memory: {mem_efficient:.2f} MB\")\n",
    "print(f\"Memory saved: {(1 - mem_efficient/mem_standard)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Memory Pool\n",
    "\n",
    "For applications with frequent tensor allocations, a custom memory pool can reduce overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorPool:\n",
    "    \"\"\"Simple tensor pool for reusing allocations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.pool = {}\n",
    "        self.stats = {'hits': 0, 'misses': 0}\n",
    "    \n",
    "    def get(self, shape, dtype=torch.float32, device='cpu'):\n",
    "        key = (tuple(shape), dtype, str(device))\n",
    "        if key in self.pool and len(self.pool[key]) > 0:\n",
    "            self.stats['hits'] += 1\n",
    "            return self.pool[key].pop()\n",
    "        self.stats['misses'] += 1\n",
    "        return torch.empty(shape, dtype=dtype, device=device)\n",
    "    \n",
    "    def release(self, tensor):\n",
    "        key = (tuple(tensor.shape), tensor.dtype, str(tensor.device))\n",
    "        if key not in self.pool:\n",
    "            self.pool[key] = []\n",
    "        self.pool[key].append(tensor)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.pool.clear()\n",
    "    \n",
    "    def get_stats(self):\n",
    "        total = self.stats['hits'] + self.stats['misses']\n",
    "        hit_rate = self.stats['hits'] / total if total > 0 else 0\n",
    "        return {\n",
    "            'hit_rate': hit_rate,\n",
    "            'pool_size': sum(len(v) for v in self.pool.values()),\n",
    "            **self.stats\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate tensor pool usage\n",
    "pool = TensorPool()\n",
    "\n",
    "# Simulate workload with repeated allocations\n",
    "shapes = [(100, 100), (50, 200), (100, 100), (50, 200)]\n",
    "num_iterations = 100\n",
    "\n",
    "# Without pool\n",
    "start = time.time()\n",
    "for _ in range(num_iterations):\n",
    "    tensors = []\n",
    "    for shape in shapes:\n",
    "        t = torch.empty(shape, device=device)\n",
    "        tensors.append(t)\n",
    "    # Simulate some computation\n",
    "    for t in tensors:\n",
    "        t.fill_(1.0)\n",
    "time_without_pool = time.time() - start\n",
    "\n",
    "# With pool\n",
    "start = time.time()\n",
    "for _ in range(num_iterations):\n",
    "    tensors = []\n",
    "    for shape in shapes:\n",
    "        t = pool.get(shape, device=device)\n",
    "        tensors.append(t)\n",
    "    # Simulate some computation\n",
    "    for t in tensors:\n",
    "        t.fill_(1.0)\n",
    "    # Release tensors back to pool\n",
    "    for t in tensors:\n",
    "        pool.release(t)\n",
    "time_with_pool = time.time() - start\n",
    "\n",
    "print(f\"Time without pool: {time_without_pool:.3f}s\")\n",
    "print(f\"Time with pool: {time_with_pool:.3f}s\")\n",
    "print(f\"Speedup: {time_without_pool/time_with_pool:.2f}x\")\n",
    "print(f\"\\nPool statistics: {pool.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimization Checklist\n",
    "\n",
    "Here's a comprehensive checklist for optimizing PyTorch models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization checklist\n",
    "checklist = [\n",
    "    (\"Profile with torch.profiler\", \"Identify bottlenecks before optimizing\"),\n",
    "    (\"Enable mixed precision training\", \"Use torch.cuda.amp for faster training\"),\n",
    "    (\"Optimize data loading pipeline\", \"Use multiple workers, pin_memory, and persistent_workers\"),\n",
    "    (\"Use gradient checkpointing\", \"Trade compute for memory in large models\"),\n",
    "    (\"Apply model quantization\", \"Reduce model size and improve inference speed\"),\n",
    "    (\"Enable CUDNN benchmarking\", \"torch.backends.cudnn.benchmark = True\"),\n",
    "    (\"Use TorchScript for inference\", \"Compile models for production deployment\"),\n",
    "    (\"Implement custom CUDA kernels\", \"For performance-critical operations\"),\n",
    "    (\"Use distributed training\", \"Scale across multiple GPUs/nodes\"),\n",
    "    (\"Monitor GPU utilization\", \"Ensure high GPU utilization throughout training\"),\n",
    "    (\"Optimize tensor operations\", \"Use vectorized operations and avoid loops\"),\n",
    "    (\"Reduce memory fragmentation\", \"Clear cache and reuse tensors when possible\"),\n",
    "    (\"Use operator fusion\", \"Combine multiple operations into one\"),\n",
    "    (\"Optimize model architecture\", \"Use efficient layers and reduce redundancy\"),\n",
    "    (\"Profile memory usage\", \"Identify and fix memory leaks\"),\n",
    "]\n",
    "\n",
    "print(\"Performance Optimization Checklist\")\n",
    "print(\"=\" * 50)\n",
    "for i, (task, description) in enumerate(checklist, 1):\n",
    "    print(f\"{i:2d}. [ ] {task}\")\n",
    "    print(f\"       {description}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered comprehensive performance optimization techniques:\n",
    "\n",
    "1. **Profiling**: Using PyTorch profiler to identify bottlenecks\n",
    "2. **Memory Optimization**: Gradient checkpointing and efficient memory usage\n",
    "3. **Mixed Precision**: Accelerating training with automatic mixed precision\n",
    "4. **Data Loading**: Optimizing data pipelines for maximum throughput\n",
    "5. **TorchScript**: Compiling models for faster inference\n",
    "6. **Tensor Operations**: Using efficient implementations\n",
    "7. **Custom Attention**: Memory-efficient attention mechanisms\n",
    "8. **Memory Pooling**: Reducing allocation overhead\n",
    "\n",
    "Remember: **Always profile before optimizing!** Premature optimization can lead to complex code without meaningful performance gains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}