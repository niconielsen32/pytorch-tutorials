{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Fundamentals\n",
    "\n",
    "This notebook covers the fundamental concepts of neural networks using PyTorch, including perceptrons, activation functions, multi-layer perceptrons, loss functions, optimizers, and training your first neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"02_neural_networks_fundamentals_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Neural Networks\n",
    "\n",
    "Neural networks are computational models inspired by the structure and function of biological neural networks in the human brain. They consist of interconnected processing units called neurons (or nodes) organized in layers.\n",
    "\n",
    "**Key Components:**\n",
    "- **Neurons (Nodes):** Basic computational units that receive inputs, perform calculations, and produce outputs\n",
    "- **Weights:** Parameters representing the strength or importance of each input\n",
    "- **Biases:** Additional parameters that allow neurons to be activated even when all inputs are zero\n",
    "- **Layers:** Neurons are organized into input, hidden, and output layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Neural Networks Fundamentals - Key Concepts:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"✓ What is a Neural Network? Biological Inspiration\")\n",
    "print(\"✓ Basic Components: Neurons, Weights, Biases, Layers\")\n",
    "print(\"✓ Types of Neural Networks (FNNs, CNNs, RNNs)\")\n",
    "print(\"✓ Focus: Feedforward Neural Networks (MLPs)\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Perceptron: The Simplest Neural Network\n",
    "\n",
    "The perceptron is the most basic neural network, consisting of a single layer of output neurons. It performs a weighted sum of inputs, adds a bias, and applies an activation function.\n",
    "\n",
    "**Formula:** `output = activation(sum(weights_i * input_i) + bias)`\n",
    "\n",
    "**Key Limitation:** A single-layer perceptron can only solve linearly separable problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate a simple perceptron\n",
    "print(\"Perceptron Demonstration:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create a perceptron with 2 inputs and 1 output\n",
    "perceptron_layer = nn.Linear(2, 1).to(device)\n",
    "sample_input = torch.tensor([0.5, -1.0], device=device)\n",
    "\n",
    "# Forward pass through perceptron\n",
    "weighted_sum = perceptron_layer(sample_input)\n",
    "output = torch.sigmoid(weighted_sum)  # Using sigmoid as activation\n",
    "\n",
    "print(f\"Sample input: {sample_input.cpu().numpy()}\")\n",
    "print(f\"Perceptron weights: {perceptron_layer.weight.data.cpu().numpy()}\")\n",
    "print(f\"Perceptron bias: {perceptron_layer.bias.data.cpu().numpy()}\")\n",
    "print(f\"Weighted sum + bias: {weighted_sum.item():.4f}\")\n",
    "print(f\"Output after sigmoid: {output.item():.4f}\")\n",
    "print(\"\\nNote: Single-layer perceptron can only solve linearly separable problems.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns and relationships. Without non-linearity, a multi-layer network would behave like a single-layer linear network.\n",
    "\n",
    "**Common Activation Functions:**\n",
    "- **Sigmoid:** `f(x) = 1 / (1 + exp(-x))` - Squashes values between 0 and 1\n",
    "- **Tanh:** `f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))` - Squashes values between -1 and 1\n",
    "- **ReLU:** `f(x) = max(0, x)` - Most popular for hidden layers\n",
    "- **Leaky ReLU:** `f(x) = max(0.01*x, x)` - Addresses \"dying ReLU\" problem\n",
    "- **Softmax:** Used in output layer for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input values for plotting activation functions\n",
    "x_vals = torch.linspace(-6, 6, 100)\n",
    "\n",
    "# Define activation functions\n",
    "sigmoid_fn = nn.Sigmoid()\n",
    "tanh_fn = nn.Tanh()\n",
    "relu_fn = nn.ReLU()\n",
    "leaky_relu_fn = nn.LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "# Compute outputs\n",
    "y_sigmoid = sigmoid_fn(x_vals)\n",
    "y_tanh = tanh_fn(x_vals)\n",
    "y_relu = relu_fn(x_vals)\n",
    "y_leaky_relu = leaky_relu_fn(x_vals)\n",
    "\n",
    "print(\"Activation Functions Computed:\")\n",
    "print(f\"Sigmoid range: [{y_sigmoid.min():.3f}, {y_sigmoid.max():.3f}]\")\n",
    "print(f\"Tanh range: [{y_tanh.min():.3f}, {y_tanh.max():.3f}]\")\n",
    "print(f\"ReLU range: [{y_relu.min():.3f}, {y_relu.max():.3f}]\")\n",
    "print(f\"Leaky ReLU range: [{y_leaky_relu.min():.3f}, {y_leaky_relu.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot activation functions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x_vals.numpy(), y_sigmoid.numpy(), 'b-', linewidth=2, label='Sigmoid')\n",
    "plt.title('Sigmoid: 1 / (1 + exp(-x))')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x_vals.numpy(), y_tanh.numpy(), 'r-', linewidth=2, label='Tanh')\n",
    "plt.title('Tanh: (exp(x) - exp(-x)) / (exp(x) + exp(-x))')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(x_vals.numpy(), y_relu.numpy(), 'g-', linewidth=2, label='ReLU')\n",
    "plt.title('ReLU: max(0, x)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(x_vals.numpy(), y_leaky_relu.numpy(), 'm-', linewidth=2, label='Leaky ReLU (slope=0.1)')\n",
    "plt.title('Leaky ReLU: max(0.1*x, x)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Softmax activation\n",
    "softmax_fn = nn.Softmax(dim=1)\n",
    "sample_logits = torch.tensor([[1.0, -0.5, 2.0], [0.1, 0.5, 0.2]])  # Batch of 2, 3 classes\n",
    "y_softmax = softmax_fn(sample_logits)\n",
    "\n",
    "print(\"Softmax Activation Function:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Sample Logits:\")\n",
    "print(sample_logits)\n",
    "print(\"\\nSoftmax Output (probabilities that sum to 1):\")\n",
    "print(y_softmax)\n",
    "print(f\"\\nSum of probabilities for sample 1: {y_softmax[0].sum():.4f}\")\n",
    "print(f\"Sum of probabilities for sample 2: {y_softmax[1].sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Layer Perceptrons (MLPs)\n",
    "\n",
    "Multi-Layer Perceptrons are feedforward neural networks with one or more hidden layers between the input and output layers. Each layer is fully connected to the next.\n",
    "\n",
    "**Architecture:**\n",
    "- **Input Layer:** Receives raw input data\n",
    "- **Hidden Layer(s):** Perform intermediate computations\n",
    "- **Output Layer:** Produces final predictions\n",
    "\n",
    "**Universal Approximation Theorem:** An MLP with at least one hidden layer and non-linear activation can approximate any continuous function to arbitrary accuracy (given enough neurons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Multi-Layer Perceptron\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)           # Input to hidden layer\n",
    "        self.relu1 = nn.ReLU()                                  # First activation\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)     # Hidden to smaller hidden\n",
    "        self.relu2 = nn.ReLU()                                  # Second activation\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, num_classes)     # Final layer to output\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"  Input shape: {x.shape}\")\n",
    "        out = self.fc1(x)\n",
    "        print(f\"  After fc1: {out.shape}\")\n",
    "        out = self.relu1(out)\n",
    "        print(f\"  After relu1: {out.shape}\")\n",
    "        out = self.fc2(out)\n",
    "        print(f\"  After fc2: {out.shape}\")\n",
    "        out = self.relu2(out)\n",
    "        print(f\"  After relu2: {out.shape}\")\n",
    "        out = self.fc3(out)\n",
    "        print(f\"  Output shape (logits): {out.shape}\")\n",
    "        return out\n",
    "\n",
    "# Create and examine the MLP\n",
    "input_dim = 100\n",
    "hidden_dim = 64\n",
    "output_dim = 5\n",
    "\n",
    "model = SimpleMLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "print(\"MLP Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal trainable parameters: {total_params:,}\")\n",
    "\n",
    "# Demonstrate forward propagation\n",
    "print(\"\\nDemonstrating Forward Propagation:\")\n",
    "print(\"-\" * 40)\n",
    "batch_size = 4\n",
    "dummy_input = torch.randn(batch_size, input_dim).to(device)\n",
    "print(f\"Input batch shape: {dummy_input.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(dummy_input)\n",
    "\n",
    "print(f\"\\nOutput predictions shape: {predictions.shape}\")\n",
    "print(\"Output for first sample (raw logits):\")\n",
    "print(predictions[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Defining Neural Networks in PyTorch (nn.Module)\n",
    "\n",
    "PyTorch provides the `nn.Module` class as the base for all neural network modules:\n",
    "\n",
    "- **Inheritance:** Custom networks inherit from `nn.Module`\n",
    "- **Layer Definition:** Layers are defined as attributes in `__init__`\n",
    "- **Forward Method:** The `forward` method defines how data flows through the network\n",
    "- **Parameter Management:** PyTorch automatically tracks parameters and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch nn.Module Key Points:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"✓ Networks inherit from nn.Module\")\n",
    "print(\"✓ Layers defined as attributes in __init__\")\n",
    "print(\"✓ forward() method defines data flow\")\n",
    "print(\"✓ Automatic parameter tracking\")\n",
    "print(\"✓ Built-in layers: nn.Linear, nn.Conv2d, etc.\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Examine the SimpleMLP class structure\n",
    "print(\"\\nSimpleMLP Class Structure:\")\n",
    "print(\"- __init__: Defines layers (fc1, relu1, fc2, relu2, fc3)\")\n",
    "print(\"- forward: Defines computation flow\")\n",
    "print(\"- Inherits from nn.Module for automatic parameter management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss Functions: Measuring Model Error\n",
    "\n",
    "Loss functions quantify the difference between model predictions and true targets. They guide the learning process by providing a measure of how well the model is performing.\n",
    "\n",
    "**Common Loss Functions:**\n",
    "- **MSE Loss (`nn.MSELoss`):** For regression tasks\n",
    "- **Cross-Entropy Loss (`nn.CrossEntropyLoss`):** For multi-class classification\n",
    "- **Binary Cross-Entropy (`nn.BCELoss`, `nn.BCEWithLogitsLoss`):** For binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different loss functions\n",
    "print(\"Loss Functions Demonstration:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Mean Squared Error (MSE) - For Regression\n",
    "loss_mse_fn = nn.MSELoss()\n",
    "predictions_reg = torch.tensor([1.0, 2.5, 3.8], device=device)\n",
    "targets_reg = torch.tensor([1.2, 2.3, 4.0], device=device)\n",
    "mse = loss_mse_fn(predictions_reg, targets_reg)\n",
    "\n",
    "print(\"1. MSE Loss (Regression):\")\n",
    "print(f\"   Predictions: {predictions_reg.cpu().numpy()}\")\n",
    "print(f\"   Targets:     {targets_reg.cpu().numpy()}\")\n",
    "print(f\"   MSE Loss:    {mse.item():.4f}\")\n",
    "\n",
    "# Cross-Entropy Loss - For Multi-class Classification\n",
    "loss_ce_fn = nn.CrossEntropyLoss()\n",
    "predictions_mc = torch.tensor([[2.0, 0.5, -1.0], [0.1, 1.5, 0.2]], device=device)\n",
    "targets_mc = torch.tensor([0, 1], device=device)\n",
    "ce = loss_ce_fn(predictions_mc, targets_mc)\n",
    "\n",
    "print(\"\\n2. Cross-Entropy Loss (Multi-class Classification):\")\n",
    "print(\"   Predictions (logits):\")\n",
    "print(f\"   {predictions_mc.cpu().numpy()}\")\n",
    "print(f\"   Targets (class indices): {targets_mc.cpu().numpy()}\")\n",
    "print(f\"   Cross-Entropy Loss: {ce.item():.4f}\")\n",
    "\n",
    "# Binary Cross-Entropy with Logits Loss\n",
    "loss_bce_logits_fn = nn.BCEWithLogitsLoss()\n",
    "predictions_bc = torch.tensor([-0.5, 1.5, -2.0, 3.0], device=device).unsqueeze(1)\n",
    "targets_bc = torch.tensor([0.0, 1.0, 0.0, 1.0], device=device).unsqueeze(1)\n",
    "bce_wl = loss_bce_logits_fn(predictions_bc, targets_bc)\n",
    "\n",
    "print(\"\\n3. Binary Cross-Entropy with Logits Loss:\")\n",
    "print(f\"   Predictions (logits): {predictions_bc.cpu().numpy().flatten()}\")\n",
    "print(f\"   Targets (0 or 1):     {targets_bc.cpu().numpy().flatten()}\")\n",
    "print(f\"   BCEWithLogits Loss:   {bce_wl.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimizers: How Neural Networks Learn\n",
    "\n",
    "Optimizers implement algorithms to update model parameters (weights and biases) based on computed gradients, aiming to minimize the loss function.\n",
    "\n",
    "**Common Optimizers:**\n",
    "- **SGD (Stochastic Gradient Descent):** Basic gradient-based optimization\n",
    "- **SGD with Momentum:** Adds momentum to accelerate learning\n",
    "- **Adam:** Adaptive learning rate optimization, often a good default choice\n",
    "- **Learning Rate:** Key hyperparameter controlling step size during updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate optimizers\n",
    "print(\"Optimizers Demonstration:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create a dummy model for demonstration\n",
    "dummy_model = nn.Linear(10, 2).to(device)\n",
    "print(f\"Initial parameter value: {dummy_model.weight[0,0].item():.4f}\")\n",
    "\n",
    "# Create optimizers\n",
    "optimizer_sgd = optim.SGD(dummy_model.parameters(), lr=0.01, momentum=0.9)\n",
    "print(\"\\nOptimizers created:\")\n",
    "print(\"✓ SGD with lr=0.01, momentum=0.9\")\n",
    "\n",
    "optimizer_adam = optim.Adam(dummy_model.parameters(), lr=0.001)\n",
    "print(\"✓ Adam with lr=0.001\")\n",
    "\n",
    "# Simulate one optimization step with SGD\n",
    "dummy_input = torch.randn(5, 10).to(device)\n",
    "dummy_target = torch.randn(5, 2).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(\"\\nSimulating one SGD optimization step:\")\n",
    "optimizer_sgd.zero_grad()                    # Clear gradients\n",
    "outputs = dummy_model(dummy_input)           # Forward pass\n",
    "loss = criterion(outputs, dummy_target)      # Calculate loss\n",
    "loss.backward()                              # Backward pass (compute gradients)\n",
    "optimizer_sgd.step()                         # Update weights\n",
    "\n",
    "print(f\"Parameter after SGD step: {dummy_model.weight[0,0].item():.4f}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Training Loop: Forward and Backward Propagation\n",
    "\n",
    "The training loop is the core process of neural network training:\n",
    "\n",
    "**Steps:**\n",
    "1. **Forward Propagation:** Pass data through network, calculate predictions and loss\n",
    "2. **Zero Gradients:** Clear old gradients with `optimizer.zero_grad()`\n",
    "3. **Backward Propagation:** Compute gradients with `loss.backward()`\n",
    "4. **Optimizer Step:** Update parameters with `optimizer.step()`\n",
    "5. **Repeat:** Iterate over epochs and batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Loop Steps:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"1. Forward Propagation:\")\n",
    "print(\"   → outputs = model(inputs)\")\n",
    "print(\"   → loss = criterion(outputs, labels)\")\n",
    "print()\n",
    "print(\"2. Clear Gradients:\")\n",
    "print(\"   → optimizer.zero_grad()\")\n",
    "print()\n",
    "print(\"3. Backward Propagation:\")\n",
    "print(\"   → loss.backward()\")\n",
    "print()\n",
    "print(\"4. Update Parameters:\")\n",
    "print(\"   → optimizer.step()\")\n",
    "print()\n",
    "print(\"5. Repeat for all batches and epochs\")\n",
    "print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Building and Training Your First Neural Network\n",
    "\n",
    "Let's solve the XOR problem - a classic example that demonstrates why we need multi-layer networks. XOR is not linearly separable, so a single perceptron cannot solve it, but an MLP can.\n",
    "\n",
    "**XOR Truth Table:**\n",
    "- (0,0) → 0\n",
    "- (0,1) → 1  \n",
    "- (1,0) → 1\n",
    "- (1,1) → 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the XOR data\n",
    "print(\"Step 1: Preparing XOR Data\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# XOR inputs and outputs\n",
    "X_xor = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]], device=device)\n",
    "y_xor = torch.tensor([[0.], [1.], [1.], [0.]], device=device)\n",
    "\n",
    "print(\"XOR Truth Table:\")\n",
    "print(\"Input  | Output\")\n",
    "print(\"-------|-------\")\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\"{X_xor[i].cpu().numpy()} | {y_xor[i].item():.0f}\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "xor_dataset = TensorDataset(X_xor, y_xor)\n",
    "xor_dataloader = DataLoader(xor_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(f\"\\nDataset created with {len(xor_dataset)} samples\")\n",
    "print(f\"Batch size: {xor_dataloader.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the XOR Neural Network\n",
    "print(\"Step 2: Defining XOR Neural Network\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "class XORNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XORNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 8)      # 2 inputs, 8 hidden neurons\n",
    "        self.relu = nn.ReLU()           # ReLU activation\n",
    "        self.fc2 = nn.Linear(8, 1)      # 8 hidden, 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)                 # Linear transformation\n",
    "        x = self.relu(x)                # Non-linear activation\n",
    "        x = self.fc2(x)                 # Output layer (logits)\n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "xor_model = XORNet().to(device)\n",
    "print(\"XORNet Architecture:\")\n",
    "print(xor_model)\n",
    "\n",
    "# Count parameters\n",
    "xor_params = sum(p.numel() for p in xor_model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal trainable parameters: {xor_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Loss Function and Optimizer\n",
    "print(\"Step 3: Loss Function and Optimizer\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # Handles sigmoid internally\n",
    "optimizer = optim.Adam(xor_model.parameters(), lr=0.05)\n",
    "\n",
    "print(f\"Loss Function: {criterion}\")\n",
    "print(f\"Optimizer: Adam with lr=0.05\")\n",
    "print(f\"Optimizer parameters: {len(list(optimizer.param_groups[0]['params']))} parameter groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Training Loop\n",
    "print(\"Step 4: Training the XOR Network\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "num_epochs = 1000\n",
    "losses_history = []\n",
    "\n",
    "print(f\"Training for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in xor_dataloader:\n",
    "        # Forward pass\n",
    "        outputs = xor_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    losses_history.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(f\"Training completed! Final loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses_history, 'b-', linewidth=2)\n",
    "plt.title('XOR Training Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BCEWithLogitsLoss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale to better see the convergence\n",
    "plt.show()\n",
    "\n",
    "print(f\"Loss decreased from {losses_history[0]:.4f} to {losses_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluate the Model\n",
    "print(\"Step 5: Model Evaluation\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "xor_model.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_predictions_logits = xor_model(X_xor)\n",
    "    test_predictions_probs = torch.sigmoid(test_predictions_logits)\n",
    "    predicted_classes = (test_predictions_probs >= 0.5).float()\n",
    "    \n",
    "    accuracy = (predicted_classes == y_xor).float().mean()\n",
    "    \n",
    "    print(f\"Final Accuracy: {accuracy.item()*100:.2f}%\")\n",
    "    print(\"\\nDetailed Results:\")\n",
    "    print(\"Input  | True | Pred Prob | Pred Class | Correct\")\n",
    "    print(\"-------|------|-----------|------------|--------\")\n",
    "    \n",
    "    for i in range(len(X_xor)):\n",
    "        input_vals = X_xor[i].cpu().numpy()\n",
    "        true_val = y_xor[i].item()\n",
    "        pred_prob = test_predictions_probs[i].item()\n",
    "        pred_class = predicted_classes[i].item()\n",
    "        correct = \"✓\" if pred_class == true_val else \"✗\"\n",
    "        \n",
    "        print(f\"{input_vals} | {true_val:.0f}    | {pred_prob:.4f}    | {pred_class:.0f}          | {correct}\")\n",
    "\n",
    "xor_model.train()  # Set back to training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully built and trained your first neural network using PyTorch. Here's what we covered:\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. **Neural Networks** are computational models inspired by biological neurons\n",
    "2. **Perceptrons** are the simplest neural networks but limited to linearly separable problems\n",
    "3. **Activation Functions** introduce non-linearity, enabling complex pattern learning\n",
    "4. **Multi-Layer Perceptrons** can solve non-linearly separable problems like XOR\n",
    "5. **Loss Functions** measure prediction error and guide learning\n",
    "6. **Optimizers** update model parameters to minimize loss\n",
    "7. **Training Loop** combines forward propagation, gradient computation, and parameter updates\n",
    "\n",
    "**What's Next:**\n",
    "- Explore automatic differentiation in detail\n",
    "- Learn advanced training techniques\n",
    "- Work with real datasets and data preprocessing\n",
    "- Dive into specialized architectures (CNNs, RNNs, Transformers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
