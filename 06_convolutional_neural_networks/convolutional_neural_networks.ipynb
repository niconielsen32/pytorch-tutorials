{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs) in PyTorch\n",
    "\n",
    "This notebook provides a comprehensive guide to understanding and implementing Convolutional Neural Networks (CNNs) using PyTorch. CNNs are specialized neural networks designed for processing data with grid-like topology, particularly effective for image analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from PIL import Image\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to CNNs\n",
    "\n",
    "CNNs are designed to automatically and adaptively learn spatial hierarchies of features from input images. They use three key concepts:\n",
    "\n",
    "- **Local Receptive Fields**: Each neuron is connected to only a small region of the input\n",
    "- **Shared Weights**: The same filter is used across different spatial locations \n",
    "- **Pooling**: Summarizes features in neighborhoods for translation invariance\n",
    "\n",
    "Let's explore the core components that make CNNs so effective for computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN Components Explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample input to demonstrate CNN components\n",
    "sample_input = torch.randn(1, 1, 28, 28)  # Batch size, channels, height, width\n",
    "print(f\"Sample input shape: {sample_input.shape}\")\n",
    "\n",
    "# Demonstrate convolutional layer\n",
    "conv_layer = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "conv_output = conv_layer(sample_input)\n",
    "print(f\"Conv2d: Input {sample_input.shape} -> Output {conv_output.shape}\")\n",
    "print(f\"Conv2d parameters: {sum(p.numel() for p in conv_layer.parameters())}\")\n",
    "\n",
    "# Demonstrate batch normalization\n",
    "bn_layer = nn.BatchNorm2d(16)\n",
    "bn_output = bn_layer(conv_output)\n",
    "print(f\"BatchNorm2d: Input {conv_output.shape} -> Output {bn_output.shape}\")\n",
    "\n",
    "# Demonstrate activation function\n",
    "relu_output = F.relu(bn_output)\n",
    "print(f\"ReLU: Input {bn_output.shape} -> Output {relu_output.shape}\")\n",
    "\n",
    "# Demonstrate max pooling\n",
    "pool_layer = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "pool_output = pool_layer(relu_output)\n",
    "print(f\"MaxPool2d: Input {relu_output.shape} -> Output {pool_output.shape}\")\n",
    "\n",
    "# Demonstrate adaptive average pooling\n",
    "avgpool_layer = nn.AdaptiveAvgPool2d((1, 1))\n",
    "avgpool_output = avgpool_layer(pool_output)\n",
    "print(f\"AdaptiveAvgPool2d: Input {pool_output.shape} -> Output {avgpool_output.shape}\")\n",
    "\n",
    "# Demonstrate flattening for fully connected layers\n",
    "flattened = torch.flatten(avgpool_output, 1)\n",
    "print(f\"Flatten: Input {avgpool_output.shape} -> Output {flattened.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Complete CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNComponents(nn.Module):\n",
    "    \"\"\"A comprehensive CNN demonstrating various architectural components.\"\"\"\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super(CNNComponents, self).__init__()\n",
    "        \n",
    "        # Convolutional Layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Batch Normalization\n",
    "        \n",
    "        # Convolutional Layer 2\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # Batch Normalization\n",
    "        \n",
    "        # Max Pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First convolutional block\n",
    "        x = self.conv1(x)              # Convolution\n",
    "        x = self.bn1(x)                # Batch Normalization\n",
    "        x = F.relu(x)                  # ReLU Activation\n",
    "        x = self.pool(x)               # Max Pooling\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv2(x)              # Convolution\n",
    "        x = self.bn2(x)                # Batch Normalization\n",
    "        x = F.relu(x)                  # ReLU Activation\n",
    "        x = self.pool(x)               # Max Pooling\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.avgpool(x)            # Average Pooling\n",
    "        x = torch.flatten(x, 1)        # Flatten\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        x = self.dropout(x)            # Dropout\n",
    "        x = self.fc1(x)                # Fully Connected\n",
    "        x = F.relu(x)                  # ReLU Activation\n",
    "        x = self.dropout(x)            # Dropout\n",
    "        x = self.fc2(x)                # Output Layer\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create and inspect the model\n",
    "model = CNNComponents(in_channels=1, num_classes=10).to(device)\n",
    "print(\"CNN Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Test with a sample input\n",
    "sample_input = torch.randn(2, 1, 28, 28).to(device)\n",
    "output = model(sample_input)\n",
    "print(f\"\\nInput shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation for CNNs\n",
    "\n",
    "CNNs require proper data preprocessing and augmentation for optimal performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for training and testing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),                    # Random rotation Â±10 degrees\n",
    "    transforms.RandomHorizontalFlip(p=0.5),          # Random horizontal flip\n",
    "    transforms.ToTensor(),                            # Convert to tensor\n",
    "    transforms.Normalize((0.1307,), (0.3081,))       # MNIST normalization\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                            # Convert to tensor\n",
    "    transforms.Normalize((0.1307,), (0.3081,))       # MNIST normalization\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=test_transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Visualize some samples\n",
    "def visualize_samples(dataset, num_samples=8):\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i in range(num_samples):\n",
    "        image, label = dataset[i]\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(image.squeeze(), cmap='gray')\n",
    "        plt.title(f'Label: {label}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = CNNComponents(in_channels=1, num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=2):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Print progress\n",
    "            if (i + 1) % 200 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], '\n",
    "                      f'Loss: {loss.item():.4f}, Accuracy: {100 * correct / total:.2f}%')\n",
    "        \n",
    "        # Calculate epoch statistics\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] completed: '\n",
    "              f'Average Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting CNN training...\")\n",
    "train_losses, train_accuracies = train_model(model, train_loader, criterion, optimizer, num_epochs=2)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    overall_accuracy = 100 * correct / total\n",
    "    print(f'Overall Test Accuracy: {overall_accuracy:.2f}%')\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    print('\\nPer-class Accuracy:')\n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            class_acc = 100 * class_correct[i] / class_total[i]\n",
    "            print(f'Class {i}: {class_acc:.2f}%')\n",
    "    \n",
    "    return overall_accuracy\n",
    "\n",
    "# Evaluate the trained model\n",
    "test_accuracy = evaluate_model(model, test_loader)\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies)\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Filter Visualization\n",
    "\n",
    "Let's visualize what the CNN has learned by examining the filters in the first convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_filters(model, layer_name=\"conv1\"):\n",
    "    \"\"\"Visualize the filters of a specific layer in a CNN model.\"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if name == layer_name:\n",
    "            weights = module.weight.data.cpu()\n",
    "            \n",
    "            # Normalize the weights for better visualization\n",
    "            weights = (weights - weights.min()) / (weights.max() - weights.min())\n",
    "            \n",
    "            # Plot the filters\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            num_filters = min(32, weights.shape[0])  # Show up to 32 filters\n",
    "            num_cols = 8\n",
    "            num_rows = (num_filters + num_cols - 1) // num_cols\n",
    "            \n",
    "            for i in range(num_filters):\n",
    "                plt.subplot(num_rows, num_cols, i + 1)\n",
    "                \n",
    "                # For single-channel input (grayscale)\n",
    "                if weights.shape[1] == 1:\n",
    "                    plt.imshow(weights[i, 0], cmap='viridis')\n",
    "                else:\n",
    "                    # For multi-channel input (RGB)\n",
    "                    plt.imshow(weights[i].permute(1, 2, 0))\n",
    "                    \n",
    "                plt.axis('off')\n",
    "                plt.title(f'Filter {i+1}', fontsize=8)\n",
    "            \n",
    "            plt.suptitle(f'Filters in {layer_name}', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            return\n",
    "    \n",
    "    print(f\"Layer '{layer_name}' not found in the model.\")\n",
    "\n",
    "# Visualize the learned filters\n",
    "visualize_filters(model, \"conv1\")\n",
    "\n",
    "# Function to visualize feature maps\n",
    "def visualize_feature_maps(model, input_image, layer_name=\"conv1\"):\n",
    "    \"\"\"Visualize feature maps produced by a specific layer.\"\"\"\n",
    "    activation = {}\n",
    "    \n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    \n",
    "    # Register hook\n",
    "    for name, module in model.named_modules():\n",
    "        if name == layer_name:\n",
    "            module.register_forward_hook(get_activation(name))\n",
    "    \n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(input_image.unsqueeze(0).to(device))\n",
    "    \n",
    "    # Get feature maps\n",
    "    if layer_name in activation:\n",
    "        feature_maps = activation[layer_name].cpu().squeeze(0)\n",
    "        \n",
    "        # Plot feature maps\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        num_maps = min(16, feature_maps.shape[0])  # Show up to 16 feature maps\n",
    "        num_cols = 8\n",
    "        num_rows = (num_maps + num_cols - 1) // num_cols\n",
    "        \n",
    "        for i in range(num_maps):\n",
    "            plt.subplot(num_rows, num_cols, i + 1)\n",
    "            plt.imshow(feature_maps[i], cmap='viridis')\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Map {i+1}', fontsize=8)\n",
    "        \n",
    "        plt.suptitle(f'Feature Maps from {layer_name}', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No activation captured for layer '{layer_name}'\")\n",
    "\n",
    "# Visualize feature maps for a sample image\n",
    "sample_image, _ = test_dataset[0]\n",
    "print(f\"Visualizing feature maps for a sample image:\")\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(sample_image.squeeze(), cmap='gray')\n",
    "plt.title('Input Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "visualize_feature_maps(model, sample_image, \"conv1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Transfer Learning Example\n",
    "\n",
    "Transfer learning allows us to leverage pre-trained models for our specific tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of transfer learning with a pre-trained ResNet\n",
    "def create_transfer_learning_model(num_classes=10, pretrained=True):\n",
    "    \"\"\"Create a transfer learning model based on ResNet18.\"\"\"\n",
    "    # Load pre-trained ResNet18\n",
    "    model = models.resnet18(pretrained=pretrained)\n",
    "    \n",
    "    # Modify the first convolutional layer to accept single-channel input (for MNIST)\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    \n",
    "    # Replace the final fully connected layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create transfer learning model\n",
    "transfer_model = create_transfer_learning_model(num_classes=10, pretrained=True).to(device)\n",
    "print(\"Transfer Learning Model (ResNet18-based):\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in transfer_model.parameters())}\")\n",
    "\n",
    "# Feature extraction vs fine-tuning\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    \\\"\\\"\\\"Set requires_grad for model parameters based on feature extraction mode.\\\"\\\"\\\"\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Only train the final classifier\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "# Example: Feature extraction mode\n",
    "print(\"\\\\nSetting up for feature extraction (freezing pre-trained weights)...\")\n",
    "set_parameter_requires_grad(transfer_model, feature_extracting=True)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in transfer_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in transfer_model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "# Test the transfer learning model\n",
    "sample_input = torch.randn(2, 1, 28, 28).to(device)\n",
    "with torch.no_grad():\n",
    "    output = transfer_model(sample_input)\n",
    "print(f\"\\\\nTransfer model output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook covered the fundamentals of Convolutional Neural Networks in PyTorch:\n",
    "\n",
    "1. **CNN Architecture**: Understanding convolution, pooling, and fully connected layers\n",
    "2. **Building CNNs**: Creating complete architectures with batch normalization and dropout\n",
    "3. **Data Preprocessing**: Applying transforms and augmentation for better performance\n",
    "4. **Training**: Implementing training loops with proper monitoring\n",
    "5. **Evaluation**: Testing model performance and analyzing results\n",
    "6. **Visualization**: Understanding what CNNs learn through filter and feature map visualization\n",
    "7. **Transfer Learning**: Leveraging pre-trained models for improved performance\n",
    "\n",
    "**Key Takeaways:**\n",
    "- CNNs excel at capturing spatial hierarchies in images\n",
    "- Data augmentation is crucial for generalization\n",
    "- Batch normalization and dropout help with training stability and overfitting\n",
    "- Transfer learning can significantly improve performance, especially with limited data\n",
    "- Visualization helps understand what the network has learned\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with different CNN architectures (ResNet, DenseNet, EfficientNet)\n",
    "- Try different datasets (CIFAR-10, ImageNet)\n",
    "- Explore advanced techniques like attention mechanisms\n",
    "- Learn about object detection and segmentation architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
