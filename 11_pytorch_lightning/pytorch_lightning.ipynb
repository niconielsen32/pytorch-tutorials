{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lightning Tutorial\n",
    "\n",
    "This notebook demonstrates how to use PyTorch Lightning to simplify deep learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch Lightning if not already installed\n",
    "# !pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# Set random seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Check PyTorch Lightning version\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to PyTorch Lightning\n",
    "\n",
    "PyTorch Lightning is a lightweight wrapper that:\n",
    "- Eliminates boilerplate code\n",
    "- Provides automatic optimization\n",
    "- Enables easy distributed training\n",
    "- Integrates logging and checkpointing\n",
    "- Ensures reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lightning Module\n",
    "\n",
    "The LightningModule organizes PyTorch code into a standard structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNISTClassifier(pl.LightningModule):\n",
    "    \"\"\"A simple CNN for MNIST classification using PyTorch Lightning.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        # Save hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Define model architecture\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_accuracy = pl.metrics.Accuracy()\n",
    "        self.val_accuracy = pl.metrics.Accuracy()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step - called for each batch.\"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.train_accuracy(preds, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Validation step.\"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.val_accuracy(preds, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"Test step.\"\"\"\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure optimizers and schedulers.\"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and inspect the model\n",
    "model = LitMNISTClassifier(learning_rate=1e-3)\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Hyperparameters: {model.hparams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Module\n",
    "\n",
    "DataModules encapsulate all data loading logic in a reusable class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    \"\"\"PyTorch Lightning DataModule for MNIST.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='./data', batch_size=64, num_workers=2):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        # Define transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Download data if needed. Called only on 1 GPU/process.\"\"\"\n",
    "        torchvision.datasets.MNIST(self.data_dir, train=True, download=True)\n",
    "        torchvision.datasets.MNIST(self.data_dir, train=False, download=True)\n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"Setup train/val/test datasets. Called on every GPU.\"\"\"\n",
    "        if stage == 'fit' or stage is None:\n",
    "            mnist_full = torchvision.datasets.MNIST(\n",
    "                self.data_dir, train=True, transform=self.transform\n",
    "            )\n",
    "            # Split into train and validation\n",
    "            self.mnist_train, self.mnist_val = random_split(\n",
    "                mnist_full, [55000, 5000]\n",
    "            )\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = torchvision.datasets.MNIST(\n",
    "                self.data_dir, train=False, transform=self.transform\n",
    "            )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.mnist_train, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.mnist_val, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.mnist_test, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data module\n",
    "data_module = MNISTDataModule(batch_size=64, num_workers=0)\n",
    "data_module.prepare_data()\n",
    "data_module.setup('fit')\n",
    "\n",
    "print(f\"Train samples: {len(data_module.mnist_train)}\")\n",
    "print(f\"Val samples: {len(data_module.mnist_val)}\")\n",
    "\n",
    "# Visualize some samples\n",
    "train_loader = data_module.train_dataloader()\n",
    "batch = next(iter(train_loader))\n",
    "images, labels = batch\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Label: {labels[i]}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Training\n",
    "\n",
    "Training with PyTorch Lightning is as simple as creating a Trainer and calling fit()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and data\n",
    "model = LitMNISTClassifier(learning_rate=1e-3)\n",
    "data_module = MNISTDataModule(batch_size=64, num_workers=0)\n",
    "\n",
    "# Create trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    progress_bar_refresh_rate=20\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_results = trainer.test(model, data_module)\n",
    "print(f\"Test accuracy: {test_results[0]['test_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Callbacks\n",
    "\n",
    "Callbacks allow you to add functionality at various points during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints/',\n",
    "    filename='mnist-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "# Create trainer with callbacks\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],\n",
    "    progress_bar_refresh_rate=20\n",
    ")\n",
    "\n",
    "# Create new model\n",
    "model = LitMNISTClassifier(learning_rate=1e-3)\n",
    "\n",
    "# Train with callbacks\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best model saved at: {best_model_path}\")\n",
    "\n",
    "# Load and test best model\n",
    "best_model = LitMNISTClassifier.load_from_checkpoint(best_model_path)\n",
    "test_results = trainer.test(best_model, data_module)\n",
    "print(f\"Best model test accuracy: {test_results[0]['test_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Lightning Module\n",
    "\n",
    "Here's a more advanced example with additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedLitModel(pl.LightningModule):\n",
    "    \"\"\"Advanced Lightning module with more features.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Enhanced architecture\n",
    "        self.features = nn.Sequential(\n",
    "            # First block\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Second block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log_dict({\n",
    "            'train_loss': loss,\n",
    "            'train_acc': acc\n",
    "        }, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log_dict({\n",
    "            'val_loss': loss,\n",
    "            'val_acc': acc\n",
    "        }, prog_bar=True)\n",
    "        \n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Calculate average metrics\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        \n",
    "        print(f\"\\nValidation - Loss: {avg_loss:.4f}, Acc: {avg_acc:.4f}\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Optimizer\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss',\n",
    "                'interval': 'epoch',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create advanced model\n",
    "advanced_model = AdvancedLitModel(learning_rate=1e-3)\n",
    "print(f\"Advanced model parameters: {sum(p.numel() for p in advanced_model.parameters()):,}\")\n",
    "\n",
    "# Train with advanced features\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    precision=16,  # Mixed precision training\n",
    "    gradient_clip_val=1.0,  # Gradient clipping\n",
    "    accumulate_grad_batches=2,  # Gradient accumulation\n",
    "    progress_bar_refresh_rate=20\n",
    ")\n",
    "\n",
    "trainer.fit(advanced_model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Logging with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorBoard logger\n",
    "tb_logger = TensorBoardLogger(\n",
    "    save_dir='lightning_logs',\n",
    "    name='mnist_experiment'\n",
    ")\n",
    "\n",
    "# Create model with custom logging\n",
    "class LoggingLitModel(LitMNISTClassifier):\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        \n",
    "        # Log images occasionally\n",
    "        if batch_idx % 100 == 0:\n",
    "            # Log sample images\n",
    "            grid = torchvision.utils.make_grid(x[:8])\n",
    "            self.logger.experiment.add_image('train_images', grid, self.global_step)\n",
    "            \n",
    "            # Log predictions\n",
    "            self.logger.experiment.add_text(\n",
    "                'predictions',\n",
    "                f'True: {y[:8].tolist()}, Pred: {preds[:8].tolist()}',\n",
    "                self.global_step\n",
    "            )\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Train with logging\n",
    "logging_model = LoggingLitModel()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    logger=tb_logger,\n",
    "    progress_bar_refresh_rate=20\n",
    ")\n",
    "\n",
    "trainer.fit(logging_model, data_module)\n",
    "\n",
    "print(f\"\\nTensorBoard logs saved to: {tb_logger.log_dir}\")\n",
    "print(\"Run 'tensorboard --logdir=lightning_logs' to view logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Export and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to TorchScript\n",
    "model.eval()\n",
    "example_input = torch.randn(1, 1, 28, 28)\n",
    "traced_model = model.to_torchscript(method='trace', example_inputs=example_input)\n",
    "\n",
    "# Save traced model\n",
    "torch.jit.save(traced_model, 'mnist_lightning_model.pt')\n",
    "print(\"Model exported to TorchScript\")\n",
    "\n",
    "# Load and use for inference\n",
    "loaded_model = torch.jit.load('mnist_lightning_model.pt')\n",
    "loaded_model.eval()\n",
    "\n",
    "# Test inference\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(1, 1, 28, 28)\n",
    "    output = loaded_model(test_input)\n",
    "    prediction = torch.argmax(output, dim=1)\n",
    "    print(f\"Inference test - Output shape: {output.shape}, Prediction: {prediction.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of best practices\n",
    "best_practices = \"\"\"\n",
    "# PyTorch Lightning Best Practices\n",
    "\n",
    "## 1. Code Organization\n",
    "- Keep model logic in LightningModule\n",
    "- Use DataModules for data handling\n",
    "- Separate training, validation, and test logic\n",
    "\n",
    "## 2. Reproducibility\n",
    "- Use pl.seed_everything()\n",
    "- Save hyperparameters with save_hyperparameters()\n",
    "- Version control your experiments\n",
    "\n",
    "## 3. Monitoring\n",
    "- Use self.log() for metrics\n",
    "- Integrate with TensorBoard, W&B, etc.\n",
    "- Monitor hardware utilization\n",
    "\n",
    "## 4. Performance\n",
    "- Use mixed precision training (precision=16)\n",
    "- Enable gradient accumulation for large batches\n",
    "- Profile your code to find bottlenecks\n",
    "\n",
    "## 5. Distributed Training\n",
    "- Start with DDP for multi-GPU\n",
    "- Test on single GPU first\n",
    "- Use appropriate batch sizes\n",
    "\n",
    "## 6. Production\n",
    "- Export to TorchScript for deployment\n",
    "- Use ModelCheckpoint for saving best models\n",
    "- Implement proper error handling\n",
    "\"\"\"\n",
    "\n",
    "print(best_practices)\n",
    "\n",
    "# Save summary\n",
    "with open('lightning_best_practices.md', 'w') as f:\n",
    "    f.write(best_practices)\n",
    "print(\"\\nBest practices saved to 'lightning_best_practices.md'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. **LightningModule**: Organizing PyTorch code into a standard structure\n",
    "2. **DataModule**: Encapsulating data loading logic\n",
    "3. **Trainer**: Automating the training loop with minimal code\n",
    "4. **Callbacks**: Adding functionality like checkpointing and early stopping\n",
    "5. **Logging**: Integrating with TensorBoard for experiment tracking\n",
    "6. **Advanced Features**: Mixed precision, gradient accumulation, and more\n",
    "7. **Export**: Converting models for production deployment\n",
    "\n",
    "PyTorch Lightning simplifies deep learning workflows while maintaining flexibility and performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}