{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 17: Model Optimization Techniques\n",
    "\n",
    "This tutorial covers advanced model optimization techniques including quantization, pruning, knowledge distillation, and model compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.quantization import quantize_dynamic, quantize_fx\n",
    "import torch.nn.utils.prune as prune\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from typing import Tuple, List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Quantization\n",
    "\n",
    "Quantization reduces the numerical precision of weights and activations, significantly reducing model size and improving inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model for demonstration\n",
    "class SimpleModel(nn.Module):\n",
    "    \"\"\"Simple model for demonstrating optimization techniques\"\"\"\n",
    "    def __init__(self, input_size=784, hidden_size=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate model\n",
    "def evaluate_model(model, data_loader=None):\n",
    "    \"\"\"Evaluate model size, speed, and accuracy\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Model size\n",
    "    param_size = 0\n",
    "    buffer_size = 0\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    \n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    model_size = (param_size + buffer_size) / 1024 / 1024  # MB\n",
    "    \n",
    "    # Speed test\n",
    "    dummy_input = torch.randn(100, 784)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_input)\n",
    "    \n",
    "    # Time inference\n",
    "    start_time = time.time()\n",
    "    num_iterations = 100\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iterations):\n",
    "            _ = model(dummy_input)\n",
    "    \n",
    "    inference_time = (time.time() - start_time) / num_iterations * 1000  # ms\n",
    "    \n",
    "    return model_size, inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different quantization methods\n",
    "original_model = SimpleModel()\n",
    "original_size, original_time = evaluate_model(original_model)\n",
    "\n",
    "print(\"Model Quantization Comparison\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original Model:\")\n",
    "print(f\"  Size: {original_size:.2f} MB\")\n",
    "print(f\"  Inference: {original_time:.2f} ms\")\n",
    "\n",
    "# Dynamic Quantization\n",
    "dynamic_quantized_model = quantize_dynamic(\n",
    "    copy.deepcopy(original_model),\n",
    "    {nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "dq_size, dq_time = evaluate_model(dynamic_quantized_model)\n",
    "\n",
    "print(f\"\\nDynamic Quantization:\")\n",
    "print(f\"  Size: {dq_size:.2f} MB ({original_size/dq_size:.2f}x smaller)\")\n",
    "print(f\"  Inference: {dq_time:.2f} ms ({original_time/dq_time:.2f}x speedup)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Quantization (with calibration)\n",
    "class QuantizableModel(nn.Module):\n",
    "    \"\"\"Model prepared for static quantization\"\"\"\n",
    "    def __init__(self, input_size=784, hidden_size=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# Prepare for static quantization\n",
    "static_model = QuantizableModel()\n",
    "static_model.eval()\n",
    "\n",
    "# Set quantization config\n",
    "static_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "torch.quantization.prepare(static_model, inplace=True)\n",
    "\n",
    "# Calibrate with representative data\n",
    "calibration_data = torch.randn(1000, 784)\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(calibration_data), 100):\n",
    "        static_model(calibration_data[i:i+100])\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.quantization.convert(static_model, inplace=True)\n",
    "\n",
    "sq_size, sq_time = evaluate_model(static_model)\n",
    "print(f\"\\nStatic Quantization:\")\n",
    "print(f\"  Size: {sq_size:.2f} MB ({original_size/sq_size:.2f}x smaller)\")\n",
    "print(f\"  Inference: {sq_time:.2f} ms ({original_time/sq_time:.2f}x speedup)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quantization comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Model sizes\n",
    "models = ['Original', 'Dynamic\\nQuantized', 'Static\\nQuantized']\n",
    "sizes = [original_size, dq_size, sq_size]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "bars1 = ax1.bar(models, sizes, color=colors)\n",
    "ax1.set_ylabel('Model Size (MB)')\n",
    "ax1.set_title('Model Size Comparison')\n",
    "ax1.set_ylim(0, max(sizes) * 1.2)\n",
    "\n",
    "# Add value labels\n",
    "for bar, size in zip(bars1, sizes):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{size:.2f} MB', ha='center', va='bottom')\n",
    "\n",
    "# Inference times\n",
    "times = [original_time, dq_time, sq_time]\n",
    "bars2 = ax2.bar(models, times, color=colors)\n",
    "ax2.set_ylabel('Inference Time (ms)')\n",
    "ax2.set_title('Inference Speed Comparison')\n",
    "ax2.set_ylim(0, max(times) * 1.2)\n",
    "\n",
    "# Add value labels\n",
    "for bar, time in zip(bars2, times):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{time:.2f} ms', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network Pruning\n",
    "\n",
    "Pruning removes unnecessary connections or neurons from the network, creating sparse models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count total and non-zero parameters\"\"\"\n",
    "    total_params = 0\n",
    "    nonzero_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if hasattr(param, 'data'):\n",
    "            nonzero_params += torch.count_nonzero(param.data).item()\n",
    "        else:\n",
    "            nonzero_params += param.numel()\n",
    "    \n",
    "    return total_params, nonzero_params\n",
    "\n",
    "def visualize_weight_distribution(model, title=\"Weight Distribution\"):\n",
    "    \"\"\"Visualize weight distribution of the model\"\"\"\n",
    "    weights = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            weights.extend(param.data.cpu().numpy().flatten())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(weights, bins=100, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Weight Value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstructured Pruning\n",
    "pruning_model = SimpleModel()\n",
    "total_params, nonzero_params = count_parameters(pruning_model)\n",
    "\n",
    "print(\"Unstructured Pruning\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Before pruning:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Non-zero parameters: {nonzero_params:,}\")\n",
    "print(f\"  Sparsity: {(1 - nonzero_params/total_params)*100:.1f}%\")\n",
    "\n",
    "# Visualize original weights\n",
    "visualize_weight_distribution(pruning_model, \"Original Weight Distribution\")\n",
    "\n",
    "# Apply L1 unstructured pruning\n",
    "for name, module in pruning_model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.5)\n",
    "\n",
    "total_params, nonzero_params = count_parameters(pruning_model)\n",
    "print(f\"\\nAfter 50% pruning:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Non-zero parameters: {nonzero_params:,}\")\n",
    "print(f\"  Sparsity: {(1 - nonzero_params/total_params)*100:.1f}%\")\n",
    "\n",
    "# Visualize pruned weights\n",
    "visualize_weight_distribution(pruning_model, \"Pruned Weight Distribution (50% sparsity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different pruning methods comparison\n",
    "pruning_methods = {\n",
    "    'L1': prune.L1Unstructured,\n",
    "    'L2': prune.L2Unstructured,\n",
    "    'Random': prune.RandomUnstructured\n",
    "}\n",
    "\n",
    "pruning_amounts = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "results = {method: [] for method in pruning_methods}\n",
    "\n",
    "for method_name, method_class in pruning_methods.items():\n",
    "    for amount in pruning_amounts:\n",
    "        # Create fresh model\n",
    "        model = SimpleModel()\n",
    "        \n",
    "        # Apply pruning\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                method_class.apply(module, name='weight', amount=amount)\n",
    "        \n",
    "        # Count parameters\n",
    "        total, nonzero = count_parameters(model)\n",
    "        sparsity = 1 - (nonzero / total)\n",
    "        results[method_name].append(sparsity)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "for method_name, sparsities in results.items():\n",
    "    plt.plot([a*100 for a in pruning_amounts], \n",
    "             [s*100 for s in sparsities], \n",
    "             marker='o', linewidth=2, label=method_name)\n",
    "\n",
    "plt.xlabel('Pruning Amount (%)')\n",
    "plt.ylabel('Actual Sparsity (%)')\n",
    "plt.title('Pruning Methods Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured Pruning (channel/filter pruning)\n",
    "structured_model = SimpleModel()\n",
    "\n",
    "print(\"\\nStructured Pruning\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get original dimensions\n",
    "for name, module in structured_model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        print(f\"{name}: {module.weight.shape}\")\n",
    "\n",
    "# Apply structured pruning\n",
    "for name, module in structured_model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        # Prune output channels (neurons)\n",
    "        prune.ln_structured(module, name='weight', amount=0.3, n=2, dim=0)\n",
    "\n",
    "print(\"\\nAfter structured pruning (30% of neurons):\")\n",
    "# Note: Structured pruning doesn't change tensor dimensions, \n",
    "# it zeros out entire rows/columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Knowledge Distillation\n",
    "\n",
    "Knowledge distillation transfers knowledge from a large teacher model to a smaller student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherModel(nn.Module):\n",
    "    \"\"\"Large teacher model\"\"\"\n",
    "    def __init__(self, input_size=784, hidden_size=512, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc4 = nn.Linear(hidden_size // 2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "class StudentModel(nn.Module):\n",
    "    \"\"\"Small student model\"\"\"\n",
    "    def __init__(self, input_size=784, hidden_size=128, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(student_outputs, teacher_outputs, labels, temperature=4.0, alpha=0.7):\n",
    "    \"\"\"Knowledge distillation loss\"\"\"\n",
    "    # Soft targets loss\n",
    "    soft_targets = F.softmax(teacher_outputs / temperature, dim=1)\n",
    "    soft_loss = F.kl_div(\n",
    "        F.log_softmax(student_outputs / temperature, dim=1),\n",
    "        soft_targets,\n",
    "        reduction='batchmean'\n",
    "    ) * (temperature ** 2)\n",
    "    \n",
    "    # Hard targets loss\n",
    "    hard_loss = F.cross_entropy(student_outputs, labels)\n",
    "    \n",
    "    # Combined loss\n",
    "    return alpha * soft_loss + (1 - alpha) * hard_loss, soft_loss, hard_loss\n",
    "\n",
    "# Create teacher and student models\n",
    "teacher = TeacherModel()\n",
    "student = StudentModel()\n",
    "\n",
    "teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "student_params = sum(p.numel() for p in student.parameters())\n",
    "\n",
    "print(\"Knowledge Distillation\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Teacher parameters: {teacher_params:,}\")\n",
    "print(f\"Student parameters: {student_params:,} ({teacher_params/student_params:.1f}x smaller)\")\n",
    "print(f\"Compression ratio: {teacher_params/student_params:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate distillation training\n",
    "def train_with_distillation(teacher, student, num_epochs=10, temperature=4.0, alpha=0.7):\n",
    "    \"\"\"Train student model with knowledge distillation\"\"\"\n",
    "    teacher.eval()  # Teacher in eval mode\n",
    "    student.train()\n",
    "    \n",
    "    optimizer = optim.Adam(student.parameters(), lr=0.001)\n",
    "    losses = {'total': [], 'soft': [], 'hard': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Generate dummy data\n",
    "        batch_size = 128\n",
    "        x = torch.randn(batch_size, 784)\n",
    "        labels = torch.randint(0, 10, (batch_size,))\n",
    "        \n",
    "        # Teacher predictions\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(x)\n",
    "        \n",
    "        # Student predictions\n",
    "        student_outputs = student(x)\n",
    "        \n",
    "        # Calculate distillation loss\n",
    "        loss, soft_loss, hard_loss = distillation_loss(\n",
    "            student_outputs, teacher_outputs, labels, temperature, alpha\n",
    "        )\n",
    "        \n",
    "        # Update student\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses['total'].append(loss.item())\n",
    "        losses['soft'].append(soft_loss.item())\n",
    "        losses['hard'].append(hard_loss.item())\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train with different temperatures\n",
    "temperatures = [1, 4, 10, 20]\n",
    "all_losses = {}\n",
    "\n",
    "for temp in temperatures:\n",
    "    teacher_model = TeacherModel()\n",
    "    student_model = StudentModel()\n",
    "    losses = train_with_distillation(teacher_model, student_model, \n",
    "                                    num_epochs=50, temperature=temp)\n",
    "    all_losses[temp] = losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distillation results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (temp, losses) in enumerate(all_losses.items()):\n",
    "    ax = axes[idx]\n",
    "    epochs = range(len(losses['total']))\n",
    "    \n",
    "    ax.plot(epochs, losses['total'], label='Total Loss', linewidth=2)\n",
    "    ax.plot(epochs, losses['soft'], label='Soft Loss', linewidth=2, linestyle='--')\n",
    "    ax.plot(epochs, losses['hard'], label='Hard Loss', linewidth=2, linestyle=':')\n",
    "    \n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(f'Temperature = {temp}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Effect of temperature on soft targets\n",
    "plt.figure(figsize=(10, 6))\n",
    "logits = torch.tensor([2.0, 1.0, 0.1, -1.0, -2.0])\n",
    "temps = [0.5, 1, 2, 5, 10, 20]\n",
    "\n",
    "for temp in temps:\n",
    "    probs = F.softmax(logits / temp, dim=0)\n",
    "    plt.plot(probs.numpy(), marker='o', label=f'T={temp}')\n",
    "\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Effect of Temperature on Softmax Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Compression Techniques\n",
    "\n",
    "Let's explore more advanced compression techniques like low-rank factorization and weight clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-rank factorization\n",
    "class LowRankLinear(nn.Module):\n",
    "    \"\"\"Low-rank factorization of linear layer\"\"\"\n",
    "    def __init__(self, in_features, out_features, rank):\n",
    "        super().__init__()\n",
    "        self.U = nn.Linear(in_features, rank, bias=False)\n",
    "        self.V = nn.Linear(rank, out_features, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.V(self.U(x))\n",
    "\n",
    "# Compare original vs low-rank\n",
    "def compare_lowrank_compression(in_features, out_features, ranks):\n",
    "    \"\"\"Compare different low-rank approximations\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Original layer\n",
    "    original = nn.Linear(in_features, out_features)\n",
    "    orig_params = sum(p.numel() for p in original.parameters())\n",
    "    results.append(('Original', orig_params, 1.0))\n",
    "    \n",
    "    # Low-rank versions\n",
    "    for rank in ranks:\n",
    "        lr_layer = LowRankLinear(in_features, out_features, rank)\n",
    "        lr_params = sum(p.numel() for p in lr_layer.parameters())\n",
    "        compression = orig_params / lr_params\n",
    "        results.append((f'Rank-{rank}', lr_params, compression))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test different configurations\n",
    "in_features, out_features = 512, 512\n",
    "ranks = [64, 128, 256]\n",
    "results = compare_lowrank_compression(in_features, out_features, ranks)\n",
    "\n",
    "print(\"Low-Rank Factorization\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original: {in_features} × {out_features} = {in_features * out_features:,} weights\")\n",
    "print()\n",
    "print(f\"{'Method':<15} {'Parameters':<15} {'Compression':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for method, params, compression in results:\n",
    "    print(f\"{method:<15} {params:<15,} {compression:<15.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight clustering visualization\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def apply_weight_clustering(weight, num_clusters=16):\n",
    "    \"\"\"Apply k-means clustering to weights\"\"\"\n",
    "    # Flatten weights\n",
    "    w_flat = weight.flatten().cpu().numpy()\n",
    "    \n",
    "    # Cluster weights\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "    kmeans.fit(w_flat.reshape(-1, 1))\n",
    "    \n",
    "    # Replace weights with cluster centers\n",
    "    clustered = kmeans.cluster_centers_[kmeans.labels_]\n",
    "    clustered_weight = torch.tensor(clustered.reshape(weight.shape))\n",
    "    \n",
    "    return clustered_weight, kmeans\n",
    "\n",
    "# Demonstrate weight clustering\n",
    "example_weight = torch.randn(256, 128)\n",
    "num_clusters_list = [4, 8, 16, 32]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Original weights\n",
    "axes[0].hist(example_weight.flatten().numpy(), bins=50, alpha=0.7)\n",
    "axes[0].set_title(f'Original Weights\\n{len(torch.unique(example_weight))} unique values')\n",
    "axes[0].set_xlabel('Weight Value')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Clustered weights\n",
    "for idx, num_clusters in enumerate(num_clusters_list):\n",
    "    clustered_weight, kmeans = apply_weight_clustering(example_weight, num_clusters)\n",
    "    \n",
    "    ax = axes[idx + 1]\n",
    "    ax.hist(clustered_weight.flatten().numpy(), bins=50, alpha=0.7)\n",
    "    ax.set_title(f'{num_clusters} Clusters\\n{len(torch.unique(clustered_weight))} unique values')\n",
    "    ax.set_xlabel('Weight Value')\n",
    "    ax.set_ylabel('Count')\n",
    "    \n",
    "    # Mark cluster centers\n",
    "    for center in kmeans.cluster_centers_:\n",
    "        ax.axvline(x=center[0], color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Optimization Pipeline\n",
    "\n",
    "Let's create a complete optimization pipeline that combines multiple techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationPipeline:\n",
    "    \"\"\"Complete model optimization pipeline\"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.original_model = copy.deepcopy(model)\n",
    "        self.optimized_model = copy.deepcopy(model)\n",
    "        self.results = {}\n",
    "        \n",
    "    def evaluate(self, model, stage_name):\n",
    "        \"\"\"Evaluate and store results\"\"\"\n",
    "        size, time = evaluate_model(model)\n",
    "        total_params, nonzero_params = count_parameters(model)\n",
    "        \n",
    "        self.results[stage_name] = {\n",
    "            'size_mb': size,\n",
    "            'time_ms': time,\n",
    "            'total_params': total_params,\n",
    "            'nonzero_params': nonzero_params,\n",
    "            'sparsity': 1 - (nonzero_params / total_params)\n",
    "        }\n",
    "        \n",
    "    def apply_pruning(self, amount=0.5):\n",
    "        \"\"\"Apply pruning\"\"\"\n",
    "        for name, module in self.optimized_model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "        self.evaluate(self.optimized_model, 'pruned')\n",
    "        \n",
    "    def apply_quantization(self):\n",
    "        \"\"\"Apply quantization\"\"\"\n",
    "        self.optimized_model = quantize_dynamic(\n",
    "            self.optimized_model,\n",
    "            {nn.Linear},\n",
    "            dtype=torch.qint8\n",
    "        )\n",
    "        self.evaluate(self.optimized_model, 'quantized')\n",
    "        \n",
    "    def run_pipeline(self, prune_amount=0.5):\n",
    "        \"\"\"Run complete optimization pipeline\"\"\"\n",
    "        # Evaluate original\n",
    "        self.evaluate(self.original_model, 'original')\n",
    "        \n",
    "        # Apply pruning\n",
    "        self.apply_pruning(prune_amount)\n",
    "        \n",
    "        # Apply quantization\n",
    "        self.apply_quantization()\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Visualize optimization results\"\"\"\n",
    "        stages = list(self.results.keys())\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # Model size\n",
    "        sizes = [self.results[s]['size_mb'] for s in stages]\n",
    "        axes[0, 0].bar(stages, sizes, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "        axes[0, 0].set_ylabel('Size (MB)')\n",
    "        axes[0, 0].set_title('Model Size')\n",
    "        \n",
    "        # Inference time\n",
    "        times = [self.results[s]['time_ms'] for s in stages]\n",
    "        axes[0, 1].bar(stages, times, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "        axes[0, 1].set_ylabel('Time (ms)')\n",
    "        axes[0, 1].set_title('Inference Time')\n",
    "        \n",
    "        # Parameter count\n",
    "        params = [self.results[s]['nonzero_params'] for s in stages]\n",
    "        axes[1, 0].bar(stages, params, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "        axes[1, 0].set_ylabel('Parameters')\n",
    "        axes[1, 0].set_title('Non-zero Parameters')\n",
    "        \n",
    "        # Compression summary\n",
    "        ax = axes[1, 1]\n",
    "        ax.text(0.1, 0.9, 'Optimization Summary:', fontsize=14, fontweight='bold', \n",
    "                transform=ax.transAxes)\n",
    "        \n",
    "        y_pos = 0.7\n",
    "        for stage in stages[1:]:\n",
    "            size_reduction = self.results['original']['size_mb'] / self.results[stage]['size_mb']\n",
    "            speed_up = self.results['original']['time_ms'] / self.results[stage]['time_ms']\n",
    "            sparsity = self.results[stage]['sparsity'] * 100\n",
    "            \n",
    "            text = f\"{stage.capitalize()}:\\n\"\n",
    "            text += f\"  Size reduction: {size_reduction:.2f}x\\n\"\n",
    "            text += f\"  Speed up: {speed_up:.2f}x\\n\"\n",
    "            text += f\"  Sparsity: {sparsity:.1f}%\"\n",
    "            \n",
    "            ax.text(0.1, y_pos, text, transform=ax.transAxes, fontsize=11)\n",
    "            y_pos -= 0.3\n",
    "        \n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete optimization pipeline\n",
    "model = SimpleModel()\n",
    "pipeline = OptimizationPipeline(model)\n",
    "results = pipeline.run_pipeline(prune_amount=0.6)\n",
    "\n",
    "print(\"Optimization Pipeline Results\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Stage':<15} {'Size (MB)':<12} {'Time (ms)':<12} {'Parameters':<15} {'Sparsity':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for stage, metrics in results.items():\n",
    "    print(f\"{stage:<15} {metrics['size_mb']:<12.2f} {metrics['time_ms']:<12.2f} \"\n",
    "          f\"{metrics['nonzero_params']:<15,} {metrics['sparsity']*100:<10.1f}%\")\n",
    "\n",
    "pipeline.visualize_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices and Guidelines\n",
    "\n",
    "Let's summarize the best practices for model optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimization technique comparison\n",
    "techniques_data = {\n",
    "    'Technique': ['Dynamic Quantization', 'Static Quantization', 'Unstructured Pruning', \n",
    "                  'Structured Pruning', 'Knowledge Distillation', 'Low-Rank Factorization'],\n",
    "    'Size Reduction': [2, 4, 10, 5, 20, 3],\n",
    "    'Speed Up': [2, 3, 2, 3, 10, 2],\n",
    "    'Accuracy Loss': [0.5, 1, 3, 2, 1, 2],\n",
    "    'Implementation Difficulty': [1, 3, 2, 3, 4, 2]\n",
    "}\n",
    "\n",
    "# Create radar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Categories\n",
    "categories = ['Size\\nReduction', 'Speed\\nUp', 'Low Accuracy\\nLoss', 'Easy\\nImplementation']\n",
    "num_vars = len(categories)\n",
    "\n",
    "# Angles for each category\n",
    "angles = [n / float(num_vars) * 2 * np.pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Plot each technique\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(techniques_data['Technique'])))\n",
    "\n",
    "for idx, technique in enumerate(techniques_data['Technique']):\n",
    "    values = [\n",
    "        techniques_data['Size Reduction'][idx],\n",
    "        techniques_data['Speed Up'][idx],\n",
    "        5 - techniques_data['Accuracy Loss'][idx],  # Invert for \"low loss is good\"\n",
    "        5 - techniques_data['Implementation Difficulty'][idx]  # Invert for \"easy is good\"\n",
    "    ]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=technique, color=colors[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylim(0, 5)\n",
    "ax.set_title('Model Optimization Techniques Comparison', size=16, y=1.1)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree for optimization techniques\n",
    "print(\"Model Optimization Decision Guide\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"1. What is your primary constraint?\")\n",
    "print(\"   ├─ Model Size → Try Pruning or Distillation\")\n",
    "print(\"   ├─ Inference Speed → Try Quantization or TorchScript\")\n",
    "print(\"   └─ Both → Combine Pruning + Quantization\")\n",
    "print()\n",
    "print(\"2. What is your deployment target?\")\n",
    "print(\"   ├─ Mobile/Edge → Use INT8 Quantization + Pruning\")\n",
    "print(\"   ├─ Server CPU → Use Dynamic Quantization\")\n",
    "print(\"   └─ Server GPU → Use Mixed Precision (FP16)\")\n",
    "print()\n",
    "print(\"3. How much accuracy loss is acceptable?\")\n",
    "print(\"   ├─ < 1% → Use Quantization only\")\n",
    "print(\"   ├─ 1-3% → Add moderate Pruning\")\n",
    "print(\"   └─ > 3% → Consider Knowledge Distillation\")\n",
    "print()\n",
    "print(\"Recommended Pipeline:\")\n",
    "print(\"1. Profile and identify bottlenecks\")\n",
    "print(\"2. Apply quantization (easy win)\")\n",
    "print(\"3. Add pruning if needed\")\n",
    "print(\"4. Fine-tune after optimization\")\n",
    "print(\"5. Validate accuracy thoroughly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered comprehensive model optimization techniques:\n",
    "\n",
    "1. **Quantization**: Reducing numerical precision for smaller, faster models\n",
    "2. **Pruning**: Removing unnecessary connections to create sparse networks\n",
    "3. **Knowledge Distillation**: Transferring knowledge from large to small models\n",
    "4. **Advanced Compression**: Low-rank factorization and weight clustering\n",
    "5. **Optimization Pipelines**: Combining techniques for maximum compression\n",
    "\n",
    "### Key Takeaways:\n",
    "- Start with quantization for easy performance gains\n",
    "- Combine multiple techniques for best results\n",
    "- Always validate accuracy after optimization\n",
    "- Consider your deployment constraints\n",
    "- Use profiling to guide optimization efforts\n",
    "\n",
    "Model optimization is crucial for deploying deep learning in production!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}