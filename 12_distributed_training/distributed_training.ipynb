{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training\n",
    "\n",
    "This notebook demonstrates various distributed training techniques in PyTorch, including Data Parallel, Distributed Data Parallel, Model Parallel, and FSDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DataParallel, DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Distributed Training\n",
    "\n",
    "Distributed training enables us to:\n",
    "- Scale training across multiple GPUs and machines\n",
    "- Train larger models that don't fit on a single GPU\n",
    "- Reduce training time significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of parallelism\n",
    "print(\"Types of Parallelism:\")\n",
    "print(\"1. Data Parallel: Split data across devices, replicate model\")\n",
    "print(\"2. Model Parallel: Split model across devices\")\n",
    "print(\"3. Pipeline Parallel: Split model into stages\")\n",
    "print(\"4. Fully Sharded Data Parallel: Shard everything across devices\")\n",
    "\n",
    "# Check available backends\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nAvailable backends:\")\n",
    "    print(\"- NCCL (recommended for GPUs)\")\n",
    "    print(\"- Gloo (CPU and GPU support)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset(Dataset):\n",
    "    \"\"\"A synthetic dataset for demonstration.\"\"\"\n",
    "    def __init__(self, size=10000, input_dim=784, num_classes=10):\n",
    "        self.size = size\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Generate random data\n",
    "        data = torch.randn(self.input_dim)\n",
    "        label = torch.randint(0, self.num_classes, (1,)).item()\n",
    "        return data, label\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"A simple neural network for demonstration.\"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dim=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Test the model\n",
    "model = SimpleNet()\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(4, 784)\n",
    "output = model(dummy_input)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Parallel (DP)\n",
    "\n",
    "DataParallel is the simplest way to use multiple GPUs on a single machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Parallel example\n",
    "if torch.cuda.device_count() >= 2:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "    \n",
    "    # Create model and wrap with DataParallel\n",
    "    model_dp = SimpleNet()\n",
    "    model_dp = DataParallel(model_dp)\n",
    "    model_dp = model_dp.cuda()\n",
    "    \n",
    "    # Create a small batch\n",
    "    batch_size = 32\n",
    "    data = torch.randn(batch_size, 784).cuda()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model_dp(data)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    # DataParallel automatically splits the batch across GPUs\n",
    "    print(f\"Batch split across GPUs: {batch_size} / {torch.cuda.device_count()} = {batch_size // torch.cuda.device_count()} per GPU\")\n",
    "else:\n",
    "    print(\"DataParallel requires at least 2 GPUs\")\n",
    "    print(\"Demonstrating concept with single device...\")\n",
    "    \n",
    "    model_dp = SimpleNet()\n",
    "    if torch.cuda.is_available():\n",
    "        model_dp = model_dp.cuda()\n",
    "        data = torch.randn(32, 784).cuda()\n",
    "    else:\n",
    "        data = torch.randn(32, 784)\n",
    "    \n",
    "    output = model_dp(data)\n",
    "    print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_dp(num_epochs=2):\n",
    "    \"\"\"Train a model using DataParallel.\"\"\"\n",
    "    # Create model\n",
    "    model = SimpleNet()\n",
    "    \n",
    "    # Wrap with DataParallel if multiple GPUs available\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "        model = DataParallel(model)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = SyntheticDataset(size=1000)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 5 == 0:\n",
    "                print(f\"Epoch {epoch}, Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch} - Average Loss: {avg_loss:.4f}\\n\")\n",
    "\n",
    "# Train with DataParallel\n",
    "train_with_dp(num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distributed Data Parallel (DDP)\n",
    "\n",
    "DDP is more efficient than DP and supports multi-node training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDP setup functions\n",
    "def setup_ddp(rank, world_size):\n",
    "    \"\"\"Initialize the distributed environment.\"\"\"\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    \n",
    "    # Initialize process group\n",
    "    backend = \"nccl\" if torch.cuda.is_available() else \"gloo\"\n",
    "    dist.init_process_group(backend, rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup_ddp():\n",
    "    \"\"\"Clean up the distributed environment.\"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# Note: DDP requires spawning multiple processes\n",
    "print(\"DDP Training Function:\")\n",
    "print(\"\"\"def train_ddp(rank, world_size):\n",
    "    setup_ddp(rank, world_size)\n",
    "    \n",
    "    # Create model and move to device\n",
    "    device = torch.device(f'cuda:{rank}' if torch.cuda.is_available() else 'cpu')\n",
    "    model = SimpleNet().to(device)\n",
    "    \n",
    "    # Wrap with DDP\n",
    "    ddp_model = DDP(model, device_ids=[rank] if torch.cuda.is_available() else None)\n",
    "    \n",
    "    # Create dataset with DistributedSampler\n",
    "    dataset = SyntheticDataset()\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, sampler=sampler)\n",
    "    \n",
    "    # Training loop...\n",
    "    cleanup_ddp()\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nTo launch DDP training:\")\n",
    "print(\"torchrun --nproc_per_node=2 distributed_training.py --distributed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDP Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDP best practices\n",
    "print(\"DDP Best Practices:\")\n",
    "print(\"\\n1. Use DistributedSampler:\")\n",
    "print(\"   - Ensures each process gets different data\")\n",
    "print(\"   - Call sampler.set_epoch(epoch) for proper shuffling\")\n",
    "\n",
    "print(\"\\n2. Synchronize when needed:\")\n",
    "print(\"   - Use dist.barrier() for synchronization\")\n",
    "print(\"   - Use dist.all_reduce() for metric aggregation\")\n",
    "\n",
    "print(\"\\n3. Save checkpoints from rank 0:\")\n",
    "print(\"\"\"   if rank == 0:\n",
    "       torch.save(model.state_dict(), 'checkpoint.pth')\"\"\")\n",
    "\n",
    "print(\"\\n4. Handle random seeds:\")\n",
    "print(\"\"\"   torch.manual_seed(42 + rank)  # Different seed per process\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Parallel\n",
    "\n",
    "Model Parallel splits the model across multiple devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParallelNet(nn.Module):\n",
    "    \"\"\"A model split across multiple devices.\"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dim=256, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Determine devices\n",
    "        self.device1 = torch.device('cuda:0' if torch.cuda.device_count() > 0 else 'cpu')\n",
    "        self.device2 = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cpu')\n",
    "        \n",
    "        # Split model across devices\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim).to(self.device1)\n",
    "        self.relu1 = nn.ReLU().to(self.device1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim).to(self.device2)\n",
    "        self.relu2 = nn.ReLU().to(self.device2)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes).to(self.device2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Move input to first device\n",
    "        x = x.to(self.device1)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        \n",
    "        # Move to second device\n",
    "        x = x.to(self.device2)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create and test model parallel network\n",
    "if torch.cuda.device_count() >= 2:\n",
    "    print(\"Creating Model Parallel network across 2 GPUs\")\n",
    "    model_mp = ModelParallelNet()\n",
    "    \n",
    "    # Test forward pass\n",
    "    data = torch.randn(4, 784)\n",
    "    output = model_mp(data)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output device: {output.device}\")\n",
    "else:\n",
    "    print(\"Model Parallel requires at least 2 GPUs\")\n",
    "    print(\"Demonstrating concept with CPU...\")\n",
    "    model_mp = ModelParallelNet()\n",
    "    data = torch.randn(4, 784)\n",
    "    output = model_mp(data)\n",
    "    print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pipeline Parallel\n",
    "\n",
    "Pipeline parallelism processes micro-batches through model stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Parallel visualization\n",
    "print(\"Pipeline Parallel Concept:\")\n",
    "print(\"\\nModel is split into stages, each on a different device.\")\n",
    "print(\"Micro-batches are processed in a pipeline fashion.\\n\")\n",
    "\n",
    "# Visualize pipeline schedule\n",
    "stages = ['Stage 1', 'Stage 2', 'Stage 3', 'Stage 4']\n",
    "micro_batches = 4\n",
    "\n",
    "print(\"Pipeline Schedule (F=Forward, B=Backward):\")\n",
    "print(\"Time →\")\n",
    "for i, stage in enumerate(stages):\n",
    "    schedule = ' ' * (i * 3)\n",
    "    for mb in range(1, micro_batches + 1):\n",
    "        schedule += f'[F{mb}]'\n",
    "    for mb in range(micro_batches, 0, -1):\n",
    "        schedule += f'[B{mb}]'\n",
    "    print(f\"{stage}: {schedule}\")\n",
    "\n",
    "print(\"\\nBenefits:\")\n",
    "print(\"- Reduces GPU idle time\")\n",
    "print(\"- Enables training of very deep models\")\n",
    "print(\"- Can be combined with data parallelism\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fully Sharded Data Parallel (FSDP)\n",
    "\n",
    "FSDP enables training of extremely large models by sharding everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSDP concept demonstration\n",
    "print(\"Fully Sharded Data Parallel (FSDP)\\n\")\n",
    "\n",
    "# Memory usage comparison\n",
    "model_params = 7_000_000_000  # 7B parameters\n",
    "bytes_per_param = 4  # FP32\n",
    "num_gpus = 8\n",
    "\n",
    "# Calculate memory usage\n",
    "model_size_gb = (model_params * bytes_per_param) / (1024**3)\n",
    "optimizer_size_gb = model_size_gb * 2  # Adam has 2 states per parameter\n",
    "gradient_size_gb = model_size_gb\n",
    "\n",
    "print(f\"Model: {model_params/1e9:.0f}B parameters\")\n",
    "print(f\"\\nMemory Requirements (FP32):\")\n",
    "print(f\"- Model parameters: {model_size_gb:.1f} GB\")\n",
    "print(f\"- Optimizer states: {optimizer_size_gb:.1f} GB\")\n",
    "print(f\"- Gradients: {gradient_size_gb:.1f} GB\")\n",
    "print(f\"- Total per GPU (DDP): {model_size_gb + optimizer_size_gb + gradient_size_gb:.1f} GB\")\n",
    "\n",
    "print(f\"\\nWith FSDP ({num_gpus} GPUs):\")\n",
    "print(f\"- Per GPU: {(model_size_gb + optimizer_size_gb + gradient_size_gb) / num_gpus:.1f} GB\")\n",
    "print(f\"- Memory reduction: {(1 - 1/num_gpus) * 100:.0f}%\")\n",
    "\n",
    "print(\"\\nFSDP Features:\")\n",
    "print(\"- Shards model parameters across GPUs\")\n",
    "print(\"- Shards optimizer states\")\n",
    "print(\"- Shards gradients\")\n",
    "print(\"- Optional CPU offloading\")\n",
    "print(\"- Mixed precision support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FSDP Configuration Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSDP configuration example\n",
    "print(\"FSDP Configuration Example:\\n\")\n",
    "\n",
    "fsdp_config = \"\"\"\n",
    "from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel as FSDP,\n",
    "    MixedPrecision,\n",
    "    BackwardPrefetch,\n",
    "    ShardingStrategy,\n",
    "    CPUOffload,\n",
    ")\n",
    "\n",
    "# Configure mixed precision\n",
    "mixed_precision = MixedPrecision(\n",
    "    param_dtype=torch.float16,\n",
    "    reduce_dtype=torch.float16,\n",
    "    buffer_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# Configure CPU offloading\n",
    "cpu_offload = CPUOffload(offload_params=True)\n",
    "\n",
    "# Wrap model with FSDP\n",
    "model = FSDP(\n",
    "    model,\n",
    "    sharding_strategy=ShardingStrategy.FULL_SHARD,\n",
    "    mixed_precision=mixed_precision,\n",
    "    cpu_offload=cpu_offload,\n",
    "    backward_prefetch=BackwardPrefetch.BACKWARD_PRE,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(fsdp_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison visualization\n",
    "methods = ['Single GPU', 'DP (4 GPUs)', 'DDP (4 GPUs)', 'FSDP (4 GPUs)']\n",
    "throughput = [100, 320, 380, 350]  # Samples/second\n",
    "memory_usage = [16, 64, 64, 20]  # GB\n",
    "scaling_efficiency = [100, 80, 95, 87.5]  # Percentage\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Throughput comparison\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "ax1.bar(methods, throughput, color=colors)\n",
    "ax1.set_ylabel('Throughput (samples/sec)')\n",
    "ax1.set_title('Training Throughput')\n",
    "ax1.set_ylim(0, 400)\n",
    "for i, v in enumerate(throughput):\n",
    "    ax1.text(i, v + 10, str(v), ha='center')\n",
    "\n",
    "# Memory usage comparison\n",
    "ax2.bar(methods, memory_usage, color=colors)\n",
    "ax2.set_ylabel('Memory Usage (GB)')\n",
    "ax2.set_title('GPU Memory Usage')\n",
    "ax2.set_ylim(0, 70)\n",
    "for i, v in enumerate(memory_usage):\n",
    "    ax2.text(i, v + 2, str(v), ha='center')\n",
    "\n",
    "# Scaling efficiency\n",
    "ax3.bar(methods, scaling_efficiency, color=colors)\n",
    "ax3.set_ylabel('Scaling Efficiency (%)')\n",
    "ax3.set_title('Multi-GPU Scaling Efficiency')\n",
    "ax3.set_ylim(0, 110)\n",
    "for i, v in enumerate(scaling_efficiency):\n",
    "    ax3.text(i, v + 2, f'{v}%', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(\"- DDP provides best scaling efficiency\")\n",
    "print(\"- FSDP reduces memory usage significantly\")\n",
    "print(\"- DP has lower efficiency due to Python GIL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Choosing the Right Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree for distributed training\n",
    "print(\"Choosing the Right Distributed Training Strategy:\\n\")\n",
    "\n",
    "print(\"1. Model fits on single GPU?\")\n",
    "print(\"   YES → Use DDP for multi-GPU speedup\")\n",
    "print(\"   NO  → Continue to #2\\n\")\n",
    "\n",
    "print(\"2. Model fits with gradient checkpointing?\")\n",
    "print(\"   YES → Use DDP with gradient checkpointing\")\n",
    "print(\"   NO  → Continue to #3\\n\")\n",
    "\n",
    "print(\"3. Model has natural splitting points?\")\n",
    "print(\"   YES → Consider Pipeline Parallel\")\n",
    "print(\"   NO  → Use FSDP\\n\")\n",
    "\n",
    "print(\"Additional Considerations:\")\n",
    "print(\"- Single machine? → DP is simplest (but DDP is better)\")\n",
    "print(\"- Multiple machines? → Must use DDP or FSDP\")\n",
    "print(\"- Very large model (>10B params)? → FSDP is likely necessary\")\n",
    "print(\"- Low bandwidth between nodes? → Consider gradient compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices and Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices for distributed training\n",
    "print(\"Distributed Training Best Practices:\\n\")\n",
    "\n",
    "best_practices = {\n",
    "    \"Data Loading\": [\n",
    "        \"Use DistributedSampler for proper data distribution\",\n",
    "        \"Set sampler.set_epoch(epoch) for different shuffling\",\n",
    "        \"Pin memory for faster GPU transfer\",\n",
    "        \"Use multiple workers for data loading\"\n",
    "    ],\n",
    "    \"Gradient Synchronization\": [\n",
    "        \"Use SyncBatchNorm for batch normalization layers\",\n",
    "        \"Consider gradient accumulation for large batches\",\n",
    "        \"Use gradient clipping to prevent instabilities\"\n",
    "    ],\n",
    "    \"Checkpointing\": [\n",
    "        \"Save checkpoints only from rank 0 process\",\n",
    "        \"Use torch.save with proper map_location when loading\",\n",
    "        \"Save optimizer state for resuming training\"\n",
    "    ],\n",
    "    \"Performance\": [\n",
    "        \"Profile with torch.profiler to find bottlenecks\",\n",
    "        \"Use mixed precision training (AMP) for speedup\",\n",
    "        \"Overlap computation and communication\",\n",
    "        \"Tune batch size for optimal GPU utilization\"\n",
    "    ],\n",
    "    \"Debugging\": [\n",
    "        \"Set TORCH_DISTRIBUTED_DEBUG=DETAIL for debugging\",\n",
    "        \"Use dist.barrier() for synchronization points\",\n",
    "        \"Monitor GPU utilization and memory usage\",\n",
    "        \"Start with small scale before scaling up\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"{category}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"  • {practice}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "1. **Data Parallel (DP)** - Simple multi-GPU on single machine\n",
    "2. **Distributed Data Parallel (DDP)** - Efficient multi-GPU/multi-node\n",
    "3. **Model Parallel** - For models too large for single GPU\n",
    "4. **Pipeline Parallel** - Efficient training of deep models\n",
    "5. **Fully Sharded Data Parallel (FSDP)** - For extremely large models\n",
    "\n",
    "Key takeaways:\n",
    "- Use DDP for most distributed training scenarios\n",
    "- Consider FSDP for very large models\n",
    "- Combine strategies for optimal performance\n",
    "- Always profile and monitor your training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}